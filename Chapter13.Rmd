# Ch 13: Experimental Design and Analysis of Variance  {-}

<div class="hero-image-container"> 
  <img class= "hero-image" src="images/art/05.png">
</div>

## Experimental Design {-}

An **experimental design** is a plan and a process for gathering data in
which the researcher controls, modifies, or manipulates at least one
variable. The purpose of gathering data in any research design,
including experimental design, is to test hypotheses.

In a study with an experimental design, the researcher begins with an
idea about cause and effect. The researcher wants to test whether a
treatment causes a response. To do this, each treatment is applied to a
different group, and the mean responses of the groups are measured and
compared to each other.

### Terminology {-}

A **factor** is an explanatory variable, also called an
Independent Variable (IV). It is the variable that the researcher
controls, modifies, or manipulates to see what happens.

The **treatments** are different levels or categories
of the factor.

**Experimental units** are the objects or subjects whose
response the researcher is measuring. Experimental units are
assigned to **treatment groups** which are groups of
experimental units receiving a particular treatment.

A **response variable** is the Dependent Variable (DV). It
is what the researcher expects to vary with different treatments.

A **response** is a single measurement of the response
variable for a given experimental unit.

**Random assignment** is the process of randomly assigning
experimental units to treatment groups, or randomly assigning
treatments to experimental units. **Random assignment makes the
groups equivalent at the outset of the study.... so any changes you
see in the groups after applying the treatments, can be attributed
to the treatments themselves.**

Two types of experimental design:

1)  **Fully randomized experimental design:** The treatments
    are randomly assigned to the groups, or, equivalently, the groups
    are randomly assigned to the treatments. In either case, the
    treatments are randomly assigned **by the researcher.**
    This is the key component of a fully randomized experimental design.
    The researcher imposes changes or otherwise manipulates the
    treatments that the groups receive.  
    
    **Can prove cause and effect.** This is because
    the process of randomly assigning treatment groups makes the
    groups equivalent at the outset. Everything other than the
    treatment that could affect the response is randomly distributed
    among the groups. Therefore, any difference in the mean response
    between treatment groups must be due to the treatment itself.  
    
    In a fully randomized experimental design, it is appropriate
    to call the Independent Variable (the factor) the **cause**
    and the Dependent Variable (the response) the **effect.** This
    is because you can prove cause and effect with a fully
    randomized experiment.

2)  **Nonrandomized experimental design:** also called
    quasi-experimental design. The treatments are not randomly assigned
    by the researcher, but different groups still receive different
    treatments.
    
    This type of design **cannot prove cause and effect.**

<br>

Both experimental designs contrast with another type of study design:
**observational design.** In observational designs, outcomes
are observed and measured for different groups, but there is no
assignment of treatments to different groups. Researchers using
observational designs look at ongoing behavior -- they **do not impose
any changes** in any variables. These studies are useful to
illuminate associations and relationships between variables but **they
cannot prove cause and effect**. In order for observational studies to
have validity, they must use random sampling from the population of
interest.

In both nonrandomized experimental design and observational design,
there is no way to completely rule out the possibility that there are
pre-existing differences between treatment groups that explain differing
responses. This is why these types of studies do not prove cause and
effect.


**Exercise**

A local grocery store would like to increase sales on produce. This
grocery store tracks purchases at the customer level.

The grocery store decided that an effective way to boost sales of
produce might be to redesign the weekly ad that customers receive. In
the current design (Weekly Ad A), produce is on an inside page, the
pictures are small, and the emphasis is on price. In the second design
(Weekly Ad B), the produce is featured on the front page, beautifully
photographed in full color, and there are recipes that incorporate
produce. In the third design (Weekly Ad C) is the same as Design B
except that there are no recipes.

The grocery store takes a random sample of customers. These customers
are randomly assigned to receive one of the three Weekly Ad designs. The
customers' purchases are tracked and their weekly spending on produce is
measured.

1\) What is the **factor** in this experiment?

2\) What are two other terms for **factor**?

3\) What are the **treatments**? How many treatments are there?

4\) How many groups of customers will there be in this experiment?

5\) What is the **response variable** in this experiment?

6\) What is another term for **response variable**?

7\) What are the **experimental units**?

8\) Give an example of a **response** in this study.

9\) Is this a fully randomized experiment? How do you know?

10\) Suppose that differences in Weekly Spending on Produce are observed
among the groups after they receive the Ads. Can the researcher conclude
that the Weekly Ad Design *caused* those differences?

<br>

## Introduction to Analysis of Variance (ANOVA) {-}

**ANOVA** is a hypothesis testing procedure that allows us to test the
equality of three or more population means in one hypothesis test.

ANOVA is used to test whether the mean response differs between
treatment groups in fully randomized and quasi-experimental designs. It
is also used to test between-group differences in observational studies
when the samples are random.

### 1. Formulating the Hypotheses {-}

There is only one general form of hypotheses for ANOVA. It is adapted
to each particular ANOVA depending on how many groups there are in a
particular analysis.

**NOTE:** $k$ = the number of groups


| **Hypotheses for Analysis of Variance (ANOVA)** |
|:-----------------------------------------------:|
| $H_0:\mu_1 = \mu_2 = \ldots = \mu_k$ <br> $H_A: Not\ all\ of\ the\ means\ are\ equal$ |
| **Answers questions about:** |
| Whether at least one group has a different mean than the others. |

**NOTES:**  

1.  $\mu_1, \mu_2,\ldots, \mu_k$ are the means of the dependent (response)
variable for each group.  
2.  $k$ = the number of groups                
3.  the alternative hypothesis may be stated as: $H_A: At\ least\ two\ of\ the\ population\ means\ are\ not\ equal.$   
These two forms of $H_A$ are equivalent to one another.  

<br>

### 2. The Test Statistics {-}

In order to understand the test statistic, we need to know something
about the process behind ANOVA. ANOVA estimates the variance of the
dependent variable two different ways, and compares those variance
estimates using an F test statistic. The two variance estimates are:

1)  The between-treatments estimate of the variance, also called the
    **Mean Square due to Treatments (MSTR).** The MSTR is the variance estimated under the assumption that $H_0$ is true.

2)  The within-treatments estimate of the variance, also called the
    **Mean Square due to Error (MSE).** The MSE is the variance estimated under the assumption that $H_0$ is false.  
    
The test statistic in ANOVA procedures is $F$, and it is the ratio of
the two variance estimates: the MSTR and the MSE. The by-hand
calculation of this test statistic is the subject of its own handout.
The equation is:

$$F_{test} = \frac{MSTR}{MSE}$$
with
<center>
numerator degrees of freedom = $df_1 = k - 1$  
denominator degrees of freedom = $df_2 = n_T - k$
</center>
where
<center>
$k$ = the number of groups  
$n_T$ = total sample size (of all the groups)

<br>

### 3.  Deciding whether or not to reject $\mathbf{H_0}$ {-}

ANOVA $F$ tests are always **one-tailed, upper tail
tests.** The reason for this is due to the nature of the
two variance estimates: the MSTR and the MSE. Here is how ANOVA works.
If $H_0$ is actually true and all the means of the groups are equal,
then both the MSTR and the MSE will yield similar estimates of the
true variance. Therefore, the ratio of MSTR and MSE (which is
$F$) will be a small number -- too small to cause us to reject
$H_0$. If, on the other hand, $H_0$ is actually false, and the
means of the groups are not all equal, the MSTR will vastly
overestimate the true variance, while the MSE will estimate the true
variance accurately. Therefore, the ratio of the MSTR and MSE (which
is $F$) will be a large number -- large enough to cause us to reject
$H_0$. We only want to reject the null when the difference between
MSTR and MSE is large, hence the use of the upper tail test.

<br>

<center> **When to Reject $H_0$ in ANOVA (the Test Statistic is $F_test$)**
</center>


| | **Always an Upper Tail Test** |
|-|--|
| **p-value approach** | Calculate the upper tail p-value of $F_{test}$ <br><br> If the UT p-value $\leq \alpha$, then reject $H_0$ and accept $H_A$. <br> If the UT p-value $> \alpha$, then do not reject $H_0$. $H_A$ is unsupported. |
| **CV approach**  |  If $F_{test} \geq F_{\alpha}$, then reject $H_{0}$ and accept $H_A$. <br> If $F_{test} < F_{\alpha}$, then do not reject $H_0$. $H_A$ is unsupported.      

**NOTES:**  

1. $F_{test}$ is the Test Statistic  
2. $F_{\alpha}$ is the Upper Tail Critical Value (from F table)
3. The numerator degrees of freedom = $k-1$,  
denominator degrees of freedom = $n_T-k$  
where   
$k$ = the number of groups  
$n_T$ = total sample size (of all the groups)

<br>

### 4. Interpreting the test {-}

<center> **How to Interpret an ANOVA**
</center>

| **When you** | **Interpretation** | 
|:---:|-----------|
| **Reject $\mathbf{H_0}$** | At the $\alpha$ significance level, we can conclude that not all of the population means are equal. Could also be stated: At the $\alpha$ significance level, we can conclude that at least one of the populations has a different mean than the others. |
| **Do not reject $\mathbf{H_0}$** | At the $\alpha$ significance level, we cannot conclude that any of the population means are different from one another. |
    
    
**NOTES:**    
1. When $H_0$  is rejected, you can then perform comparisons of pairs of means from the ANOVA to determine which one is different (or which ones are different)

<br>

### Assumptions Underlying ANOVA Tests {-}

ANOVA relies on **three assumptions:**

1)  For each population, the response variable (dependent variable) is
    normally distributed.

    Each treatment group in an experiment represents a potentially
    different population, and in each population, the response
    variable, also called the dependent variable, must be normally
    distributed. This holds true for groups in observational studies
    as well.

2)  The variance of the response variable is the same for all of the
    populations.

3)  The observations must be independent.

    a.  In experimental designs, this assumption is satisfied by the use
        of **random assignment** to place experimental units
        into treatment groups.

    b.  In observational designs, this assumption is satisfied by using
        **random sampling.**
        

## Calculating the ANOVA Tables {-}

We calculate the $F$ test statistic from the sample data. In ANOVA, this
is no easy task. It involves a series of new equations and careful attention to detail.

**NOTE:** A **balanced design** is one in which all the groups are the same
size (all have the same $n$). An **unbalanced design** is one in which at
least one group is a different size than the others. The book and
MindTap use two different sets of equations for calculating the
$F_{test}$ in ANOVA, and one set **only** works if you have a
balanced design. **The formulas in this handout will work properly no
matter which design is being used, so I recommend you just stick with
these.**

**For the following equations:** There are $k$ treatment groups. Each group has a sample size, called
$n_j$ for the jth treatment group (e.g. the second treatment group has
a sample size of $n_2$). Keep this notation in mind.

The $F$ test statistic has a **numerator** and a
**denominator** that must be calculated separately.

<br>

**Part One: The numerator of the F test statistic is the MSTR:**

Recall: the MSTR is the **Mean Square due to Treatments,** also
called the between-treatments estimate of the variance. This is
the estimate of the true variance under the assumption that
$H_0$ is true, which would mean that all the group population
means were equal.

Here are the **four** steps to calculate the $MSTR$:

1)  Calculate the total sample size, $n_T$:  
$$n_T = n_1 + n_2 + \ldots + n_k$$  **NOTE:** The total sample size, $n_T$ is just the sum of the sample sizes of each group  
<br><br>
2)  Calculate the overall sample mean, $\bar{x}$:  
$$\bar{x} = \frac{n_1{\bar{x}_1} + \ n_2{\bar{x}_2} + \ldots + n_k\bar{x}_k}{n_T}\ $$ where <center> $n_1, n_2, \ldots, n_k$ = sample size for each group  
$\bar{x}_1, \bar{x}_2, \ldots, \bar{x}_k$ = the sample mean for each group  
**NOTE:** $\bar{\bar{\bar{x}}}$ = the overall sample mean, also called the Grand Mean, or x-triple-bar  
<br><br>
3) Calculate the Sum of Squares due to Treatments, $SSTR$:  
$$SSTR = \ \sum_{j = 1}^{k}n_{j}(\bar{x}_j - \bar{\bar{\bar{x}}})^2$$ where <center> $n_j$ = sample size for group $j$  
$\bar{x}_j$ = the sample mean for group $j$  
$k$ = the number of groups </center>
<br><br>
4) Finally, use the $SSTR$ to calculate the $MSTR$:  
$$MSTR = \frac{SSTR}{k - 1}$$ where <center> $k$ = the number of groups </center>

<br><br><br>

**Part Two: The denominator of the F test statistic is the $\mathbf{MSE}$:**

The MSE is the **Mean Square due to Error,** also called the within-treatments estimate of the variance. This is the estimate of the true variance under the assumption that $H_0$ is false, which would mean that at least one group population mean was different than the others.

Here are the **two** steps to calculate the $MSE$:

1)  First, calculate the Sum of Squares due to Error, $SSE$:  
$$SSE = \sum_{j = 1}^{k}{(n_j - 1)(s_j^2)}$$ where <center> $n_j$ = the sample size of group $j$  
$s_j^2$ = the sample variance of group $j$  
$k$ = the number of groups
<br><br>
2) Second, use the $SSE$ to calculate the $MSE$:  
$$MSE = \frac{SSE}{n_T - k}$$
<br>

**Now calculate the test statistic, $\mathbf{F_{test}}$**
$$F_{test}=\frac{MSTR}{MSE}$$  with <center> numerator degrees of freedom = $df_1 = k - 1$  
denominator degrees of freedom = $df_2 = n_T - k$ </center>

<br>

In the process of calculating the ANOVA test statistic, we organize all
of the results of our calculations into an ANOVA table. ANOVA tables may
be labeled with different terminology (e.g. the book uses different
labels than Excel) but the numbers always represent the same
calculations.

**ANOVA Summary Table: Equations**

+---------+---------+---------+---------+---------+---------+---------+
| *       | **Sum   | **      | **Mean  | **F**   | **p-    | **C     |
| *Source | of      | Degrees | S       |         | value** | ritical |
| of      | Sq      | of      | quare** |         |         | value   |
| Vari    | uares** | Fr      |         |         |         | of F**  |
| ation** |         | eedom** |         |         |         |         |
+=========+=========+=========+=========+=========+=========+=========+
| **Treat | $$SST   | $$df    | $$M     | $$F     | Upper   | $$F     |
| ments** | R = \ \ | _{1} =  | STR = \ | _{\text | tail    | _{\alph |
|         | sum_{j  | k - 1$$ |  \frac{ | {test}} |         | a,UT}$$ |
|         | = 1}^{k |         | \text{S |  = \fra | p-value |         |
|         | }{n_{j} |         | STR}}{k | c{\text | for     |         |
|         | \ (\ove |         |  - 1}$$ | {MSTR}} |         |         |
|         | rline{x |         |         | {\text{ | $$F_{   |         |
|         | _{j}} - |         |         | MSE}}$$ | \text{t |         |
|         |  \ \ove |         |         |         | est}}$$ |         |
|         | rset{̿}{ |         |         |         |         |         |
|         | \overli |         |         |         |         |         |
|         | ne{x}}} |         |         |         |         |         |
|         | )^{2}$$ |         |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| **      | $       | $       | $$M     |         |         |         |
| Error** | $SSE =  | $df_{2} | SE = \f |         |         |         |
|         | \sum_{j |  = n_{T | rac{\te |         |         |         |
|         |  = 1}^{ | } - k$$ | xt{SSE} |         |         |         |
|         | k}{(n_{ |         | }{n_{T} |         |         |         |
|         | j} - 1) |         |  - k}$$ |         |         |         |
|         | (s_{j}^ |         |         |         |         |         |
|         | {2})}$$ |         |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| **      | $$SST   | $$n_{T  |         |         |         |         |
| Total** | = SSTR  | } - 1$$ |         |         |         |         |
|         | + SSE$$ |         |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+

**ANOVA Summary Table: Simplified**

+---------+---------+---------+---------+---------+---------+---------+
| *       | **Sum   | **      | **Mean  | **F**   | **p-    | **C     |
| *Source | of      | Degrees | S       |         | value** | ritical |
| of      | Sq      | of      | quare** |         |         | value   |
| Vari    | uares** | Fr      |         |         |         | of F**  |
| ation** |         | eedom** |         |         |         |         |
+=========+=========+=========+=========+=========+=========+=========+
| **Treat | $       | nu      | $       | $$F_{   | Upper   | $$F     |
| ments** | $\text{ | merator | $\text{ | \text{t | tail    | _{\alph |
|         | SSTR}$$ | $       | MSTR}$$ | est}}$$ |         | a,UT}$$ |
|         |         | df_{1}$ |         |         | p-value |         |
|         |         |         |         |         | for     |         |
|         |         |         |         |         |         |         |
|         |         |         |         |         | $$F_{   |         |
|         |         |         |         |         | \text{t |         |
|         |         |         |         |         | est}}$$ |         |
+---------+---------+---------+---------+---------+---------+---------+
| **      | $$\text | deno    | $$\text |         |         |         |
| Error** | {SSE}$$ | minator | {MSE}$$ |         |         |         |
|         |         | $       |         |         |         |         |
|         |         | df_{2}$ |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| **      | $$SST   | $$n_{T  |         |         |         |         |
| Total** | = SSTR  | } - 1$$ |         |         |         |         |
|         | + SSE$$ |         |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+

### Exercise: ANOVA {-}

*Exercise 1.* Let's suppose that the grocery store from Chapter 13:
Handout \#1 decided to perform the experiment. The objective of the
experiment is to see whether any one of the three different ad designs
causes people to buy more produce. The store randomly samples customers
from its database, and then randomly assigns 22 of those customers to
receive Weekly Ad A, 21 of the customers to receive Weekly Ad B, and 20
of the customers to receive Weekly Ad C. Weekly produce purchases by
each customer are tracked, and the sample mean and variance of weekly
produce spending for each group are calculated. For each group, the
weekly spending on produce is normally distributed, and the population
variances are assumed to be equal.

+---+-------------+-----------+------------------------+-----------------+
| j | Treatment   | $$n_{j}$$ | Sample Mean            | Sample Variance |
|   |             |           |                        |                 |
|   |             |           | $${\overline{x}}_{j}$$ | $${s_{j}}^{2}$$ |
+===+=============+===========+========================+=================+
| 1 | Weekly Ad A |           | 25.11                  | 9.21            |
+---+-------------+-----------+------------------------+-----------------+
| 2 | Weekly Ad B |           | 28.51                  | 10.05           |
+---+-------------+-----------+------------------------+-----------------+
| 3 | Weekly Ad C |           | 29.20                  | 9.91            |
+---+-------------+-----------+------------------------+-----------------+

Use ANOVA to test whether the means for the three treatment groups are
equal at an $\alpha\  = \ 0.05$ significance level. Use your results to
fill in the ANOVA table.

**ANOVA Table for Exercise 1: Grocery Store Ads**

  **Source of Variation**   **Sum of Squares**   **Degrees of Freedom**   **Mean Square**   **F**   **Critical value of F**
  ------------------------- -------------------- ------------------------ ----------------- ------- -------------------------
  **Treatments**                                                                                    
  **Error**                                                                                         
  **Total**                                                                                         

*\
*

*Exercise 2.* In a study with a fully randomized experimental design,
four treatments were applied to randomly assigned groups of experimental
units. The first treatment group had 12 experimental units, the second
treatment group had 9 experimental units, the third treatment group had
9 experimental units, and the fourth treatment group had 14 experimental
units.

Given the following ANOVA table, test whether the population means are
equal at the α = 0.01 significance level.

  **Source of Variation**   **Sum of Squares**   **Degrees of Freedom**   **Mean Square**   **F**   **Critical value of F**
  ------------------------- -------------------- ------------------------ ----------------- ------- -------------------------
  **Treatments**                                                                                    
  **Error**                 50                                                                      
  **Total**                 65          
  
## Fisher's Least Significant Difference: a Multiple Comparison Procedure {-}

If the null hypothesis in an ANOVA is rejected, then you may conclude
that not all the population means are equal. However, ANOVA by itself
cannot tell you *which* of the means are different. It could be only one
group that has a different mean with the rest being equal, or two
groups, or all of the groups could have different means. ANOVA is silent
in that respect.

Consider the case where there are four groups in an ANOVA analysis, so
the null and alternative hypotheses are:

$H_{0}:\ \mu_{1} = \mu_{2} = \mu_{3} = \mu_{4}$

$H_{A}:Not\ all\ the\ population\ means\ are\ equal$

If the null is rejected, then the question becomes: which mean is not
equal to the others? Are they all different, or is only one or two
different from the others? This is where *multiple comparison
procedures* come in. Multiple comparison procedures are only used [after
ANOVA confirms a difference]{.underline}, to find out which groups have
a different mean from the others.

Multiple comparison procedures test pairs of means, and you have to run
one test per pair. Mathematically, we need to ask:
$\text{is\ }\mu_{1} \neq \mu_{2}?\ \text{is\ }\mu_{2} \neq \mu_{3}?\ \text{is\ }\mu_{3} \neq \mu_{4}?$
is
$\mu_{1} \neq \mu_{3}?\ \text{is\ }\mu_{2} \neq \mu_{4}?\text{is}\ \mu_{1} \neq \mu_{4}?\ \ $Notice
how those questions look like alternative hypotheses in a two tailed
test? Well, that is exactly what they are used for in multiple
comparisons.

Fisher's Least Significant Difference (Fisher's LSD) procedure is
commonly used for multiple comparisons. Fisher's LSD is a special type
of two-tailed t test.

There are three different methods to use Fisher's LSD: a traditional t
test; a modified t test; and, a confidence interval. All three methods
will come to the same conclusion, because they are all different ways of
using the same information. All three ways are presented in this handout
and in 13.3 in the book, but in this class only **METHOD 2** will be
used on the homework and on exams.

**[*Method 1*: *Fisher's LSD as a series of traditional t
tests*]{.underline}*:***

1)  Take any two groups from the ANOVA (here they are labeled $i$ and
    $j$ just to differentiate them). Set an $\alpha$ significance level.

2)  The hypotheses are:

> $H_{0}:\ \mu_{i} = \mu_{j}$
>
> $H_{A}:\ \mu_{i} \neq \mu_{j}$

3)  The test statistic is:

> $t_{\text{test}} = \frac{{\overline{x}}_{i}\  - \ {\overline{x}}_{j}}{\sqrt{\text{MSE}\left( \frac{1}{n_{i}}\  + \ \frac{1}{n_{j}} \right)}}$

$$\text{where\ }$$

> ${\overline{x}}_{i} = the\ sample\ mean\ of\ group\ i$

$${\overline{x}}_{j} = the\ sample\ mean\ of\ group\ j$$

$$MSE = Mean\ Square\ Error\ from\ the\ ANOVA$$

$$n_{j} = sample\ size\ of\ the\ j^{\text{th}}\text{\ group}$$

> and the degrees of freedom are $df_{2} = n_{T} - k$ from the ANOVA

4)  The rejection rules are:

    a.  $p\text{-}value\ approach:\ Reject\ H_{0}\text{\ if\ }2T\ p\text{-}value \leq \ \alpha$

    b.  $Critical\ Value\ approach:Reject\ H_{0}\text{\ if\ }t_{\text{test}} \leq - t_{\frac{\alpha}{2}}\text{\ or\ }t_{\text{test}} \geq t_{\frac{\alpha}{2}}\ $

$$with\ df = n_{T}\ –k$$

$$n_{T} = total\ sample\ size,\ from\ the\ ANOVA$$

$$k = the\ number\ of\ groups\ in\ the\ ANOVA$$

> NOTE: the df here are the same df from the $\text{MSE}$ in the ANOVA

5)  If the null hypothesis is rejected, then the two group means
    $\mu_{i}\text{\ and\ }\mu_{j}\ $are different. If the null
    hypothesis is not rejected, then there is no evidence that
    $\mu_{i}\text{\ and\ }\mu_{j}\ $are different.

6)  Conduct one test for each possible pair of means from the ANOVA

***[Method 2: Fisher's LSD as a series of modified t tests]{.underline}:
(This is the one we will use in this course)***

Take any two groups from the ANOVA (here they are labeled $i$ and $j$
just to differentiate them). Set an $\alpha$ significance level.

1)  The hypotheses are:

> $H_{0}:\ \mu_{i} = \mu_{j}$
>
> $H_{A}:\ \mu_{i} \neq \mu_{j}$

2)  The test statistic is:

> ${\overline{x}}_{i}\  - \ {\overline{x}}_{j}$
>
> $\text{where}$
>
> ${\overline{x}}_{i} = the\ sample\ mean\ of\ group\ i$
>
> ${\overline{x}}_{j} = the\ sample\ mean\ of\ group\ j$

3)  The Least Significant Difference is:

> $LSD = t_{\alpha/2}\sqrt{\text{MSE}\left( \frac{1}{n_{i}}\  + \ \frac{1}{n_{j}} \right)}\text{\ where\ the\ }2T\ t_{\alpha/2}\text{\ Critical\ Value\ has\ }df_{2} = n_{T} - k$

$$\text{and}$$

$${n_{i} = sample\ size\ of\ the\ i^{\text{th}}\text{\ group}
}{n_{j} = sample\ size\ of\ the\ j^{\text{th}}\text{\ group}
}{n_{T} = the\ total\ sample\ size\ from\ the\ ANOVA
}{k = the\ number\ of\ groups\ in\ the\ ANOVA}$$

and the rejection rule is:

> $\text{If\ }\left| {\overline{x}}_{i}\  - \ {\overline{x}}_{j} \right| \geq LSD\ then\ reject\ H_{0}\text{\ and\ accept\ }H_{A}\text{.\ }$
>
> $\text{If\ }\left| {\overline{x}}_{i}\  - \ {\overline{x}}_{j} \right| < LSD\ then\ \text{do\ not}\text{\ reject\ }H_{0}\text{\ and\ conclude\ }H_{A}\text{\ is\ unsupported.\ \ \ }$

4)  If the null hypothesis is rejected, then the two group means
    $\mu_{i}\text{\ and\ }\mu_{j}$ are different. If the null hypothesis
    is not rejected, then there is no evidence that
    $\mu_{i}\text{\ and\ }\mu_{j}$ are different.

5)  Conduct one test for each possible pair of means from the ANOVA.
    Note that this method is very efficient to use for multiple
    comparisons in balanced designs (i.e. those in which the groups are
    all the same size). In balanced designs, you only have to compute
    the LSD once because it will be the same no matter which two groups
    you are testing. Then you can compute the difference between each
    pair of means and compare the absolute value of each difference to
    the LSD to see whether to reject the null. In unbalanced designs
    (i.e. those in which the groups are not all the same size), you will
    have to compute the LSD individually for each test.

***[Method 3: Fisher's LSD as a series of confidence
intervals]{.underline}:***

Take any two groups from the ANOVA (here they are labeled $i$ and $j$
just to differentiate them). Set an α significance level.

1)  The hypotheses are:

> $H_{0}:\ \mu_{i} = \mu_{j}$
>
> $H_{A}:\ \mu_{i} \neq \mu_{j}$

2)  Calculate a $100\left( 1 - \alpha \right)\%$ confidence interval for
    the difference between $\mu_{i}\text{\ and\ }\mu_{j}$ using the
    following equation:

> ${\overline{x}}_{i}\  - \ {\overline{x}}_{j}\  \pm LSD\ where\ $
>
> $LSD = t_{\frac{\alpha}{2}}\sqrt{\text{MSE}\left( \frac{1}{n_{i}}\  + \ \frac{1}{n_{j}} \right)}\ \ and\ the\ t\ critical\ value\ has\ df = n_{T} - k$

3)  The rejection rule is:

> $\text{Reject\ }H_{0}\text{\ if\ the\ confidence\ interval\ does\ not\ include\ zero.}$

4)  If the null hypothesis is rejected, then the two group means
    $\mu_{i}\text{\ and\ }\mu_{j}$ are different.

5)  Conduct one test for each possible unique pair of means from the
    ANOVA

**[\
]{.underline}**

*Example.* We are going to use Fisher's LSD to test the groups from the
Grocery Store Ad example on Ch 13: Handout \#4.

*Part 1.* Using Method 2, perform a Fisher's LSD procedure to test
whether the population mean produce spending for the group that received
Ad A is different than Ad B. Use an $\alpha = 0.05$ significance level.
If you can confirm a difference between the means, which group spent
more on average?

*Example, part 2:* Using Method 2, perform a Fisher's LSD procedure to
test whether the population mean produce spending for the group that
received Ad A is different than Ad C. Use an $\alpha = 0.05$
significance level. If you can confirm a difference between the means,
which group spent more on average?

*Example, part 3:* Using Method 2, perform a Fisher's LSD procedure to
test whether the population mean produce spending for the group that
received Ad B is different than Ad C. Use an $\alpha = 0.05$
significance level. If you can confirm a difference between the means,
which group spent more on average?

