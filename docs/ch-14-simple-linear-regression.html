<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Ch 14: Simple Linear Regression | Statistics Book</title>
  <meta name="description" content="Ch 14: Simple Linear Regression | Statistics Book" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Ch 14: Simple Linear Regression | Statistics Book" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Ch 14: Simple Linear Regression | Statistics Book" />
  
  
  

<meta name="author" content="Catherine Schmitt-Sands" />


<meta name="date" content="2020-07-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-13-experimental-design-and-analysis-of-variance.html"/>
<link rel="next" href="ch-15-multiple-regression.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!DOCTYPE html>
<!-- KaTeX requires the use of the HTML5 doctype. Without it, KaTeX may not render properly -->
<html>
  <head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
  </head>
</html>



<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Statistics Book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#definitions-and-notation"><i class="fa fa-check"></i>Definitions and Notation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ch-9-hypothesis-tests.html"><a href="ch-9-hypothesis-tests.html"><i class="fa fa-check"></i>Ch 9: Hypothesis Tests</a>
<ul>
<li class="chapter" data-level="" data-path="ch-9-hypothesis-tests.html"><a href="ch-9-hypothesis-tests.html#the-z-distribution"><i class="fa fa-check"></i>The z Distribution</a></li>
<li class="chapter" data-level="" data-path="ch-9-hypothesis-tests.html"><a href="ch-9-hypothesis-tests.html#hypothesis-testing-the-process"><i class="fa fa-check"></i>Hypothesis Testing: The Process</a></li>
<li class="chapter" data-level="" data-path="ch-9-hypothesis-tests.html"><a href="ch-9-hypothesis-tests.html#hypothesis-tests-about-a-single-population-mean"><i class="fa fa-check"></i>Hypothesis Tests about a Single Population Mean</a></li>
<li class="chapter" data-level="" data-path="ch-9-hypothesis-tests.html"><a href="ch-9-hypothesis-tests.html#hypothesis-tests-about-a-single-population-proportion"><i class="fa fa-check"></i>Hypothesis Tests about a Single Population Proportion</a></li>
<li class="chapter" data-level="" data-path="ch-9-hypothesis-tests.html"><a href="ch-9-hypothesis-tests.html#the-t-distribution"><i class="fa fa-check"></i>The t Distribution</a></li>
<li class="chapter" data-level="" data-path="ch-9-hypothesis-tests.html"><a href="ch-9-hypothesis-tests.html#hypotheses-about-a-single-population-proportion"><i class="fa fa-check"></i>Hypotheses about a Single Population Proportion</a></li>
<li class="chapter" data-level="" data-path="ch-9-hypothesis-tests.html"><a href="ch-9-hypothesis-tests.html#type-i-and-type-ii-error"><i class="fa fa-check"></i>Type I and Type II Error</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ch-10-inference-about-means-and-proportions-with-two-populations.html"><a href="ch-10-inference-about-means-and-proportions-with-two-populations.html"><i class="fa fa-check"></i>Ch 10: Inference About Means and Proportions with Two Populations</a>
<ul>
<li class="chapter" data-level="" data-path="ch-10-inference-about-means-and-proportions-with-two-populations.html"><a href="ch-10-inference-about-means-and-proportions-with-two-populations.html#notation-definitions"><i class="fa fa-check"></i>Notation &amp; Definitions</a></li>
<li class="chapter" data-level="" data-path="ch-10-inference-about-means-and-proportions-with-two-populations.html"><a href="ch-10-inference-about-means-and-proportions-with-two-populations.html#hypothesis-tests-about-the-difference-between-population-means"><i class="fa fa-check"></i>Hypothesis Tests about the Difference between Population Means</a></li>
<li class="chapter" data-level="" data-path="ch-10-inference-about-means-and-proportions-with-two-populations.html"><a href="ch-10-inference-about-means-and-proportions-with-two-populations.html#hypothesis-tests-about-the-difference-between-two-population-proportions"><i class="fa fa-check"></i>Hypothesis Tests about the Difference between Two Population Proportions</a></li>
<li class="chapter" data-level="" data-path="ch-10-inference-about-means-and-proportions-with-two-populations.html"><a href="ch-10-inference-about-means-and-proportions-with-two-populations.html#confidence-intervals"><i class="fa fa-check"></i>Confidence Intervals</a></li>
<li class="chapter" data-level="" data-path="ch-10-inference-about-means-and-proportions-with-two-populations.html"><a href="ch-10-inference-about-means-and-proportions-with-two-populations.html#confidence-intervals-for-the-difference-between-two-population-means"><i class="fa fa-check"></i>Confidence Intervals for the Difference between Two Population Means</a></li>
<li class="chapter" data-level="" data-path="ch-10-inference-about-means-and-proportions-with-two-populations.html"><a href="ch-10-inference-about-means-and-proportions-with-two-populations.html#confidence-intervals-for-the-difference-between-two-proportions"><i class="fa fa-check"></i>Confidence Intervals for the Difference Between Two Proportions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ch-11-inferences-about-population-variances.html"><a href="ch-11-inferences-about-population-variances.html"><i class="fa fa-check"></i>Ch 11: Inferences About Population Variances</a>
<ul>
<li class="chapter" data-level="" data-path="ch-11-inferences-about-population-variances.html"><a href="ch-11-inferences-about-population-variances.html#notation-definitions-1"><i class="fa fa-check"></i>Notation &amp; Definitions</a></li>
<li class="chapter" data-level="" data-path="ch-11-inferences-about-population-variances.html"><a href="ch-11-inferences-about-population-variances.html#hypothesis-tests-about-a-single-population-variance"><i class="fa fa-check"></i>Hypothesis Tests about a Single Population Variance</a></li>
<li class="chapter" data-level="" data-path="ch-11-inferences-about-population-variances.html"><a href="ch-11-inferences-about-population-variances.html#hypothesis-tests-about-two-population-variances"><i class="fa fa-check"></i>Hypothesis Tests about Two Population Variances</a></li>
<li class="chapter" data-level="" data-path="ch-11-inferences-about-population-variances.html"><a href="ch-11-inferences-about-population-variances.html#the-chi-square-distribution"><i class="fa fa-check"></i>The Chi-Square Distribution</a></li>
<li class="chapter" data-level="" data-path="ch-11-inferences-about-population-variances.html"><a href="ch-11-inferences-about-population-variances.html#the-f-distribution"><i class="fa fa-check"></i>The F distribution</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ch-12-tests-of-goodness-of-fit-and-independence.html"><a href="ch-12-tests-of-goodness-of-fit-and-independence.html"><i class="fa fa-check"></i>Ch 12: Tests of Goodness of Fit and Independence</a>
<ul>
<li class="chapter" data-level="" data-path="ch-12-tests-of-goodness-of-fit-and-independence.html"><a href="ch-12-tests-of-goodness-of-fit-and-independence.html#multinomial-probability-distributions"><i class="fa fa-check"></i>Multinomial Probability Distributions</a></li>
<li class="chapter" data-level="" data-path="ch-12-tests-of-goodness-of-fit-and-independence.html"><a href="ch-12-tests-of-goodness-of-fit-and-independence.html#goodness-of-fit-test-for-a-single-categorical-variable"><i class="fa fa-check"></i>Goodness-of-Fit Test for a Single Categorical Variable</a></li>
<li class="chapter" data-level="" data-path="ch-12-tests-of-goodness-of-fit-and-independence.html"><a href="ch-12-tests-of-goodness-of-fit-and-independence.html#test-of-independence-for-two-categorical-variables"><i class="fa fa-check"></i>Test of Independence for Two Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ch-13-experimental-design-and-analysis-of-variance.html"><a href="ch-13-experimental-design-and-analysis-of-variance.html"><i class="fa fa-check"></i>Ch 13: Experimental Design and Analysis of Variance</a>
<ul>
<li class="chapter" data-level="" data-path="ch-13-experimental-design-and-analysis-of-variance.html"><a href="ch-13-experimental-design-and-analysis-of-variance.html#experimental-design"><i class="fa fa-check"></i>Experimental Design</a></li>
<li class="chapter" data-level="" data-path="ch-13-experimental-design-and-analysis-of-variance.html"><a href="ch-13-experimental-design-and-analysis-of-variance.html#introduction-to-analysis-of-variance-anova"><i class="fa fa-check"></i>Introduction to Analysis of Variance (ANOVA)</a></li>
<li class="chapter" data-level="" data-path="ch-13-experimental-design-and-analysis-of-variance.html"><a href="ch-13-experimental-design-and-analysis-of-variance.html#calculating-the-anova-test-statistic-and-summary-table"><i class="fa fa-check"></i>Calculating the ANOVA Test Statistic and Summary Table</a></li>
<li class="chapter" data-level="" data-path="ch-13-experimental-design-and-analysis-of-variance.html"><a href="ch-13-experimental-design-and-analysis-of-variance.html#fishers-least-significant-difference-a-multiple-comparison-procedure"><i class="fa fa-check"></i>Fisher’s Least Significant Difference: a Multiple Comparison Procedure</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ch-14-simple-linear-regression.html"><a href="ch-14-simple-linear-regression.html"><i class="fa fa-check"></i>Ch 14: Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="" data-path="ch-14-simple-linear-regression.html"><a href="ch-14-simple-linear-regression.html#introduction-1"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="ch-14-simple-linear-regression.html"><a href="ch-14-simple-linear-regression.html#hypothesis-testing-in-simple-linear-regression"><i class="fa fa-check"></i>Hypothesis Testing in Simple Linear Regression</a></li>
<li class="chapter" data-level="" data-path="ch-14-simple-linear-regression.html"><a href="ch-14-simple-linear-regression.html#simple-linear-regression-what-it-all-means"><i class="fa fa-check"></i>Simple Linear Regression: What it all means</a></li>
<li class="chapter" data-level="" data-path="ch-14-simple-linear-regression.html"><a href="ch-14-simple-linear-regression.html#simple-linear-regression-an-example"><i class="fa fa-check"></i>Simple Linear Regression: An Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ch-15-multiple-regression.html"><a href="ch-15-multiple-regression.html"><i class="fa fa-check"></i>Ch 15: Multiple Regression</a>
<ul>
<li class="chapter" data-level="" data-path="ch-15-multiple-regression.html"><a href="ch-15-multiple-regression.html#multiple-regression"><i class="fa fa-check"></i>Multiple Regression</a></li>
<li class="chapter" data-level="" data-path="ch-15-multiple-regression.html"><a href="ch-15-multiple-regression.html#hypothesis-testing-for-significance-in-multiple-regression"><i class="fa fa-check"></i>Hypothesis Testing for Significance in Multiple Regression</a></li>
<li class="chapter" data-level="" data-path="ch-15-multiple-regression.html"><a href="ch-15-multiple-regression.html#how-to-handle-insignificant-xs"><i class="fa fa-check"></i>How to handle insignificant x’s</a></li>
<li class="chapter" data-level="" data-path="ch-15-multiple-regression.html"><a href="ch-15-multiple-regression.html#model-comparisons"><i class="fa fa-check"></i>Model Comparisons</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-14-simple-linear-regression" class="section level1 unnumbered" number="">
<h1>Ch 14: Simple Linear Regression</h1>
<div class="hero-image-container">
<p><img class= "hero-image" src="images/art/06.png"></p>
</div>
<div id="introduction-1" class="section level2 unnumbered" number="">
<h2>Introduction</h2>
<p>Simple linear regression is a statistical procedure that models the
relationship between two variables. The two variables are the dependent
variable (DV) called y, and the independent variable (IV), called x.</p>
<p>A statistical model is a simplified picture of the relationship between
variables that emphasizes key features and regular patterns. In simple
linear regression, we model the relationship between two variables as a
straight line.</p>
<p>Why do we build statistical models? It is to be able to explain what is
going on with the dependent variable, and to make predictions about the
future based on data in the past. For example, we can model the
relationship between a person’s level of education (x) and salary (y) to
predict how much money they will make – in fact we could use regression
on this problem and predict how much more people will make for each
additional year of education they have. We can model the durability of a
painted coating (y) based on the thickness of the coating (x). How thick
does the coating have to be to hit our durability target? Statistical
modeling can help with that. Regression in particular is for modeling
linear relationships. Other statistical models exist to model other
kinds of relationships.</p>
<p>The <strong>Regression Model</strong> is a straight line defined by the following
equation:</p>
<p><span class="math display">\[y = \ \beta_{0} + \beta_{1}x + \ \epsilon\]</span></p>
<blockquote>
<p>where</p>
</blockquote>
<p><span class="math display">\[{y = the\ dependent\ variable
}{x = the\ independent\ variable
}{\beta_{0} = the\ population\ intercept
}{\beta_{1} = the\ population\ slope\ on\ x,the\ coefficient\ on\ x\ 
}{\epsilon = random\ error
}\]</span></p>
<p>The regression model describes the relationship between x and y in the
population – so <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> are <strong>population
parameters.</strong> Since they are population parameters, they are not
directly measured. Rather, we use sample data to estimate these
population values, and draw conclusions about them using statistical
inference, like confidence intervals and hypothesis tests.</p>
<p>In regression, a random sample is taken from a population, and the data
in the sample is used to calculate a linear equation, called the
<strong>Estimated Regression Equation (ERE)</strong>:</p>
<p><span class="math display">\[\widehat{y} = b_{0} + b_{1}x\]</span></p>
<p><span class="math display">\[{\text{where\ }\widehat{y} = the\ expected\ or\ predicted\ value\ of\ y\ \left( \text{the\ mean\ of\ y} \right)
}{b_{0} = the\ sample\ intercept;the\ estimate\ of\ \beta_{0}
}{b_{1} = the\ sample\ slope;the\ estimate\ of\ \beta_{1}}\]</span></p>
<p>The Estimated Regression Equation gives us a way to calculate the
<strong>expected value of y at a given value of x.</strong> The expected value of y
at a given x is <strong>the long-run mean of y: the mean value of y at a given
x if many samples were taken from the same population.</strong> To state this
more formally, <span class="math inline">\(\widehat{y}\)</span> is a point estimate for E(y), the expected
value of y at a given x. It is often called the <strong>predicted value of
y</strong>.</p>
<p><strong><span class="underline">A Refresher on the Equation of a Line</span></strong></p>
<p>The equation of a line is:</p>
<p><span class="math display">\[y = b + mx\]</span></p>
<p><span class="math display">\[{\text{where\ b} = the\ intercept
}{m = the\ slope\ of\ the\ line}\]</span></p>
<p>The slope of the line:
<span class="math inline">\(m = \frac{\text{change\ in\ y}}{\text{change\ in\ x}}\)</span></p>
<p>When graphing a line, the vertical axis is the y axis, and the
horizontal axis is the x axis. Hence, the slope is often described as
<span class="math inline">\(m = \frac{\text{rise}}{\text{run}}\)</span> because it describes how much the
line rises up the y axis as it runs along the x axis.</p>
<p><strong><em>Example 1: Refresher 1.</em></strong> Graph the following equation. What is the
slope? Interpret the slope by stating in words how much y changes (and
in what direction) for a one unit change in x.</p>
<p><span class="math display">\[y = 1 + 0.5x\]</span></p>
<p><img src="media/image1.emf" style="width:3.71771in;height:3.45833in" /></p>
<p><strong><em>Example 2: Refresher 2.</em></strong> Graph the following equation. What is the
slope? Interpret the slope by stating in words how much y changes (and
in what direction) for a one unit change in x.</p>
<p><span class="math display">\[y = 8 - 2x\]</span></p>
<p><img src="media/image2.emf" style="width:3.27083in;height:4.64039in" /></p>
<p><strong><em>Example 3: Refresher 3.</em></strong> Graph the following equation. What is the
slope? Interpret the slope in this equation by stating in words how much
y changes (and in what direction) for a one unit change in x.</p>
<p><span class="math display">\[y = 5\]</span></p>
<p><img src="media/image3.emf" style="width:3.29167in;height:3.76872in" /></p>
<p><em><br />
</em></p>
<p><strong><span class="underline">And now back to regression…</span></strong></p>
<p>Now, data do not typically fall exactly on a nice straight line when
graphed. Real-world data looks more like a cloud of points. Linear
regression can only be used in cases where there is a plausible
straight-line relationship between x and y. If the relationship between
x and y is not linear, then you have to use some other technique to
model it. Scatterplots which graph each data point are an important tool
to recognize the relationships in your data.</p>
<p>Here is how data that would typically be modeled with regression might
look. Can you visualize the line that could model this data? Would it
have a positive or a negative slope?</p>
<p><strong>Figure 1.</strong></p>
<p><span class="chart"><span class="math display">\[CHART\]</span></span></p>
<p>Here is another example of data that could be modeled with linear
regression. Can you visualize the line that could model this data? Would
it have a positive or a negative slope?</p>
<p><strong>Figure 2.</strong></p>
<p><span class="chart"><span class="math display">\[CHART\]</span></span></p>
<p>What about this one?</p>
<p><strong>Figure 3.</strong></p>
<p><span class="chart"><span class="math display">\[CHART\]</span></span></p>
<p>Now let’s look at some examples of data you would <strong>NOT</strong> model with
linear regression. Do you see the nonlinear pattern?</p>
<p><strong>Figure 4.</strong></p>
<p><span class="chart"><span class="math display">\[CHART\]</span></span></p>
<p>If you modeled the data in <strong>Figure 4</strong> with linear regression, the
regression procedure would give you a straight line with a positive
slope. But that straight line would misrepresent the actual
relationship, which would cause your predictions to be way off! You as
the researcher are responsible for avoiding this mistake – no computer
is going to tell you not to make it. (NOTE: there are ways to transform
data like this so that it can be modeled – if you continue studying
statistics, you will find out about those). For now, protect yourself
against such errors by graphing scatterplots of your data.</p>
<p><strong>Figure 5.</strong></p>
<p><span class="chart"><span class="math display">\[CHART\]</span></span></p>
<p>This is a dangerous pattern, because regression would model this as a
horizontal line, or something close to that. And a horizontal line
(which means the slope is zero) indicates there is <strong>no relationship</strong>
between x and y. But there clearly is a relationship here – it is just
not a linear relationship.</p>
<p>Let’s look at some examples:</p>
<p><span class="math display">\[y = \ \beta_{0} + \beta_{1}x + \ \epsilon\]</span></p>
<p>The <strong>estimated regression equation (ERE)</strong> is the relationship between
x and y in the sample:</p>
<p><span class="math display">\[\widehat{y} = b_{0} + b_{1}x\]</span></p>
<p>Remember, <span class="math inline">\(\widehat{y}\)</span> is the <strong>average</strong> <span class="math inline">\(y\)</span> for a given <span class="math inline">\(x\)</span>, and is
called the <strong>expected</strong> or <strong>predicted</strong> value of <span class="math inline">\(y\)</span>.</p>
<p><strong><em>Example 4.</em></strong> Regression could be used to investigate the
relationship between weekly sales and weekly advertising.</p>
<p>To use regression to model this relationship, first designate the DV and
the IV.</p>
<p><span class="math display">\[{y = Weekly\ Sales\ \left( \$ \right)
}{x = Weekly\ Advertising\ \left( \$ \right)}\]</span></p>
<p>Then take a random sample of weeks and measure sales and advertising in
the prior week for each. The graph of the data looks like this. Do you
expect a positive or a negative slope on the regression line?:</p>
<p><span class="chart"><span class="math display">\[CHART\]</span></span></p>
<p>The sample data is used to calculate the following values:</p>
<p><span class="math display">\[{Intercept:\ b}_{0} = 267.30\]</span></p>
<p><span class="math display">\[Slope:b_{1} = 4.51\]</span></p>
<p><span class="math display">\[95\%\ confidence\ interval\ for\ the\ population\ slope,\ \beta_{1} = \lbrack 3.35,\ 5.66\rbrack\]</span></p>
<p>a) Write down the Estimated Regression Equation (ERE).</p>
<p>b) If $235 is spent on advertising, what would be the predicted sales?
Interpret this value.</p>
<p>c) What is the interpretation of the slope, <span class="math inline">\(b_{1}?\)</span></p>
<p>d) Interpret the 95% confidence interval for the population slope,
<span class="math inline">\(\beta_{1}\)</span></p>
<p><strong><em><br />
</em></strong></p>
<p><strong><em>Example 5.</em></strong> The owners of a restaurant think that as the
temperature of a certain drink increases, customer satisfaction will
decrease. A random sample of customers who order this drink is taken,
and the temperature of the drink along with the customer satisfaction
rating (0 to 100) is measured.</p>
<p>To use regression to model this relationship, first we designate the DV
and the IV.</p>
<p><span class="math display">\[{y = Customer\ Satisfaction\ Rating
}{x = Temperature\ (℉)}\]</span></p>
<p>Here is the scatterplot of the data collected:<span class="chart"><span class="math display">\[CHART\]</span></span></p>
<p>The sample data is used to calculate the following values:</p>
<p><span class="math display">\[{{Intercept:\ b}_{0} = 105.2
}{{Slope:\ b}_{1} = \  - 1.42}\]</span></p>
<p><span class="math display">\[95\%\ confidence\ interval\ for\ the\ population\ slope,\ \beta_{1} = \lbrack - 1.74,\  - 1.09\rbrack\]</span></p>
<p>a) Write down the Estimated Regression Equation (ERE)</p>
<p>b) If the drink is served at <span class="math inline">\(22℉\)</span>, what would be the predicted
customer satisfaction score? Interpret this value.</p>
<p>c) What is the interpretation of the slope, <span class="math inline">\(b_{1}?\)</span></p>
<p>d) Interpret the 95% confidence interval for the population slope,
<span class="math inline">\(\beta_{1}\)</span></p>
</div>
<div id="hypothesis-testing-in-simple-linear-regression" class="section level2 unnumbered" number="">
<h2>Hypothesis Testing in Simple Linear Regression</h2>
<p>The <strong>Regression Model</strong>,
<span class="math inline">\(\mathbf{y =}\mathbf{\beta}_{\mathbf{0}}\mathbf{+}\mathbf{\beta}_{\mathbf{1}}\mathbf{x + \epsilon}\)</span>,
gives the relationship between <span class="math inline">\(\text{x\ }\)</span>and <span class="math inline">\(y\)</span> in the
<span class="underline">population</span>. From the sample, regression calculates the
<strong>Estimated Regression Equation</strong>,
<span class="math inline">\(\widehat{\mathbf{y}}\mathbf{=}\mathbf{b}_{\mathbf{0}}\mathbf{+}\mathbf{b}_{\mathbf{1}}\mathbf{x}\)</span>,
which gives the relationship between <span class="math inline">\(\text{x\ }\)</span>and <span class="math inline">\(y\)</span> in the
<span class="underline">sample</span>.</p>
<p><strong>VERY IMPORTANT:</strong>
<span class="math inline">\(\mathbf{\ }\mathbf{b}_{\mathbf{1}}\mathbf{\text{\ is\ NOT\ the\ same\ as\ }}\mathbf{\beta}_{\mathbf{1}}\mathbf{\text{\ and\ }}\mathbf{b}_{\mathbf{0}}\mathbf{\text{\ is\ NOT\ the\ same\ as\ }}\mathbf{\beta}_{\mathbf{0}}\mathbf{!!!!!!}\)</span></p>
<p>You must use hypothesis tests to learn about the <span class="math inline">\(\beta^{&#39;}s\)</span> from the
<span class="math inline">\(b&#39;s\)</span></p>
<p>The following table summarizes the possible relationships between
<span class="math inline">\(\text{x\ }\)</span>and <span class="math inline">\(y:\)</span></p>
<table>
<thead>
<tr class="header">
<th align="left"><strong>Slope Coefficient</strong></th>
<th align="left"><strong>Relationship between</strong> <span class="math inline">\(\mathbf{\text{x\ }}\)</span><strong>and</strong> <span class="math inline">\(\mathbf{y}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math display">\[\text{If\ }\beta_{1} &gt; 0\]</span></td>
<td align="left">Positive linear relationship in the population</td>
</tr>
<tr class="even">
<td align="left"><span class="math display">\[\text{If\ }\beta_{1} &lt; 0\]</span></td>
<td align="left">Negative linear relationship in the population</td>
</tr>
<tr class="odd">
<td align="left"><span class="math display">\[\text{If\ }\beta_{1} = 0\]</span></td>
<td align="left">No relationship in the population</td>
</tr>
</tbody>
</table>
<p>In order to conclude that there <span class="underline">is</span> a relationship between
<span class="math inline">\(\text{x\ }\)</span>and <span class="math inline">\(y\ \)</span>in the population, we need to confirm that
<span class="math inline">\(\beta_{1} \neq 0\)</span>. But we cannot observe the population parameter
<span class="math inline">\(\beta_{1}\)</span> – we only have the sample statistic <span class="math inline">\(b_{1}.\)</span> How can we
draw conclusions about a population parameter from a sample?? We have to
use good old hypothesis testing!</p>
<p>If the hypothesis test confirms a relationship between x and y, we
typically say that <em>“the relationship between x and y is statistically
significant,”</em> or, <em>“there is a statistically significant relationship
between x and y.”</em></p>
<p><strong><span class="underline">Hypothesis Test for Significance of the Population
Slope</span></strong> <span class="math inline">\(\mathbf{\beta}_{\mathbf{1}}\)</span></p>
<ol style="list-style-type: decimal">
<li><strong><span class="underline">Formulating the Hypotheses</span>:</strong></li>
</ol>
<blockquote>
<p>There is only one form of hypotheses, because in regression the
question is: is the population slope different from zero? If the slope
of a line is zero, there is no relationship between x and y. If the
slope is either positive or negative, then there is a relationship
between x and y. Therefore, the question calls for a two-tailed test.</p>
</blockquote>
<table style="width:99%;">
<colgroup>
<col width="98%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Hypotheses about the Slope Coefficient</strong>
<span class="math inline">\(\mathbf{\beta}_{\mathbf{1}}\)</span> <strong>in Regression</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Two-Tailed Test</strong></td>
</tr>
<tr class="even">
<td><p><span class="math display">\[H_{0}:\ \beta_{1} = 0\]</span></p>
<p><span class="math display">\[H_{A}:\ \beta_{1} \neq 0\]</span></p></td>
</tr>
<tr class="odd">
<td>Answers the question:</td>
</tr>
<tr class="even">
<td><p>Whether the population slope is different from zero.</p>
<p>That is to say, whether there is a relationship between
<span class="math inline">\(\text{x\ and\ y}\)</span></p></td>
</tr>
</tbody>
</table>
<p><strong><span class="underline"><br />
</span></strong></p>
<ol start="2" style="list-style-type: decimal">
<li><strong><span class="underline">The Test Statistic</span>:</strong></li>
</ol>
<blockquote>
<p>The sampling distribution of <span class="math inline">\(b_{1}\)</span> is the <span class="math inline">\(t\)</span> distribution, and so
we use a <span class="math inline">\(t\)</span> test statistic. The test statistic is:</p>
</blockquote>
<p><span class="math display">\[t_{\text{test}} = \frac{b_{1}}{s_{b_{1}}}\]</span></p>
<p><span class="math display">\[{b_{1} = the\ sample\ slope\ coefficient,\ from\ the\ regression\ output
}{s_{b_{1}} = the\ standard\ error\ on\ the\ slope\ coefficient,\ from\ the\ regression\ output}\]</span></p>
<blockquote>
<p>and the degrees of freedom are <span class="math inline">\(df = n - p - 1\)</span></p>
</blockquote>
<p><span class="math display">\[{\text{where\ n} = the\ number\ of\ observations
}{p = the\ number\ of\ independent\ variables\ \left( x^{&#39;}s \right)\text{\ in\ the\ ERE}}\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><strong><span class="underline">Deciding whether or not to Reject</span></strong>
<span class="math inline">\(\mathbf{H}_{\mathbf{0}}\)</span><strong>:</strong></li>
</ol>
<table style="width:97%;">
<colgroup>
<col width="48%" />
<col width="48%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>For this Two-Tailed Test:</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>p-value approach:</strong></td>
<td><p>The two-tailed
<span class="math inline">\(p\text{-}\text{value}\)</span> is two
times the one-tailed probability
of <span class="math inline">\(t_{\text{test}}.\)</span></p>
<p>If the
<span class="math inline">\(2T \ p\text{-}value \leq \ \alpha,\)</span>
then reject <span class="math inline">\(H_{0}\)</span> and accept
<span class="math inline">\(H_{A}.\)</span></p>
<p>If the
<span class="math inline">\(2T\ p\text{-}value &gt; \ \alpha,\)</span>
then do not reject <span class="math inline">\(H_{0}\)</span>.
<span class="math inline">\(H_{A}\)</span> is unsupported.</p>
<p><strong>NOTE: the
<span class="underline">two-tailed</span></strong>
<span class="math inline">\(\mathbf{ p}\text{-}\mathbf{\text{value}}\)</span>
<strong>is reported by Excel in the
Regression Output!!</strong></p></td>
</tr>
<tr class="even">
<td><strong>Critical Value Approach:</strong></td>
<td><p>If
<span class="math inline">\(t_{\text{test}} \
leq {- t}_{\alpha/2}\text{\ OR}\)</span>
<span class="math inline">\(t_{ \text{test}} \geq t_{\alpha/2}\)</span>,
then reject <span class="math inline">\(H_{0}\)</span>and accept
<span class="math inline">\(H_{A}\)</span>.</p>
<p>If
<span class="math inline">\({- t}_{\alpha/2} &lt; t_{\text{test}} &lt; t_{\alpha/2}\)</span>,
then do not reject <span class="math inline">\(H_{0}\)</span>.
<span class="math inline">\(H_{A}\)</span> is unsupported.</p></td>
</tr>
<tr class="odd">
<td><p>NOTES:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(t_{\text{test}}\ \)</span>is a Test
Statistic</p></li>
<li><p><span class="math inline">\(\pm t_{\alpha/2}\)</span> are
Critical Values</p></li>
<li><p>The degrees of freedom are
<span class="math inline">\(df = n - p - 1\)</span></p></li>
</ol></td>
<td></td>
</tr>
</tbody>
</table>
<ol start="4" style="list-style-type: decimal">
<li><strong><span class="underline">Interpreting the test</span>:</strong></li>
</ol>
<blockquote>
<p>In the following interpretations, you should substitute in the actual
meaning of <span class="math inline">\(\text{x\ and\ y}\)</span>.</p>
</blockquote>
<table style="width:97%;">
<colgroup>
<col width="48%" />
<col width="48%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>When you:</strong></th>
<th><strong>The Interpretation is:</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Reject</strong>
<span class="math inline">\(\mathbf{H}_{\mathbf{0}}\)</span></td>
<td><p>At the <span class="math inline">\(\alpha\)</span> significance
level, we can conclude that the
population slope is different
from zero. Therefore, the
relationship between
<span class="math inline">\(\text{x\ and\ y}\)</span> is
statistically significant.</p>
<p>Further:</p>
<p>If <span class="math inline">\(b_{1} &gt; 0,\ \)</span>there is a
positive linear relationship
between <span class="math inline">\(\text{x\ and\ y.}\)</span></p>
<p>If <span class="math inline">\(b_{1} &lt; 0,\ \)</span>there is a
negative linear relationship
between <span class="math inline">\(\text{x\ and\ y.}\)</span></p></td>
</tr>
<tr class="even">
<td><strong>Do not reject</strong>
<span class="math inline">\(\mathbf{H}_{\mathbf{0}}\)</span></td>
<td>At the <span class="math inline">\(\alpha\)</span> significance
level, we cannot conclude that
the population slope is
different from zero. Therefore,
the relationship between
<span class="math inline">\(\text{x\ and\ y}\)</span> is
<span class="underline">not</span> statistically
significant.</td>
</tr>
</tbody>
</table>
</div>
<div id="simple-linear-regression-what-it-all-means" class="section level2 unnumbered" number="">
<h2>Simple Linear Regression: What it all means</h2>
<p>Simple linear regression is a statistical modeling procedure that models
the relationship between one Dependent Variable (DV), <span class="math inline">\(y,\)</span> and one
Independent Variable (IV), <span class="math inline">\(\text{x.}\)</span> A model is a simplified
representation that captures important characteristics but leaves out
many details. The model of the relationship between <span class="math inline">\(x\)</span> and
<span class="math inline">\(\text{y\ }\)</span>in linear regression is a <strong>straight line</strong>.</p>
<p>The linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> in the whole <strong>population of
interest</strong> is represented in the <strong>Regression Model</strong>:</p>
<p><span class="math display">\[y = \ \beta_{0} + \beta_{1}x + \ \epsilon\]</span></p>
<blockquote>
<p>where</p>
</blockquote>
<p><span class="math display">\[{y = the\ dependent\ variable
}{x = the\ independent\ variable
}{\beta_{0} = the\ population\ intercept,\ called\ beta\ nought\ or\ beta\ zero
}{\beta_{1} = the\ slope\ on\ x,the\ coefficient\ on\ x,\ called\ beta\ one
}{\epsilon = random\ error
}\]</span></p>
<p>The betas are population parameters, and as such, we cannot observe them
directly. Instead, we take a random sample from the population and use
the data from the sample to <strong>estimate</strong> the betas. That is what the
regression procedure does: it analyzes all the data in the sample and
uses it to calculate the <strong>Estimated Regression Equation</strong>:</p>
<p><span class="math display">\[\widehat{y} = b_{0} + b_{1}x\]</span></p>
<p><span class="math display">\[{\text{where\ }\widehat{y} = the\ expected\ or\ predicted\ value\ of\ y\ (the\ mean\ of\ y)\ 
}{b_{0} = the\ sample\ intercept;the\ estimate\ of\ \beta_{0}
}{b_{1} = the\ sample\ slope;the\ estimate\ of\ \beta_{1}}\]</span></p>
<p>Regression output contains many different quantities, all of which are
calculated from the data: that is, from the individual values of <span class="math inline">\(x\)</span> and
<span class="math inline">\(y\)</span> in the observations in the data set.</p>
<p>The data set will look like this, although the columns do not have to be
in this order:</p>
<table style="width:96%;">
<colgroup>
<col width="31%" />
<col width="31%" />
<col width="31%" />
</colgroup>
<thead>
<tr class="header">
<th><p><strong>Observation</strong></p>
<strong>
(</strong><span class="math inline">\(\mathbf{i}\)</span><strong>)</strong></th>
<th><p><strong>Dependent</strong></p>
<p><strong>Variable</strong></p>
<strong>(</strong><span class="math inline">\(\mathbf{y }_{\mathbf{i}}\)</span><strong>)</strong></th>
<th><p><strong>Independent</strong></p>
<p><strong>Variable</strong></p>
<strong>(</strong><span class="math inline">\(\mathbf{x }_{\mathbf{i}}\)</span><strong>)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math display">\[y_{1}\]</span></td>
<td><span class="math display">\[x_{1}\]</span></td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math display">\[y_{2}\]</span></td>
<td><span class="math display">\[x_{2}\]</span></td>
</tr>
<tr class="odd">
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="even">
<td><span class="math display">\[n\]</span></td>
<td><span class="math display">\[y_{n}\]</span></td>
<td><span class="math display">\[x_{n}\]</span></td>
</tr>
</tbody>
</table>
<p>The Observation column – which may be labeled by Case or Subject, or
whatever is appropriate for the study – is a unique identifier for each
observation in the dataset. Every observation in the dataset consists of
measurements for each variable. For example, if this dataset was for a
regression investigating income and years of education, each observation
would be an individual person (<span class="math inline">\(i\)</span>), and for each person, income
(<span class="math inline">\(y_{i}\)</span>) and years of education (<span class="math inline">\(x_{i}\)</span>) would be measured and
recorded. From the values of <span class="math inline">\(y_{i}\)</span> and <span class="math inline">\(x_{i}\)</span> for each observation
<span class="math inline">\(i\)</span>, we can calculate all of the values in the regression output.</p>
<p>Linear regression calculates the estimated regression equation by
employing something called the <strong>Least Squares Criterion</strong>, which
<span class="underline">minimizes</span> the squared vertical distance between each
observed value of <span class="math inline">\(y\)</span> in the sample (each <span class="math inline">\(y_{i}\)</span>) and the predicted <span class="math inline">\(y\)</span>
from the estimated regression equation (each <span class="math inline">\({\widehat{y}}_{i}\)</span>) at
every value of <span class="math inline">\(x_{i}\)</span>. This vertical distance between observed and
predicted (<span class="math inline">\(y_{i} - {\widehat{y}}_{i}\)</span>) is a very important quantity in
regression, called the Residual. Mathematically, the Least Squares
Criterion is stated:</p>
<p><span class="math display">\[Least\ Squares\ Criterion = min\sum_{}^{}\left( y_{i} - {\widehat{y}}_{i} \right)^{2}\]</span></p>
<p>So, the Least Squares Criterion minimizes the sum of the squared
residuals. <strong>Conceptually, the estimated regression equation that linear
regression calculates is the line that is as close as possible to all
the points in the data set at once: that is what it means to satisfy the
Least Squares Criterion.</strong></p>
<p>For linear regression with more than one independent variable, matrix
algebra or calculus is required to calculate the estimated regression
equation. The equations simplify in the case of Simple Linear
Regression, and so we could easily calculate the equation, and in fact
all the regression output, by hand – if we only had the time.</p>
<p>For now, let’s concentrate on learning what the numbers in the
regression output mean.</p>
<p><strong><span class="underline">Interpreting Linear Regression Output</span></strong></p>
<p>NOTE: in many of the interpretations, I refer to the DV as <span class="math inline">\(y\)</span> and the
IVs as <span class="math inline">\(x,\ x_{1},\ x_{2}\ \)</span>etc. When actually interpreting a
regression, these variables have meaning and the meaning should be
substituted in for these placeholder variables.</p>
<p><strong><span class="underline">In the <em>Regression Statistics</em> table (in order of
importance/informativeness)</span>:</strong></p>
<ol style="list-style-type: decimal">
<li><p><em>The <strong>Coefficient of Determination,
</strong></em><span class="math inline">\(\mathbf{R}^{\mathbf{2}}\mathbf{,\ }\)</span><em>(called <strong>R Square</strong> in
Excel) gives the proportion of the variability in the dependent
variable that is explained by the independent variable or
variables.</em> <span class="math inline">\(R^{2}\)</span> <em>varies from 0 to 1, and when interpreted, it is
converted to a percentage.</em></p>
<ol style="list-style-type: lower-alpha">
<li><em>In Simple Linear Regression:</em> <span class="math inline">\(R^{2}\)</span> <em>measures the proportion
of the variability in</em> <span class="math inline">\(\text{y\ }\)</span><em>that is explained by</em> <span class="math inline">\(x\)</span><em>.</em></li>
</ol></li>
</ol>
<blockquote>
<p><em><span class="underline">Example</span>: If</em> <span class="math inline">\(R^{2} = 0.7231,\)</span> <em>then the
<strong>interpretation</strong> is: 72.31% of the variability in</em> <span class="math inline">\(y\)</span> <em>is explained
by</em> <span class="math inline">\(\text{x.}\)</span></p>
</blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>In Multiple Regression: <span class="math inline">\(R^{2}\)</span> <em>measures the proportion of the
variability in</em> <span class="math inline">\(\text{y\ }\)</span><em>that is explained by all of the
independent variables,</em> <span class="math inline">\(x_{1},\ x_{2},\ \ldots,\ x_{p}\)</span><em>.</em></li>
</ol>
<blockquote>
<p><em><span class="underline">Example</span>: If</em> <span class="math inline">\(R^{2} = 0.4979,\)</span> <em>then the
<strong>interpretation</strong> is: 49.79% of the variability in</em> <span class="math inline">\(y\)</span> <em>is explained
by</em> <span class="math inline">\(x_{1},\ x_{2},\ldots,\ x_{p}.\)</span></p>
</blockquote>
<ol start="2" style="list-style-type: decimal">
<li><p>The <strong>Standard Error of the Estimate, s,</strong> often called the <strong>Root
Mean Square Error (RMSE)</strong>, measures the accuracy of the predictions
made by the Estimated Regression Equation. It is the average
distance that the observed <span class="math inline">\(y\)</span> values in the sample fall from the
regression line.</p>
<ol style="list-style-type: lower-alpha">
<li><strong>Interpretation:</strong> the average error we would make using the
estimated regression equation to predict <span class="math inline">\(y\)</span>. In other words: if
we used the estimated regression equation to predict <span class="math inline">\(y\)</span>, we
would be off by <strong>s</strong> on average.</li>
</ol></li>
</ol>
<blockquote>
<p><span class="underline">Example</span>: If <span class="math inline">\(s = 4\ units,\)</span> then the <strong>interpretation</strong>
is: <span class="math inline">\(4\ units\)</span> <em>is the average error we would make if we used the
estimated regression equation to predict</em> <span class="math inline">\(\text{y.}\)</span></p>
<p>Another way to state the same thing is: <em>If we used the estimated
regression equation to predict y, we would be off by 4 units on
average.</em></p>
</blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>The Standard Error of the Estimate is in the <span class="underline">same units
as</span> <span class="math inline">\(y\)</span><span class="underline">,</span> which makes it very easy to
understand and interpret.</p></li>
<li><p>The lower the Standard Error of the Estimate, the more accurate the
predictions, so the lower the better!</p></li>
<li><p>Given two regression models that predict the same DV measured in the
same units, <strong>the Standard Error of the Estimate can be used to
choose between models</strong>: <strong>whichever one has the lower Standard
Error of the Estimate is the better model</strong>, because the predictions
made with it will be more accurate.</p></li>
</ol>
<!-- -->
<ol start="3" style="list-style-type: decimal">
<li><p><strong>Adjusted</strong>
<span class="math inline">\(\mathbf{R}^{\mathbf{2}}\mathbf{\ (notated\ }\mathbf{R}_{\mathbf{A}}^{\mathbf{2}}\mathbf{)}\)</span>
is used as a model comparison statistic in <span class="underline">multiple
regression</span>. When comparing two models that predict the
same DV in the same units, the one with the higher <strong>Adjusted</strong>
<span class="math inline">\(\mathbf{R}^{\mathbf{2}}\)</span> is the better model.</p>
<ol style="list-style-type: lower-alpha">
<li>NOTE: it is <strong>not</strong> appropriate to use regular old <span class="math inline">\(R^{2}\)</span> to
compare two models, because adding another IV to a regression
model will increase <span class="math inline">\(R^{2}\)</span>, whether that IV explains anything
or not. <strong>Adjusted</strong> <span class="math inline">\(\mathbf{R}^{\mathbf{2}}\)</span><strong>,</strong> on the other
hand, will only increase if the additional IV adds explanatory
power, and will actually decrease if it does not.</li>
</ol></li>
<li><p>The <strong>Correlation Coefficient</strong> (called <strong>Multiple R</strong> in Excel) is
denoted <span class="math inline">\(R_{\text{xy}}\)</span> in simple linear regression and <span class="math inline">\(R_{ŷy}\)</span> <em>in
multiple regression. It gives a descriptive measure of the strength
of the linear association between</em> <span class="math inline">\(y\)</span> <em>and</em> <span class="math inline">\(x\)</span> <em>(in simple linear
regression) and between</em> <span class="math inline">\(y\)</span> <em>and all the</em> <span class="math inline">\(x\text{&#39;}s\)</span> <em>in multiple
regression.</em></p>
<ol style="list-style-type: lower-alpha">
<li><p>The closer to 1, the stronger the association. Values close to 0
indicate that x and y are not linearly related.</p></li>
<li><p>No rules are set in stone about what constitutes a “strong” or
“weak” relationship. A common rule of thumb: &lt; 0.25: weak
linear association; between 0.25 and 0.75: moderate linear
association; &gt; 0.75: strong linear association</p></li>
</ol></li>
</ol>
<p><strong><span class="underline">The <em>ANOVA</em> table:</span></strong></p>
<ul>
<li><p>recall: the sample data is our best representation of the underlying
population, so if our model is good at predicting the sample values,
we can infer it will also be good at predicting population values</p></li>
<li><p>The ANOVA table in the regression output compares two different
methods of predicting the sample data in an effort to help us decide
if our regression model is any good.</p>
<ul>
<li><p>Premise: there are two alternative models you could use to
predict <span class="math inline">\(y\)</span>. One model includes information about <span class="math inline">\(x\)</span> and one
does not:</p>
<ul>
<li><p>First, you could use the Estimated Regression Equation to
predict <span class="math inline">\(y\)</span> (in other words, predict <span class="math inline">\({\widehat{y}}_{i}\)</span>
<span class="math inline">\(\ \)</span>at each <span class="math inline">\(x_{i}\)</span>)</p></li>
<li><p>Second, you could use the next best option, which is to just
use <span class="math inline">\(\overline{y}\)</span> to predict <span class="math inline">\(y\)</span> (in other words, predict
<span class="math inline">\(\overline{y}\)</span> (the mean of y) at each <span class="math inline">\(x_{i}\)</span>)</p>
<ul>
<li>This is also called the ‘Intercept only model’ because
the equation of <span class="math inline">\(\text{y\ }\)</span>does not include any <span class="math inline">\(x\)</span>
variables, thus being only an intercept, and
representing a horizontal line at <span class="math inline">\(y = \ \overline{y}\)</span>.</li>
</ul></li>
</ul></li>
<li><p>In the ANOVA table, you calculate the total errors you would
make if you predicted the actual observed data using each of
these models, square those errors, and sum them. In this way, we
can compare the two possible models and decide whether our
regression model is worth using.</p></li>
</ul></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>The <strong>Sum of Squares due to Error,</strong>
<span class="math inline">\(\mathbf{\text{SSE}},\ \)</span>sometimes called the <strong>Residual Sum of
Squares,</strong> is the total amount of prediction error we would make
using the estimated regression equation to predict the observed
sample data. It is the sum of the squared <strong>residuals.</strong></p>
<ol style="list-style-type: lower-alpha">
<li>A <strong>residual</strong> is the <strong>difference between the
<span class="underline">observed</span> and <span class="underline">predicted</span></strong>
<span class="math inline">\(\mathbf{y}_{\mathbf{i}}\)</span> <strong>at a given</strong>
<span class="math inline">\(\mathbf{x}_{\mathbf{i}}\text{.\ }\)</span>In other words:</li>
</ol></li>
</ol>
<p><span class="math display">\[\mathbf{\text{At\ any\ given\ }}\mathbf{x}_{\mathbf{i}}\mathbf{,\ the\ residual =}\mathbf{y}_{\mathbf{i}}\mathbf{-}{\widehat{\mathbf{y}}}_{\mathbf{i}}\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>The <span class="math inline">\(SSE = \sum_{}^{}\left( y_{i} - {\widehat{y}}_{i} \right)^{2}\)</span>,
and has degrees of freedom <span class="math inline">\(df_{2} = n - p - 1\)</span></p></li>
<li><p>Note: the sum of the squared residuals is the quantity that is
minimized by the Least Squares Criterion!</p></li>
</ol>
<!-- -->
<ol start="2" style="list-style-type: decimal">
<li><p>The <strong>Total Sum of Squares,</strong> <span class="math inline">\(\mathbf{SST,}\ \)</span>is the total amount
of prediction error we would make if we used <span class="math inline">\(\overline{y}\)</span> (the
mean of <span class="math inline">\(y_{i}\)</span>) to predict the observed sample data.</p>
<ol style="list-style-type: lower-alpha">
<li>The <span class="math inline">\(SST = \sum_{}^{}\left( y_{i} - \overline{y} \right)^{2}\)</span>,
and has degrees of freedom <span class="math inline">\(df = n - 1\)</span></li>
</ol></li>
<li><p>The <strong>Sum of Squares due to Regression,</strong> <span class="math inline">\(\mathbf{SSR,}\)</span> is how
much we will reduce prediction error by using the Estimated
Regression Equation instead of the mean to predict the observed
sample data.</p>
<ol style="list-style-type: lower-alpha">
<li>The
<span class="math inline">\(SSR = \sum_{}^{}\left( {\widehat{y}}_{i} - \overline{y} \right)^{2}\)</span>,
and has degrees of freedom <span class="math inline">\(df_{1} = p\)</span></li>
</ol></li>
<li><p>The <strong>Mean Square Error,</strong> <span class="math inline">\(\mathbf{MSE,}\)</span> is the estimate of the
variance of <span class="math inline">\(\epsilon\)</span>. Remember <span class="math inline">\(\epsilon?\)</span> It is the random error
term in the Regression Model:
<span class="math inline">\(y = \beta_{0} + \beta_{1}x + \epsilon\)</span></p>
<ol style="list-style-type: lower-alpha">
<li>The <span class="math inline">\(MSE = \frac{\text{SSE}}{n - p - 1}\)</span></li>
</ol></li>
</ol>
<p><strong><span class="underline"><br />
</span></strong></p>
<p><strong><span class="underline">The <em>Coefficients</em> table:</span></strong></p>
<ol style="list-style-type: decimal">
<li><p>The <span class="math inline">\(\mathbf{\text{sample\ slope\ coefficients}}\)</span><strong>,</strong>
<span class="math inline">\(\mathbf{b}_{\mathbf{1}}\mathbf{,\ }\mathbf{b}_{\mathbf{2}}\mathbf{,\ldots,\ }\mathbf{b}_{\mathbf{p}}\)</span>
<strong>,</strong> quantify the magnitude and direction of the relationship
between each IV and the DV. They are point estimates, respectively,
of <span class="math inline">\(\beta_{1},\ \beta_{2},\ldots,\ \beta_{p}.\)</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>In Simple Linear Regression, the <strong>interpretation</strong> <strong>of</strong>
<span class="math inline">\(\mathbf{b}_{\mathbf{1}}\)</span> is:</p>
<ol style="list-style-type: lower-roman">
<li><strong>If</strong> <span class="math inline">\(\mathbf{b}_{\mathbf{1}}\mathbf{&gt; 0}:\)</span></li>
</ol></li>
</ol></li>
</ol>
<blockquote>
<p><em>For every one unit increase in</em> <span class="math inline">\(x\)</span><em>,</em> <span class="math inline">\(\text{y\ }\)</span><em>is predicted to
increase by</em> <span class="math inline">\(b_{1}\)</span> <em>on average.</em></p>
</blockquote>
<ol start="2" style="list-style-type: lower-roman">
<li><strong>If</strong> <span class="math inline">\(\mathbf{b}_{\mathbf{1}}\mathbf{&lt; 0}:\)</span></li>
</ol>
<blockquote>
<p><em>For every one unit increase in</em> <span class="math inline">\(x\)</span><em>,</em> <span class="math inline">\(\text{y\ }\)</span><em>is predicted to
decrease by</em> <span class="math inline">\(b_{1}\)</span> <em>on average.</em></p>
</blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>In Multiple Regression, each slope is interpreted. The
interpretation for <span class="math inline">\(b_{1}\)</span> is given here, and the interpretation of
the other slopes follows the same pattern. The <strong>interpretation</strong>
<strong>of</strong> <span class="math inline">\(\mathbf{b}_{\mathbf{1}}\)</span> is:</p>
<ol style="list-style-type: lower-roman">
<li><strong>If</strong> <span class="math inline">\(\mathbf{b}_{\mathbf{1}}\mathbf{&gt; 0}:\)</span></li>
</ol></li>
</ol>
<blockquote>
<p><em>For every one unit increase in</em> <span class="math inline">\(x\)</span><em>,</em> <span class="math inline">\(\text{y\ }\)</span><em>is predicted to
increase by</em> <span class="math inline">\(b_{1}\)</span> <em>on average, holding all other independent
variables constant.</em></p>
</blockquote>
<ol start="2" style="list-style-type: lower-roman">
<li><strong>If</strong> <span class="math inline">\(\mathbf{b}_{\mathbf{1}}\mathbf{&lt; 0}:\)</span></li>
</ol>
<blockquote>
<p><em>For every one unit increase in</em> <span class="math inline">\(x\)</span><em>,</em> <span class="math inline">\(\text{y\ }\)</span><em>is predicted to
decrease by</em> <span class="math inline">\(b_{1}\)</span> <em>on average, holding all other independent
variables constant.</em></p>
</blockquote>
<ol start="2" style="list-style-type: decimal">
<li><p>The <span class="math inline">\(\mathbf{sample\ intercept,\ }\mathbf{b}_{\mathbf{0}},\)</span> is the
value of <span class="math inline">\(\text{y\ }\)</span>when <span class="math inline">\(x = 0.\)</span> This is not always interpretable
in the context of a given regression. Interpretability depends on
whether the value <span class="math inline">\(x = 0\ \)</span>has substantive meaning.</p></li>
<li><p>The
<span class="math inline">\(\mathbf{\text{confidence\ intervals\ for\ the\ slope\ coefficients}}\)</span><strong>,</strong>
<span class="math inline">\(\mathbf{\beta}_{\mathbf{1}}\mathbf{,\ }\mathbf{\beta}_{\mathbf{2}}\mathbf{,\ \ldots,\ }\mathbf{\beta}_{\mathbf{p}}\)</span><strong>,</strong>
and the <span class="math inline">\(\mathbf{intercept,\ }\mathbf{\beta}_{\mathbf{0}}\mathbf{,}\)</span>
are based on the <span class="math inline">\(\alpha\)</span> significance level and are interpreted
similarly to the other confidence intervals we have encountered:</p>
<ol style="list-style-type: lower-alpha">
<li>For a given <strong>slope coefficient, say</strong>
<span class="math inline">\(\mathbf{\beta}_{\mathbf{1}}\mathbf{,}\)</span> the <strong>interpretation</strong>
is:</li>
</ol></li>
</ol>
<blockquote>
<p><em>We can be</em> <span class="math inline">\(\_\_\_\%\ \)</span><em>confident that the true value of the
population slope coefficient for</em> <span class="math inline">\(x_{1}\)</span> <em>is between <span class="math display">\[lower bound\]</span>
and <span class="math display">\[upper bound\]</span>.</em></p>
</blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>For the <strong>intercept,</strong> <span class="math inline">\(\mathbf{\beta}_{\mathbf{0}}\)</span><strong>,</strong> the
<strong>interpretation</strong> is:</li>
</ol>
<blockquote>
<p><em>We can be</em> <span class="math inline">\(\_\_\_\%\ \)</span><em>confident that the true value of the
population intercept is between <span class="math display">\[lower bound\]</span> and <span class="math display">\[upper bound\]</span>.</em></p>
</blockquote>
</div>
<div id="simple-linear-regression-an-example" class="section level2 unnumbered" number="">
<h2>Simple Linear Regression: An Example</h2>
<p>We begin with a <strong>research question</strong> defining what we are investigating
with regression: Does the time students spend on MindTap help explain
(that is to say, predict) their grades?</p>
<p>We will use an <span class="math inline">\(\alpha = 0.01\)</span> significance level for this regression
analysis.</p>
<p>This question defines our variables and the relationship that we are
investigating. Here the variables are:</p>
<ul>
<li><p><strong>Dependent variable</strong>: <span class="math inline">\(y = Overall\ MindTap\ Score\ (\%\ points)\)</span></p></li>
<li><p><strong>Independent variable</strong>: <span class="math inline">\(x = Time\ spent\ logged\ on\ (hrs)\)</span></p></li>
</ul>
<p>The dataset collected is a random sample of <span class="math inline">\(n = 24\)</span> students enrolled
in BA 3400. For each student, time spent logged on to MindTap (in hours)
and overall MindTap score (in points) was recorded. Here is the dataset:</p>
<table>
<thead>
<tr class="header">
<th align="left"><strong>Student #</strong></th>
<th align="left"><strong>Hours</strong></th>
<th align="left"><strong>Score</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">20.85</td>
<td align="left">88.80</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">10.65</td>
<td align="left">85.40</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">25.72</td>
<td align="left">99.60</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">7.75</td>
<td align="left">68.10</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">18.28</td>
<td align="left">78.40</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">11.62</td>
<td align="left">75.10</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="left">17.30</td>
<td align="left">78.50</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left">15.03</td>
<td align="left">84.30</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="left">9.60</td>
<td align="left">77.20</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="left">10.93</td>
<td align="left">90.40</td>
</tr>
<tr class="odd">
<td align="left">11</td>
<td align="left">14.02</td>
<td align="left">82.20</td>
</tr>
<tr class="even">
<td align="left">12</td>
<td align="left">15.25</td>
<td align="left">91.10</td>
</tr>
<tr class="odd">
<td align="left">13</td>
<td align="left">17.72</td>
<td align="left">98.50</td>
</tr>
<tr class="even">
<td align="left">14</td>
<td align="left">9.57</td>
<td align="left">71.90</td>
</tr>
<tr class="odd">
<td align="left">15</td>
<td align="left">14.60</td>
<td align="left">86.00</td>
</tr>
<tr class="even">
<td align="left">16</td>
<td align="left">16.45</td>
<td align="left">85.60</td>
</tr>
<tr class="odd">
<td align="left">17</td>
<td align="left">6.77</td>
<td align="left">63.00</td>
</tr>
<tr class="even">
<td align="left">18</td>
<td align="left">12.07</td>
<td align="left">85.90</td>
</tr>
<tr class="odd">
<td align="left">19</td>
<td align="left">13.00</td>
<td align="left">83.20</td>
</tr>
<tr class="even">
<td align="left">20</td>
<td align="left">22.00</td>
<td align="left">97.30</td>
</tr>
<tr class="odd">
<td align="left">21</td>
<td align="left">4.92</td>
<td align="left">81.80</td>
</tr>
<tr class="even">
<td align="left">22</td>
<td align="left">14.67</td>
<td align="left">86.10</td>
</tr>
<tr class="odd">
<td align="left">23</td>
<td align="left">22.00</td>
<td align="left">87.00</td>
</tr>
<tr class="even">
<td align="left">24</td>
<td align="left">24.90</td>
<td align="left">90.50</td>
</tr>
</tbody>
</table>
<p>After the data collection step, the next thing to do is to graph a
scatterplot of Scores vs Time Spent. In the scatterplot, we are chiefly
concerned with non-linear patterns – if we see those, then we cannot
use linear regression to analyze this data. We may also be able to
identify a trend in the data that will give us an idea about the
relationship between these two variables. Here is the scatterplot:</p>
<p><span class="chart"><span class="math display">\[CHART\]</span></span></p>
<p>Do you see any nonlinear patterns?</p>
<p>Do you see a trend in the data? What sign do you expect on the slope of
the regression line?</p>
<p>At this point, we can run the regression in Excel (Demo). See the next
page for the results!</p>
<p>SUMMARY OUTPUT DV: MindTap Score<br />
————————- ——————- —————— ———- ———– —————— ————- ————— —————</p>
<p><em>Regression Statistics</em><br />
Multiple R 0.6917<br />
R Square 0.4785<br />
Adjusted R Square 0.4548<br />
Standard Error 6.6202<br />
Observations 24</p>
<p>ANOVA<br />
<em> </em> <em>df</em> <em>SS</em> <em>MS</em> <em>F</em> <em>Significance F</em><br />
Regression 1 884.5979 884.5979 20.1839 0.0002<br />
Residual 22 964.1917 43.8269<br />
Total 23 1848.79</p>
<p><em> </em> <em>Coefficients</em> <em>Standard Error</em> <em>t Stat</em> <em>P-value</em> <em>Lower 95%</em> <em>Upper 95%</em> <em>Lower 99.0%</em> <em>Upper 99.0%</em>
Intercept 67.4709 3.9186 17.2182 2.97E-14 59.3443 75.5976 56.4254 78.5165
Hours 1.1151 0.2482 4.4927 0.0002 0.6004 1.6299 0.4155 1.8148</p>
<p>Using the Regression Output Equations Roadmap, identify <span class="math inline">\(b_{0}\)</span> and
<span class="math inline">\(b_{1}\)</span> on this output. Then write the Estimated Regression Equation
(Remember, the ERE is
<span class="math inline">\(\widehat{\mathbf{y}}\mathbf{=}\mathbf{b}_{\mathbf{0}}\mathbf{+}\mathbf{b}_{\mathbf{1}}\mathbf{x}\)</span>):</p>
<p>Before we can use this equation for prediction or interpret the slope,
we must confirm that there is a statistically significant relationship
between MindTap Score (y) and Time Spent (x) <strong>in the population</strong>.
(Remember, if the slope of a line is zero, then there is no relationship
between x and y. If the slope of a line is a number other than zero,
then there <em>is</em> a relationship between x and y.)</p>
<p>Here is the Coefficients Table again:</p>
<table>
<thead>
<tr class="header">
<th align="left"><em> </em></th>
<th align="left"><em>Coefficients</em></th>
<th align="left"><em>Standard Error</em></th>
<th align="left"><em>t Stat</em></th>
<th align="left"><em>P-value</em></th>
<th align="left"><em>Lower 95%</em></th>
<th align="left"><em>Upper 95%</em></th>
<th align="left"><em>Lower 99.0%</em></th>
<th align="left"><em>Upper 99.0%</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">67.4709</td>
<td align="left">3.9186</td>
<td align="left">17.2182</td>
<td align="left">2.97E-14</td>
<td align="left">59.3443</td>
<td align="left">75.5976</td>
<td align="left">56.4254</td>
<td align="left">78.5165</td>
</tr>
<tr class="even">
<td align="left">Hours</td>
<td align="left">1.1151</td>
<td align="left">0.2482</td>
<td align="left">4.4927</td>
<td align="left">0.0002</td>
<td align="left">0.6004</td>
<td align="left">1.6299</td>
<td align="left">0.4155</td>
<td align="left">1.8148</td>
</tr>
</tbody>
</table>
<p>The sample slope <span class="math inline">\(b_{1}\)</span> is 1.1151 which is definitely not zero… so
that means there is a relationship between Hours and Score, right? Not
necessarily! That sample slope just reflects what is going on in this
sample of 24 students. We have to use this sample slope to prove that
the <strong>population slope</strong> <span class="math inline">\(\mathbf{\beta}_{\mathbf{1}}\)</span> is not zero. If
proven, then that would mean there is a statistically significant
relationship between Hours and Score in the <strong>whole population of BA
3400 students</strong>, not just in these 24 students that were in the sample.
That is what we need to prove before using the regression to explain or
predict anything.</p>
<p>Perform the hypothesis test detailed in <strong>Ch 14: Handout #2</strong> to
determine whether there is a statistically significant relationship
between Hours and Score in the population:</p>
<p>Now that we have confirmed that Hours and Score are related in the
population, we can use our Estimated Regression Equation (ERE) to
predict student scores, and we can interpret the slope <span class="math inline">\(b_{1}.\)</span></p>
<p>First, let’s write down the ERE again:</p>
<p>If 11 hours are spent on MindTap, what is the predicted score? Interpret
this value.</p>
<p>Interpret the slope <span class="math inline">\(b_{1}.\)</span></p>
<p>Report and interpret the confidence interval for the population slope
<span class="math inline">\(\beta_{1}.\)</span> Since we are doing this regression at the <span class="math inline">\(\alpha = 0.01\)</span>
significance level, which is a 99% confidence level, we should report
and interpret the 99% confidence interval:</p>
<p><strong><span class="underline">But there is more to regression than that…</span></strong></p>
<p>The existence of a significant relationship between y and x is a
necessary step – after all, if you cannot confirm a relationship
between the IV and the DV, then the regression is no good for anything
– but it is not sufficient to stop there. We need a way to judge the
quality of the model. How well does it fit the data? Does it make
accurate predictions? How much of the variation in y does x explain?
Such questions are answered in the Regression Statistics table. But to
understand the Regression Statistics table, we need to understand how
the regression line (the estimated model) is calculated.</p>
<p>Linear regression calculates the estimated regression equation which
<span class="underline">minimizes</span> the squared vertical distance between each
observed value of <span class="math inline">\(y\)</span> in the sample (each <span class="math inline">\(y_{i}\)</span>) and the regression
line, at every value of <span class="math inline">\(x_{i}\)</span> in the dataset. This vertical distance
between <strong>observed and predicted y</strong> is a very important quantity in
regression, called the <strong>Residual</strong> and it is calculated by taking the
difference between the observed and predicted values of y for a given
observation:</p>
<p><span class="math display">\[\text{At\ a\ given\ value\ of\ }x_{i},\ the\mathbf{\ residual =}\mathbf{y}_{\mathbf{i}}\mathbf{-}{\widehat{\mathbf{y}}}_{\mathbf{i}}\]</span></p>
<p>Mathematically speaking, the regression line satisfies the Least Squares
Criterion, which is:</p>
<p><span class="math display">\[Least\ Squares\ Criterion = min\sum_{}^{}\left( y_{i} - {\widehat{y}}_{i} \right)^{2}\]</span></p>
<p>So, the regression procedure minimizes the sum of the squared residuals.
<strong>Conceptually, the estimated regression equation that linear regression
calculates is the line that is as close as possible to all the points in
the data set at once: that is what it means to satisfy the Least Squares
Criterion.</strong> The regression line that satisfies the Least Squares
Criterion is called the best-fit line.</p>
<p>Let’s calculate some residuals for the MindTap data, to gain insight
into what regression is doing and thereby understand what the output
tells us. Here is part of the MindTap dataset, along with the graph of
the data with the regression line:</p>
<table style="width:62%;">
<colgroup>
<col width="23%" />
<col width="19%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Student #</strong></th>
<th><p><strong>Hours</strong></p>
<span class="math display">\[(x_{i})\]</span></th>
<th><p><strong>Score</strong></p>
<span class="math display">\[(y_{i})\]</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>20.85</td>
<td>88.80</td>
</tr>
<tr class="even">
<td>2</td>
<td>10.65</td>
<td>85.40</td>
</tr>
<tr class="odd">
<td>3</td>
<td>25.72</td>
<td>99.60</td>
</tr>
<tr class="even">
<td>4</td>
<td>7.75</td>
<td>68.10</td>
</tr>
<tr class="odd">
<td>5</td>
<td>18.28</td>
<td>78.40</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<p><span class="chart"><span class="math display">\[CHART\]</span></span></p>
<p>Find Student #4 on the graph. Calculate the residual for this student.</p>
<p>Find Student #3 on the graph. Calculate the residual for this student.</p>
<p>The regression procedure uses the residuals for all 24 students in its
placement of the regression line. It takes all the residuals into
account at once, squares them (why?), sums them, and places the line
where that sum is the smallest it can be. This sum of the squared
residuals is extremely important, then! And that is why it is reported
in the ANOVA table in the regression output. It is called the <strong>Sum of
Squares due to Error</strong> or the <strong>Residual Sum of Squares,</strong> and it is
notated as the <strong>SSE.</strong></p>
<p><span class="math display">\[\mathbf{\text{SSE}} = \sum_{}^{}\left( y_{i} - {\widehat{y}}_{i} \right)^{2}\]</span></p>
<p><span class="math display">\[\text{with\ d}f_{2} = n - p - 1\]</span></p>
<p>Let’s take a look at the ANOVA table from the MindTap regression output.
(Refer to the <em>Regression Output Equations Roadmap</em> throughout). Where
is the SSE and its degrees of freedom?</p>
<table>
<thead>
<tr class="header">
<th align="left">ANOVA</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><em> </em></td>
<td><em>df</em></td>
<td><em>SS</em></td>
<td><em>MS</em></td>
<td><em>F</em></td>
<td><em>Significance F</em></td>
</tr>
<tr class="even">
<td align="left">Regression</td>
<td>1</td>
<td>884.5979</td>
<td>884.5979</td>
<td>20.1839</td>
<td>0.0002</td>
</tr>
<tr class="odd">
<td align="left">Residual</td>
<td>22</td>
<td>964.1917</td>
<td>43.8269</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">Total</td>
<td>23</td>
<td>1848.79</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>The other values in the ANOVA table are also important in what we are
building towards – that is, assessing the quality of this model.</p>
<p>The <strong>Total Sum of Squares,</strong> <span class="math inline">\(\mathbf{SST,}\ \)</span>is the total amount of
prediction error we would make if we used <span class="math inline">\(\overline{y}\)</span> (the mean of
<span class="math inline">\(y_{i}\)</span>) to predict the observed sample data. Where is the SST and its
<em>df</em> in the ANOVA table?</p>
<ul>
<li>The <span class="math inline">\(SST = \sum_{}^{}\left( y_{i} - \overline{y} \right)^{2}\)</span>, and
has degrees of freedom <span class="math inline">\(df = n - 1\)</span></li>
</ul>
<p>The <strong>Sum of Squares due to Regression,</strong> <span class="math inline">\(\mathbf{SSR,}\)</span> is how much we
will reduce prediction error by using the Estimated Regression Equation
instead of the mean to predict the observed sample data. Where is the
SSR and its <em>df</em> in the ANOVA table?</p>
<ul>
<li>The
<span class="math inline">\(SSR = \sum_{}^{}\left( {\widehat{y}}_{i} - \overline{y} \right)^{2}\)</span>,
and has degrees of freedom <span class="math inline">\(df_{1} = p\)</span></li>
</ul>
<p>The <strong>Mean Square Error,</strong> <span class="math inline">\(\mathbf{MSE,}\)</span> is the estimate of the
variance of <span class="math inline">\(\epsilon\)</span>. Remember <span class="math inline">\(\epsilon?\)</span> It is the random error term
in the Regression Model: <span class="math inline">\(y = \beta_{0} + \beta_{1}x + \epsilon\)</span>. Where
is the MSE in the ANOVA table?</p>
<ul>
<li>The <span class="math inline">\(MSE = \frac{\text{SSE}}{n - p - 1}\)</span></li>
</ul>
<p><strong><span class="underline">Putting this all together to explain the ANOVA table in
regression:</span></strong></p>
<ul>
<li><p>recall: the sample data is our best representation of the underlying
population, so if our model is good at predicting the sample values,
we can infer it will also be good at predicting population values</p></li>
<li><p>The ANOVA table in the regression output compares two different
methods of predicting the sample data in an effort to help us assess
the quality of our regression model. We assess the regression by
comparing its predictions to the next best alternative</p>
<ul>
<li><p>Premise: there are two different models you could use to predict
<span class="math inline">\(y\)</span>. One model includes information about <span class="math inline">\(x\)</span> and one does not:</p>
<ul>
<li><p>First, you could use the Estimated Regression Equation to
predict <span class="math inline">\(y\)</span> (in other words, predict <span class="math inline">\({\widehat{y}}_{i}\)</span>
<span class="math inline">\(\ \)</span>at each <span class="math inline">\(x_{i}\)</span>)</p></li>
<li><p>Second, you could use the next best option, which is to just
use the mean of <span class="math inline">\(\overline{y}\)</span> to predict <span class="math inline">\(y\)</span> at each
<span class="math inline">\(x_{i}\)</span></p>
<ul>
<li>This is also called the ‘Intercept only model’ because
the equation of <span class="math inline">\(\text{y\ }\)</span>does not include any <span class="math inline">\(x\)</span>
variables, thus being only an intercept, and
representing a horizontal line at <span class="math inline">\(y = \ \overline{y}\)</span>.</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>The ANOVA table reports the total errors made when predicting the
actual observed data using each of these models. In this way, we can
compare the two possible models and decide whether our regression
model is worth using.</p></li>
</ul>
<p>So now that we have more understanding of what is in the ANOVA table, we
should not be surprised to see all of these values showing up in the
Regression Statistics table in the output, which contains the model fit
stats we need.</p>
<p>Here is the Regression Statistics table from the <em>Regression Output
Equations Roadmap</em>:</p>
<table style="width:92%;">
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
</colgroup>
<tbody>
<tr class="odd">
<td><strong><em>Regression
Statistics</em></strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Multiple R</strong></td>
<td><span class="math inline">\(\text{\ \ \
R}_{\text{xy}}\)</span>
or <span class="math inline">\(R_{ŷy}\ \)</span>=
<span class="math inline">\(\sqrt{R^{2}}\)</span></td>
<td>→<span class="math inline">\(\text{~~
}R_{\text{xy}}\)</span>
&amp; <span class="math inline">\(R_{ŷy}\ \)</span>:
Correlation
Coefficient</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>R Square</strong></td>
<td><span class="math inline">\(\text{
\ \ \ R}^{2} =\)</span>
$
$</td>
<td>→$
R^{2}$
: Coefficient
of
Determination</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Adjusted R
Square</strong></td>
<td><span class="math display">\[\text{\ \
 \ R}_{A}^{2}\]</span></td>
<td><h1 id="r_a2">→ <span class="math inline">\(\ R_{A}^{2}\)</span></h1>
<span class="math inline">\(1 - \lbrack\
left( 1 - R^{2}  \right)\left( \frac{n\  - 1}{ n\  - p\  - 1} \right)\rbrack\)</span>
: Adjusted
<span class="math inline">\(R^{2}\)</span></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Standard
Error</strong></td>
<td><span class="math display">\[\
 \ \ s\  = \sqr
t{\text{MSE}}\]</span></td>
<td><p>→<span class="math inline">\(\text {~~}\text{s~}\)</span>:
Standard Error
of the
Estimate, or
Root Mean</p>
<p>Square Error
(RMSE)</p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><ul>
<li>*Observations**</li>
</ul></td>
<td><span class="math inline">\(\text{\ n}\)</span></td>
<td>→ <em>n</em> = #
observations</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Here is the Regression Statistics table from the MindTap regression
output:</p>
<table>
<thead>
<tr class="header">
<th align="left"><em>Regression Statistics</em></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Multiple R</td>
<td>0.6917</td>
</tr>
<tr class="even">
<td align="left">R Square</td>
<td>0.4785</td>
</tr>
<tr class="odd">
<td align="left">Adjusted R Square</td>
<td>0.4548</td>
</tr>
<tr class="even">
<td align="left">Standard Error</td>
<td>6.6202</td>
</tr>
<tr class="odd">
<td align="left">Observations</td>
<td>24</td>
</tr>
</tbody>
</table>
<p>blank on purpose</p>
<p>Show the calculation of the Coefficient of Determination, <span class="math inline">\(R^{2}.\)</span>
Interpret this value.</p>
<p>Show the calculation of the Standard Error of the Estimate, <span class="math inline">\(\text{s.}\)</span>
Interpret this value.</p>
<p>Show the calculation of the Correlation Coefficient, <span class="math inline">\(R_{\text{xy}}.\)</span>
<em>Interpret this value.</em></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-13-experimental-design-and-analysis-of-variance.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-15-multiple-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
