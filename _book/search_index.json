[
["index.html", "Statistics Book Preface", " Statistics Book Catherine Schmitt-Sands 2020-07-27 Preface "],
["introduction.html", "Introduction", " Introduction Definitions and Notation A population is the group of all objects (or subjects) of interest. In a statistical study, the population is defined by the researcher. For example, a population might be defined as all women between 18 and 34 in Michigan, if a researcher was interested in studying that group of subjects. A sample is a subset of the population. For example, a sample from this population could be 100 randomly selected women between 18 and 34 in Michigan. While it is populations that we are ultimately interested in knowing about, we usually do not observe them directly. Instead, samples are what we actually observe and measure in statistical studies. Statistical inference is the process of drawing conclusions about a population, based on a sample taken from that population. Why is randomization important? Only random samples can provide the basis for statistical inference, because only random samples are representative of the population as a whole. It should be clear from the foregoing discussion that we will be dealing with characteristics of populations and characteristics of samples from those populations in this class. Therefore, it will be important to differentiate carefully between them. We do that by using different terms (when possible) and different notation (always) when referring to populations and samples. A parameter is a characteristic of a population. A sample statistic is a characteristic of a sample. For example, the mean of a population is a parameter, while the mean of a sample is a sample statistic. Both are averages, but they are measured on two different groups, and so they are two different things. Characteristics measured on samples estimate the corresponding characteristics of the populations those samples came from. Sample statistics, in other words, are estimates of their corresponding population parameters. The values of sample statistics almost always differ slightly from the values of the corresponding parameters in the underlying population due to random variation, also called random error. So, even a representative, random sample is likely to differ slightly from the underlying population. Suppose you had a population with a mean of 10. Further suppose that you took 5 different random samples from it. In the first sample, just randomly, a few more low values might be chosen. So the mean of the sample would come out a little lower than the population mean, like 9.4. But then in the second sample, a few higher values might be chosen instead. So then the second sample would come out with a mean on the high side, like 10.2. And so it would go with the other three samples – each one chosen randomly, each slightly different from the other samples, and from the underlying population they all came from. In the process of statistical inference, it will be our task to distinguish between this type of random variation, due to random chance, and true variation that can give us information about the value of a population parameter. We will do this by using a technique called hypothesis testing. Notation is a part of life when learning statistics, and you should approach it like learning a language. The symbols are shorthand ways to refer to important concepts. You will need to become familiar with the following notation: Population Parameters Sample Statistics \\(\\mu\\) mean of a population The Greek letter “mu”, pronounced “mew” \\(\\overline{x}\\) mean of a sample Pronounced “x bar” \\(\\sigma\\) standard deviation of a population The lowercase Greek letter “sigma” \\(s\\) standard deviation of sample \\(\\sigma^2\\) variance of a population “Sigma squared” \\(s^2\\) variance of a sample \\(p\\) proportion of a population \\(\\overline{p}\\) proportion of a sample Pronounced “p bar” \\(\\beta\\) slope coefficient for a population The Greek letter “beta” \\(b\\) slope coefficient for a sample More Notation \\(H_0\\) The null hypothesis Pronounced “H nought” or “H O” \\(H_A\\) The alternative hypothesis Pronounced “H A” for short \\(\\alpha\\) A significance level The Greek letter “alpha” \\(n\\) A sample size \\(p\\) or \\(P\\) a probability \\(\\Sigma\\) The summation operator The uppercase Greek letter “sigma” Read this as “The sum of all….” \\(\\Delta\\) Read this as “the change in…” The Greek letter “delta” \\(x_i\\) A variable with a subscript A subscript denotes one of several variables The \\(i^{th}\\) x, pronounced “x sub i” or “x of i” for short E.g. \\(x_2\\) is the second x in a set of x variables and could be called “x sub 2” or “x 2” \\(\\hat{y}\\) A variable with a hat pronounced “y hat” \\(\\epsilon\\) or \\(\\varepsilon\\) Random error The Greek letter “epsilon” \\(&gt;\\) greater than \\(&lt;\\) less than \\(\\ge\\) greater than or equal to \\(\\le\\) less than or equal to \\(\\ne\\) not equal to "],
["the-z-distribution.html", "The z Distribution", " The z Distribution The \\(\\mathbf{z}\\) distribution, also called the standard normal distribution, is a normal distribution with a mean of zero and a standard deviation of one. It is used to standardize values in order to determine their probability. Probability in a \\(z\\) distribution is represented as the area between the curve and the horizontal axis – often called the area under the curve. (This is actually true of all probability distributions – they all use area under the curve to measure probability). Let’s draw a \\(z\\) distribution and label some of its important features: NEEDS GRAPH \\(z\\) values are in units of standard deviations, so they express how many standard deviations away from the mean a particular value is. For example, a value that standardizes to a \\(z\\) of \\(2.0\\) is two standard deviations above the mean, while a value that standardizes to a \\(z\\) of \\(- 1.6\\) is \\(1.6\\) standard deviations below the mean. The \\(\\mathbf{z}\\) table is used to look up probabilities for different values of \\(z\\). The probability of a value falling at or above a given \\(z\\) is called the upper tail probability, or upper tail p-value. The probability of a value falling at or below a given \\(z\\) is called the lower tail probability, or lower tail p-value. The probability of a value falling a given distance away from the mean in either tail is called the two-tailed p-value (don’t worry – there will be more on this one later). These p-values will be very important in hypothesis testing, where they will be the evidence in the p-value approach to rejecting the null hypothesis. Example 1. What is the probability that a randomly drawn z value would be greater than or equal to 1.24? This is what is called the upper tail p-value of \\(z = 1.24\\) NEEDS GRAPH Example 2 What is the probability that a randomly drawn z value would be less than or equal to \\(- 1.84\\)? This is what is called the lower tail p-value of \\(z = - 1.84.\\) NEEDS GRAPH Example 3 What is the probability that a randomly drawn z value would be at least \\(2.15\\) standard deviations away from the mean? This is what is called a two-tailed p-value. NEEDS GRAPH Now it is your turn! Try the three exercises below. Exercise 1. What is the UT p-value of \\(z = 2.50\\)? Exercise 2. What is the LT p-value of \\(z = - 1.00\\)? Exercise 3. What is the 2T p-value of \\(z = - 2.82\\)? Critical Values of Z Critical Values (CVs) are values of \\(z\\) with given tail probabilities. In notation, Critical Values are always subscripted with a probability. In general, \\(z_{\\alpha}\\) is the CV of \\(z\\) with \\(\\alpha\\) probability in the upper tail, and \\(-z_{alpha}\\ \\)is the CV of \\(z\\) with \\(\\alpha\\) probability in the lower tail. Some common CVs of \\(z\\) are given in the inset table on the first page of the z table, reproduced here: \\(\\alpha\\) \\(z_{alpha}\\) .10 1.282 .05 1.645 .025 1.96 .01 2.326 .005 2.576 where \\(\\alpha = \\ \\)the tail probability and \\(z_{\\alpha} = \\ \\)the Critical Value of \\(\\text{z\\ }\\)with \\(\\alpha\\) probability in the tail Example 4 Look up the upper tail CV of \\(z\\) at \\(\\alpha = 0.05.\\) Draw and fully label the \\(z\\) distribution showing the CV and shading in the tail probability. Example 5 Look up the lower tail CV of \\(z\\) at \\(\\alpha = 0.01.\\) Draw and fully label the \\(z\\) distribution showing the CV and shading in the tail probability. Example 6 Look up the two-tailed Critical Values of \\(z\\) at \\(\\alpha = 0.05.\\)Draw and fully label the \\(z\\) distribution. Here are the steps to look up the 2T CVs: Split \\(\\alpha\\) between the two tails by dividing \\(\\alpha\\) by 2 Look up the LT CV of \\(z\\) at \\(\\alpha/2\\) and the UT CV of \\(z\\)at \\(\\alpha/2\\). These are your 2T CVs. Why do we need Critical Values? Critical Values can give us information about the p-value (the probability) of other values of \\(z\\). This use of CVs will be very important in hypothesis testing, where they will be the evidence in the Critical Value approach to rejecting the null hypothesis. What is the UT CV of \\(z\\) at \\(\\alpha = 0.10?\\) Draw and fully label the \\(z\\) distribution. What can this picture tell us about the UT p-value of \\(z = 4.1?\\) "],
["chapter-9.html", "Chapter 9", " Chapter 9 Hypothesis Tests "],
["hypothesis-testing-the-process.html", "Hypothesis Testing: The Process", " Hypothesis Testing: The Process A hypothesis test occurs in stages. The logic of hypothesis testing is as follows: Identify a population of interest, a parameter of interest, and a question you have about that parameter Make an assumption about the value of the population parameter you are interested in Examine a random sample from the population to see if it contradicts that assumption If the sample contradicts the assumption, then reject the assumption. Otherwise, do not reject the assumption. (NOTE: you can never prove that the assumption is true in a hypothesis test – you can only prove that it is wrong. Hence: “Reject,” or “Do not reject”) Answer the original question. In practice, these are the steps in a hypothesis test, in which you use a sample to infer information about the value of a population parameter: Formulate a null hypothesis \\(\\mathbf{(}\\mathbf{H}_{\\mathbf{0}}\\mathbf{)}\\) and an alternative hypothesis \\(\\mathbf{(}\\mathbf{H}_{\\mathbf{A}}\\mathbf{)}\\) according to the question you want to answer about the population parameter of interest. The null hypothesis is an assumption about the value of the population parameter; the alternative hypothesis is the mathematical opposite of the null hypothesis. Calculate the Test-Statistic. In this step, you standardize information from the sample against the appropriate sampling distribution. Decide whether or not to reject the null hypothesis. Determine the probability of getting that Test-Statistic (in other words, that sample), assuming that the null hypothesis is true. If the probability is less than the \\(\\alpha\\) significance level, then reject the null hypothesis and accept the alternative hypothesis. Otherwise, do not reject the null hypothesis and conclude that the alternative hypothesis is unsupported. Interpret the hypothesis test in order to answer the original question. "],
["hypothesis-tests-about-a-single-population-mean.html", "Hypothesis Tests about a Single Population Mean", " Hypothesis Tests about a Single Population Mean 1. Formulating the Hypotheses The three possible forms of hypotheses each correspond to a different question you might want to ask about the true value of the population mean, \\(\\mu\\). Note: in each of the hypotheses below, \\(\\mu_0\\) is a number. It is the hypothesized value of \\(\\mu\\). Hypotheses for Hypothesis Tests about a Single Population Mean \\(\\mathbf{\\mu}\\) Lower Tail Test Upper Tail Test Two-Tailed Test \\(H_0:\\mu \\geq \\mu_0\\) \\(H_A: \\mu &lt; \\mu_0\\) \\(H_0: \\mu \\leq \\mu_0\\) \\(H_A: \\mu &gt; \\mu_0\\) \\(H_0: \\mu = \\mu_0\\) \\(H_A: \\mu \\neq \\mu_0\\) Answers questions about If the true population mean, \\(\\mu,\\) is less than a given number, \\(\\mu_0\\) If the true population mean, \\(\\mu,\\) is greater than a given number, \\(\\mu_0\\) If the true population mean, \\(\\mu,\\) is different from a given number, \\(\\mu_0\\) Example Suppose you want to know whether a population mean is more than 3. That matches with the question answered by an upper tail test with a \\(\\mu_{0}\\) of 3. So, the hypotheses would be: \\[H_0: \\mu \\leq 3\\] \\[H_A: \\mu &gt; 3\\] Upper Tail and Lower Tail Tests are called one-tailed or directional tests. Two-tailed tests are called non-directional tests. 2. Calculate the Test Statistic In this step, we calculate a test statistic from the sample information. The test statistic standardizes the sample, so we can use it to judge our hypotheses. In the case of testing a population mean, there is an additional preliminary step: we have to choose the correct test statistic to use. Every hypothesis test incorporates some information about the variability of the underlying population – otherwise it would be impossible to determine how probable a given sample statistic really is. For hypothesis testing about a single population mean, the correct choice of test statistic depends on the sampling distribution of \\(\\overline{x}\\), which in turn depends on what information you have about the variability in the underlying population. While the population standard deviation, \\(\\sigma\\), cannot strictly be known without measuring the entire population and calculating it, sometimes we can assume we know the true value of \\(\\sigma\\), The basis for such an assumption would come from prior knowledge, historical information, or past experience with the population in question. Other times, we might not have knowledge of \\(\\sigma\\). In that case, we can still get some idea about the variability of the underlying population by looking at the standard deviation, \\(s\\), of a random sample taken from that population. After all, a random sample is representative of the population it comes from, so the amount of variability in the population ought to be reflected in the amount of variability in the sample. If we cannot assume we know \\(\\sigma\\), then \\(s\\) is our best estimate of the variability in the population. NOTE: \\(s\\) is a measurement on the sample – we calculate it from the sample data. In summary: The population standard deviation, \\(\\sigma\\), is sometimes assumed to be known from prior knowledge, historical information, or past experience. On the other hand, if \\(\\sigma\\) is unknown, then the only information about variability in the underlying population is the standard deviation measured on the sample itself – the sample standard deviation, \\(s\\). In hypothesis testing about a single population mean, it will be your task to carefully identify whether the standard deviation in a given problem refers to a sample or to the population. That will determine which test statistic to use. When \\(\\mathbf{\\sigma}\\) is KNOWN Under the assumption that \\(H_0\\) is true as an equality, the sampling distribution of the sample mean, \\(\\overline{x}\\), follows the \\(z\\) distribution. Therefore, we standardize the sample against the \\(z\\) distribution by calculating the following \\(z\\) test statistic: \\[z_{test} = \\frac{\\overline{x}-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\] where \\(\\overline{x}\\) = the sample mean \\(\\mu_0\\) = the hypothesized value of \\(\\mu\\) \\(\\sigma\\) = the population standard deviation \\(n\\) = the sample size NOTE: when calculating this equation, evaluate the numerator and denominator separately, and then divide When \\(\\mathbf{\\sigma}\\) is UNKNOWN, we use \\(s\\) instead Under the assumption that \\(H_0\\) is true as an equality, the sampling distribution of the sample mean, \\(\\overline{x}\\), follows the \\(t\\) distribution with \\(degrees\\ of\\ freedom = n - 1\\). Therefore, we standardize the sample against the \\(t\\) distribution by calculating the following \\(t\\) test statistic: \\[t_{test} = \\frac{\\overline{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\] where \\(\\overline{x}\\) = the sample mean \\(\\mu_0\\) = the value that \\(\\mu\\) is being compared to (from the hypotheses) \\(s\\) = the sample standard deviation \\(n\\) = the sample size and the degrees of freedom = \\(n - 1\\) NOTE: when calculating this equation, evaluate the numerator and denominator separately, and then divide 3. Deciding whether to reject \\(\\mathbf{H_0}\\) or not There are two approaches to deciding whether or not to reject the Null: In the p-value approach, we compare the p-value of our test statistic to the \\(\\mathbf{\\alpha}\\) significance level and reject \\(H_{0}\\) if the p-value is less than or equal to the \\(\\alpha\\) significance level. In the critical value approach, we compare the test statistic to a critical value – this can be done with a diagram in which we use the critical value(s) to construct a rejection region or regions; if the test statistic is in a rejection region, we reject \\(H_{0}\\) and accept \\(H_{A}.\\) Or, this step can be accomplished by following the mathematical rules given in the tables below. When to Reject \\(\\mathbf{H_0}\\) for z-test statistics For a Lower Tail Test For an Upper Tail Test For a Two-Tail Test p-value approach Look up the LT p-value of \\(z_{test}\\) If the LT p-value \\(\\leq \\alpha\\), reject \\(H_0\\) and accept \\(H_A\\). If the LT p-value &gt; \\(\\alpha\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. Look up the UT p-value of \\(z_{test}\\) If the UT p-value \\(\\leq \\alpha\\), reject \\(H_0\\) and accept \\(H_A\\). If the UT p-value &gt; \\(\\alpha\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. The two-tailed p-value is two times the one tailed p-value of \\(z_{test}\\). If the 2T p-value \\(\\leq \\alpha\\), reject \\(H_0\\) and accept \\(H_A\\). If the 2T p-value &gt; \\(\\alpha\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. CV approach If \\(z_{test} \\leq -z_{\\alpha}\\), reject \\(H_0\\) and accept \\(H_A\\). If \\(z_{test} &gt; -z_{\\alpha}\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. If \\(z_{test} \\geq z_{\\alpha}\\), reject \\(H_0\\) and accept \\(H_A\\). If \\(z_{test} &lt; z_{\\alpha}\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. If \\(z_{test} \\leq -z_{\\alpha/2}\\) OR \\(z_{test} \\geq z_{\\alpha/2}\\), reject \\(H_0\\) and accept \\(H_A\\). If \\(-z_{\\alpha/2} &lt; z_{test} &lt; z_{\\alpha/2}\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. NOTES: 1. \\(z_{test}\\) is the Test Statistic 2. \\(z_{\\alpha}, -z_{\\alpha}, and\\ z_{\\alpha/2}\\) are Critical Values When to Reject \\(\\mathbf{H_0}\\) for t-test statistics For a Lower Tail Test For an Upper Tail Test For a Two-Tail Test p-value approach Look up the LT p-value of \\(t_{test}\\) If the LT p-value \\(\\leq \\alpha\\), reject \\(H_0\\) and accept \\(H_A\\). If the LT p-value &gt; \\(\\alpha\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. Look up the UT p-value of \\(t_{test}\\) If the UT p-value \\(\\leq \\alpha\\), reject \\(H_0\\) and accept \\(H_A\\). If the UT p-value &gt; \\(\\alpha\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. The two-tailed p-value is two times the one tailed p-value of \\(t_{test}\\). If the 2T p-value \\(\\leq \\alpha\\), reject \\(H_0\\) and accept \\(H_A\\). If the 2T p-value &gt; \\(\\alpha\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. CV approach If \\(t_{test} \\leq -t_{\\alpha}\\), reject \\(H_0\\) and accept \\(H_A\\). If \\(t_{test} &gt; -t_{\\alpha}\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. If \\(t_{test} \\geq t_{\\alpha}\\), reject \\(H_0\\) and accept \\(H_A\\). If \\(t_{test} &lt; t_{\\alpha}\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. If \\(t_{test} \\leq -t_{\\alpha/2}\\) OR \\(t_{test} \\geq t_{\\alpha/2}\\), reject \\(H_0\\) and accept \\(H_A\\). If \\(-t_{\\alpha/2} &lt; t_{test} &lt; t_{\\alpha/2}\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. NOTES: 1. \\(t_{test}\\) is the Test Statistic 2. \\(t_{\\alpha}, -t_{\\alpha}, and\\ t_{\\alpha/2}\\) are Critical Values 3. The correct t distribution to use when determining the p-value or the Critical Value depends on the degrees of freedom. 4. Interpreting the Test Note: This explanation of interpretation holds for ALL hypothesis tests. We start every hypothesis test with a question about the parameter of interest, so we must end every hypothesis test with the answer to that question. In other words, we must interpret the conclusion of our test in terms of the original question. Remember: in hypothesis testing you can never prove the null hypothesis. You can only prove the alternative hypothesis: that is, when you reject the null and accept the alternative, then at your given \\(\\alpha\\) level of significance you can conclude that \\(H_{A}\\) is true. If you do reject \\(H_{0},\\ \\)then you must conclude that \\(H_{A}\\) is unsupported by the evidence. This gives us a clear guideline for how to interpret hypothesis tests: always look to the alternative hypothesis! Restate \\(H_{A}\\) in words, and say whether or not you can conclude that it is true. In the table that follows, you would substitute the actual words and numbers from your hypothesis test for the symbols. Notice that each interpretation simply states the alternative hypothesis in words, and says either that we can or cannot conclude it is true. How to Interpret a Hypothesis Test about a Single Population Mean, \\(\\mathbf{\\mu}\\) When you For a Lower Tail Test For an Upper Tail Test For a Two-Tailed Test Reject \\(\\mathbf{H_0}\\) At the \\(\\alpha\\) significance level, we can conlude that \\(\\mu\\) is less than \\(\\mu_0\\). At the \\(\\alpha\\) significance level, we can conclude that \\(\\mu\\) is greater than \\(\\mu_0\\). At the \\(\\alpha\\) significance level, we can conclude that \\(\\mu\\) is different than \\(\\mu_0\\). Do not reject \\(\\mathbf{H_0}\\) At the \\(\\alpha\\) significance level, we cannot conclude that \\(\\mu\\) is less than \\(\\mu_0\\). At the \\(\\alpha\\) significance level, we cannot conlcude that \\(\\mu\\) is greater than \\(\\mu_0\\). At the \\(\\alpha\\) significance level, we cannot conclude that \\(\\mu\\) is different than \\(\\mu_0\\). NOTES: 1. \\(\\mu\\) is the true value of the population mean 2. \\(\\mu_0\\) is a number that is the hypothesized value of \\(\\mu\\) "],
["hypothesis-tests-about-a-single-population-proportion.html", "Hypothesis Tests about a Single Population Proportion", " Hypothesis Tests about a Single Population Proportion 1. Formulating the Hypotheses The three possible forms of hypotheses each correspond to a different question you might want to ask about the true value of a population proportion, \\(p\\). It is important to remember that a proportion is a number between 0 and 1. Proportions are often expressed in words as percentages, fractions, or shares. These must be converted into proportions before being using in hypothesis tests (See the Chapter 9 handout on proportions). Note: in each of the hypotheses, \\(p_0\\) is a number. It is the hypothesized values of \\(p\\). Hypotheses for Hypothesis Tests about a Single Population Mean \\(\\mathbf{\\mu}\\) Lower Tail Test Upper Tail Test Two-Tailed Test \\(H_0:p \\geq p_0\\) \\(H_A: p &lt; p_0\\) \\(H_0: p \\leq p_0\\) \\(H_A: p &gt; p_0\\) \\(H_0: p = p_0\\) \\(H_A: p \\neq p_0\\) Answers questions about If the true population proportion, \\(p,\\) is less than a given number, \\(p_0\\) If the true population proportion, \\(p,\\) is greater than a given number, \\(p_0\\) If the true population proportion, \\(p,\\) is different from a given number, \\(p_0\\) Example Suppose you want to know whether a population proportion has decreased from 0.25. That matches with the question answered by a lower tail test with a p_0 of 0.25. So, the hypotheses would be: \\[H_0: p \\geq 0.25\\] \\[H_A: p &lt; 0.25\\] Once again, Upper Tail and Lower Tail Tests are called one-tailed or directional tests. Two-tailed tests are called non-directional tests. 2. Calculating the Test Statistic Under the assumption that \\(H_{0}\\) is true as an equality, the sampling distribution of the sample proportion, \\(\\overline{p}\\), is the \\(z\\) distribution. Therefore, we have to standardize the sample information against the \\(z\\) distribution by calculating the following \\(z\\) test statistic: \\[z_{test} = \\frac{\\overline{p} - p_{0}}{\\sqrt{\\frac{p_0\\left(1 - p_0\\right)}{n}}}\\] where \\(\\overline{p}\\) = the sample proportion \\(p_0\\) = the hypothesized value of \\(p\\) \\(n\\) = the sample size NOTE: when calculating this equation, evaluate the numerator and denominator separately, and then divide 3. Deciding whether to reject \\(\\mathbf{H_0}\\) or not The rules are the same as the rules given above in the table labeled \"When to Reject \\(\\mathbf{H_0}\\) for \\(\\mathbf{z}\\) test statistics.\" 4. Interpreting the Test Remember, in this step: always look to the alternative hypothesis! In the table that follows, you would substitute the actual words and numbers from your hypothesis test for the symbols. Notice that each interpretation simply states the alternative hypothesis in words, and says either that we can or cannot conclude it is true. How to Interpret a Hypothesis Test about a Single Population Proportion, \\(\\mathbf{p}\\) When you For a Lower Tail Test For an Upper Tail Test For a Two-Tailed Test Reject \\(\\mathbf{H_0}\\) At the \\(\\alpha\\) significance level, we can conlude that \\(p\\) is less than \\(p_0\\). At the \\(\\alpha\\) significance level, we can conclude that \\(p\\) is greater than \\(p_0\\). At the \\(\\alpha\\) significance level, we can conclude that \\(p\\) is different than \\(p_0\\). Do not reject \\(\\mathbf{H_0}\\) At the \\(\\alpha\\) significance level, we cannot conclude that \\(p\\) is less than \\(p_0\\). At the \\(\\alpha\\) significance level, we cannot conlcude that \\(p\\) is greater than \\(p_0\\). At the \\(\\alpha\\) significance level, we cannot conclude that \\(p\\) is different than \\(p_0\\). NOTES: 1. \\(p\\) is the true value of the population mean 2. \\(p_0\\) is a number that is the hypothesized value of \\(p\\) Assumptions Underlying These Hypothesis Tests All hypothesis tests use sampling distributions to determine the probability of sample statistics. In order for us to be confident that our choice of sampling distribution for any given test really is the way the sample statistic is distributed, certain assumptions must be met. (Note: these are separate from the assumption we make in the null hypothesis. We should only formulate a null hypothesis once these assumptions are met). If the assumptions are not met – that is, if any given assumption is not true – then we cannot rely on the results of the hypothesis tests. They may mislead us, give us the wrong answers, and cause us to draw the wrong conclusions. For hypothesis tests about a single population mean, \\(\\mathbf{\\mu}\\): If the population is normally distributed, then the sample size can be small If the population is not normally distributed, or if the distribution is unknown, then the sample size must be greater than or equal to 30 \\((n \\geq 30)\\) When \\(\\sigma\\) is unknown (that is, when you are using the t distribution), the t test works even with small sample sizes. However, if the population has a skewed distribution or has outliers, then the sample size must be greater than 50 \\((n \\geq 50)\\) For hypothesis tests about a single population proportion \\(\\mathbf{p}\\): The following condition must hold true: \\[np_0 \\geq 5\\ and\\ n(1 - p_0) \\geq 5\\] where \\(n\\) = sample size \\(p_0\\) = the hypothesized value of \\(p\\) from \\(H_0\\) and \\(H_A\\) "],
["exercise-formulating-the-null-and-alternative-hypotheses.html", "Exercise: Formulating the Null and Alternative Hypotheses", " Exercise: Formulating the Null and Alternative Hypotheses IMPORTANT: Hypotheses are always about population parameters. They are never about sample statistics, so if you write the notation for a sample statistic in a pair of hypotheses, that will be a signal you have done something wrong. First, let’s practice some basics, referring back to Step #1: Formulating the Hypotheses in Part 1 of Ch9: Handout #2. Example 1. Write the hypotheses for a lower tail test for a single population mean if \\(\\mu_{0}\\) is \\(1500\\). Example 2. Write the hypotheses for an upper tail test for a single population mean if \\(\\mu_{0}\\) is \\(- 17.\\) Example 3. Write the hypotheses for a two-tailed test for a single population mean if \\(\\mu_{0}\\) is \\(752.\\) **Now, try it yourself:** Exercise 1. Write the hypotheses for an upper tail test for a single population mean if \\(\\mu_{0}\\) is \\(22\\) Exercise 2. Write the hypotheses for a lower tail test for a single population mean if \\(\\mu_{0}\\) is \\(- 45\\). Exercise 3. Write the hypotheses for a two-tailed test for a single population mean if \\(\\mu_{0}\\) is \\(12,383.\\) Now, let’s build on that. To determine which tail test to choose, you have to match up the question you have about the population mean with the hypotheses that are the best fit to answer the question. Since you can only prove the Alternative Hypothesis (\\(H_{A}\\)), and you cannot ever prove the null hypothesis (\\(H_{0}\\)), it is usually helpful to frame your question in terms of the alternative hypothesis. What I mean by that is, you can read each \\(H_{A}\\) aloud in words, and see which one fits the circumstances best. Example 4. Formulate the hypotheses for a hypothesis test to determine whether a population mean is smaller than 14. Example 5. Formulate the hypotheses for a hypothesis test to determine whether a population mean is different than 419. Example 6. Formulate the hypotheses for a hypothesis test to determine whether a population mean has increased from 85. Now, try it yourself: Exercise 4. Formulate the hypotheses for a hypothesis test to determine whether a population mean has decreased from 581. Exercise 5. Formulate the hypotheses for a hypothesis test to determine whether a population mean has changed from 74. Exercise 6. Formulate the hypotheses for a hypothesis test to determine whether a population mean is higher than 620. Now let’s apply what we’ve learned to actual hypothesis testing scenarios. In order to properly formulate the Null Hypothesis (\\(H_{0}\\)) and the Alternative Hypothesis (\\(H_{A}\\)), you must: Identify the parameter of interest Identify the question being asked about the parameter of interest and match it to an LT, UT, or 2T test Example 7: A shampoo company has introduced a new formula of a particular shampoo. In the past, customers have been regularly surveyed about the shampoo, asked to rate the product on a scale of 1 to 10, and the shampoo has scored a mean of 6.2. An analyst at the company would like to test whether the mean satisfaction score changed after the new formula was introduced. Formulate the Null and Alternative Hypotheses the analyst should use. The parameter of interest is \\(\\mu:\\ \\)the population mean customer satisfaction score The question being asked is: has the mean customer satisfaction score changed from what it was before the new formula was introduced (which was 6.2)? The hypotheses will refer to \\(\\mu\\) which is the population mean customer satisfaction score. The analyst wants to test for any change from 6.2 – if the mean score has changed, then it would be different from (i.e. not equal to) 6.2. Therefore, the proper hypotheses are: \\(H_{0}:\\ \\) \\(H_{A}:\\ \\) Example 8: The same situation as Example 1, except this time, the analyst is concerned that the mean customer satisfaction score has fallen with the introduction of the new formula. Formulate the Null and Alternative Hypotheses to test whether the mean satisfaction score has decreased. The parameter of interest is (still) \\(\\mu:\\ \\)the population mean customer satisfaction score The question being asked is: has the mean customer satisfaction score decreased from what it was before (which was 6.2)? The hypotheses will still refer to \\(\\mu\\) which is the mean customer satisfaction score of the population. The analyst wants to test for a decrease – and if the mean score has decreased than it would be less than 6.2. Therefore, the proper form is: \\(H_{0}:\\ \\) \\(H_{A}:\\ \\) Example 9: Again, same situation as Example 1. But this time, the analyst would like to prove that the new formula is better than the old formula. That is, the analyst would like to test whether the mean customer satisfaction score has increased. Formulate the Null and Alternative Hypotheses to test whether the mean customer satisfaction score has gone up with the new formula’s introduction. The parameter of interest is (still) \\(\\mu:\\ \\)the population mean customer satisfaction score The question being asked is: Has the mean customer satisfaction score increased from what it was before (which was 6.2)? The hypotheses will still refer to \\(\\mu\\) which is the mean customer satisfaction score of the population. The analyst would like to show that this parameter has increased – and if the mean score has increased than it would be greater than 6.2. Therefore, the hypotheses are: \\(H_{0}:\\ \\) \\(H_{A}:\\ \\) Example 10: A manufacturer makes test tubes for laboratory use. These test tubes must hold exactly 10 milliliters (ml) of liquid. A quality control analyst would like to check whether or not the production line is running properly. If the production line goes out of adjustment, then it may produce test tubes that are on average too large or too small. Formulate the Null and Alternative Hypotheses to test whether the production process has gone out of adjustment. \\(H_{0}:\\) \\(H_{A}:\\) Example 11: A human resources manager is interested in testing the effectiveness of a new training program. Prior to the training, employees could handle an average of 11.65 cases per day. The HR manager would like to determine whether there is evidence that the training has increased the average number of cases employees can handle per work day. Formulate the Null and Alternative Hypotheses to test whether the training increased the average number of cases employees can handle per day. \\(H_{0}:\\) \\(H_{A}:\\) "],
["exercises-hypothesis-tests-about-a-single-population-mean.html", "Exercises: Hypothesis Tests about a Single Population Mean", " Exercises: Hypothesis Tests about a Single Population Mean Exercise 1: A company has a tech support staff that helps employees handle computer issues. In the past, the average time to resolve a computer issue was 7 minutes. The company has gathered enough historical data over time on computer issue resolutions to be confident that the population standard deviation is 4 minutes. The company instituted a new ticketing system for computer issues, with the goal of decreasing the time it takes to resolve them. Once the ticketing system was in place, a random sample of 35 issues was taken and the mean time to resolution in the sample was calculated as 5.5 minutes. Can the company conclude, at an \\(\\alpha = 0.05\\) significance level, that the population mean time to resolution has actually decreased? Exercise 2: A sports equipment manufacturer produces a line of 20 lb kettlebells. The manufacturing process can sometimes go out of adjustment. When this happens, it can make kettlebells that are on average too light or too heavy. In order to check whether the process is working properly, the manufacturer takes a random sample of 30 kettlebells, weighs each of them, and finds that the mean of the sample is 20.26 lbs. From the specification of its manufacturing equipment, the manufacturer knows that the population standard deviation in this manufacturing process is 0.78 lbs. Perform and interpret a hypothesis test to check whether the manufacturing process is turning out kettlebells that weigh 20 lbs on average at the \\(\\alpha = 0.05\\) significance level. Should the process be adjusted? "],
["the-t-distribution.html", "The t Distribution", " The t Distribution The t distribution is a probability distribution that is frequently used as a sampling distribution in hypothesis testing. In some ways, it is similar to the z distribution, but it has several key differences as well. The t distribution: has a total area under the curve of one is approximately bell-shaped, but is not normal has a mean of zero is centered on the mean is symmetrical around the mean “as above, so below the mean” upper tail t values are positive, lower tail t values are negative is a family of distributions, so you must decide which t distribution to use The correct distribution to use in each case will be determined by the degrees of freedom (df), which is calculated from information about the sample. We will use the t distribution to decide whether to reject the null hypothesis for t test statistics. For the Critical Value approach, we can look up the CVs on the t table. For the p-value approach, we will use Excel to calculate the p-values for t test statistics (see separate handout). Using the t table The t table is a table of Critical Values of t at particular tail probabilities. Each row of the t table corresponds to a t distribution with the given degrees of freedom (df). Each column of the t table refers to a different upper tail probability We can use the t table for lower tail Critical Values too. Because the t distribution is symmetrical around the mean of zero, a lower tail Critical Value will be the negative of the upper tail Critical Value with the same tail probability. Examples Example 1 Use the t table to look up the following: The upper tail Critical Value of t at \\(\\alpha = 0.05\\) with degrees of freedom \\(df = 44\\) The lower tail Critical Value of t at \\(\\alpha = 0.10\\) with degrees of freedom \\(df = 82\\) The two-tailed Critical Values of t at \\(\\alpha = 0.05\\) with degrees of freedom \\(df = 37\\)* Example 2 Consider a lower tail test at the \\(\\alpha = 0.10\\) significance level with a test statistic of \\(t_{test} = - 1.157\\) and degrees of freedom \\(df = 38\\). Use the Critical Value approach to decide whether to reject \\(H_{0}.\\) Use the p-value approach to decide whether to reject \\(H_{0}.\\) Example 3 Consider a two-tailed test at the \\(\\alpha = 0.01\\) significance level with a test statistic of \\(t_{test} = 3.561\\) and degrees of freedom \\(df = 85.\\) Use the Critical Value approach to decide whether to reject \\(H_{0}.\\) Use the p-value approach to decide whether to reject \\(H_{0}.\\) "],
["exercise-more-hypothesis-testing-about-a-single-population-mean.html", "Exercise: More Hypothesis Testing about a Single Population Mean", " Exercise: More Hypothesis Testing about a Single Population Mean Exercise 1: A cosmetics brand hired several Instagram influencers to post about their makeup in an effort to boost sales. The mean of monthly sales prior to hiring the influencers was $11,000 per month per store. In order to check whether sales are increasing, the company takes a sample of 50 stores and measures monthly sales. The mean of the sample is $11,800, and the standard deviation of the sample is $4500. Perform and interpret a hypothesis test to find out if the population mean sales have increased at an \\(\\alpha = 0.01\\) significance level. "],
["hypotheses-about-a-single-population-proportion.html", "Hypotheses about a Single Population Proportion", " Hypotheses about a Single Population Proportion Another population parameter we may be interested in is a population proportion, \\(p\\). The first concern when dealing with problems about proportions is that proportions are commonly expressed in words as percentages, shares, or fractions. Regardless of how they are expressed in words, they must be converted into mathematical proportions before using them in hypothesis testing. Proportions are always between 0 and 1. To convert a percentage to a proportion, divide the percentage by 100. If a proportion is expressed as a fraction, divide out the fraction to obtain a decimal. A proportion may be expressed as a share, such as 25 out of 72. To convert a share to a proportion, calculate the relative frequency: \\(\\frac{25}{72} = 0.3472.\\) Example 1 Converting a percentage to a proportion 97% of the people in Michigan live in the Lower Peninsula. What proportion of people in Michigan live in the Lower Peninsula? Answer: 97% is a percentage. To convert it to a proportion, divide 97 by 100: \\(p = \\frac{97}{100} = \\mathbf{0.97}\\) Example 2 Converting a share to a proportion A random sample of 38 customers was taken. 23 preferred Brand A. What proportion of customers preferred Brand A? Round to 4 decimal places, and recall that a sample proportion is notated \\(\\overline{p}.\\) Answer: This proportion is expressed as a share: 23 out of 38 customers prefer Brand A. To convert it to a proportion, divide the number of customers who prefer Brand A by the total number of customers: \\(\\overline{p} = \\frac{23}{38} = \\ \\mathbf{0.6053}\\) Example 3 Converting a fraction to a proportion 1/5 of people prefer oranges to apples. What proportion of people prefer oranges to apples? Answer: This proportion is expressed as a fraction. To convert it to a proportion, divide it out: \\(p = \\frac{1}{5} = \\mathbf{0.2}\\) For the following exercises, formulate the hypotheses \\(\\left( H_{0}\\text{\\ and\\ }H_{A} \\right)\\ \\)and be sure to express \\(p_{0}\\) correctly as a proportion: Exercise 1: An optician reads an article that states that industry-wide, 3 in 10 customers rate new glasses as “unsatisfactory”. This optician would like to find out if the proportion of his customers who would say the same is lower than the industry-wide proportion. Formulate the null and alternative hypotheses the optician should use. Exercise 2: A manufacturer suspects that a new plant produces a proportion of defective parts that is different from the other plants. At the other plants, the percentage of defective parts is 5.9%. What are the null and alternative hypotheses the company should use to test whether the population proportion of defective parts at the new plant differs from 5.9%? Exercise 3: Suppose that in 2012, 3/5 of people preferred to write in pen rather than pencil. What null and alternative hypotheses should you use to determine whether the proportion of people who prefer to write in pen has increased? "],
["exercise-hypothesis-test-about-a-single-population-proportion.html", "Exercise: Hypothesis Test about a Single Population Proportion", " Exercise: Hypothesis Test about a Single Population Proportion A company that puts on triathlons finds that over the years, 30% of its race entrants were women. The company decides to add a women’s only triathlon as one of its races. The company knows this is a risky move. It could attract more women to sign up for triathlons with the company because they see it as welcoming to women. Or, female triathletes might see the offering as patronizing and choose to race with some other company. Finally, it could be that the same number of women would sign up with or without a women’s only race. Sometime after introducing the women’s only triathlon into its lineup of races, the company took a random sample of 151 entries and found that 57 of the entries were for women. Conduct and interpret a hypothesis test to find out if the population proportion of women signing up for triathlons with this company has changed at the \\(\\alpha = 0.10\\) significance level. "],
["type-i-and-type-ii-error.html", "Type I and Type II Error", " Type I and Type II Error We use inferences based on probability to make decisions in hypothesis testing. Basing decisions on probability, and not certainty, introduces the possibility of error – that is, of making incorrect inferences and drawing incorrect conclusions. These are NOT errors in calculation or process – these errors are inherent in hypothesis testing, even if you do everything right. Two types of error in hypothesis testing: Type I Error: Rejecting the null hypothesis (and accepting the alternative hypothesis), when the null hypothesis is actually true. Type II Error: Not rejecting the null hypothesis, when the null hypothesis is actually false. These errors have direct, real-world consequences. If you are testing whether a new program has decreased costs, and a Type I error occurs, then you would conclude that the program decreased costs when it actually had not done so. This could lead your company to prolong an ineffective program. On the other hand, consider the kettlebell hypothesis test we did on Handout #4. In that test we did not reject the null hypotheses, and so we concluded that there was no reason to shut down the manufacturing line. If a Type II error had occurred in that situation, then we would have let the line continue when actually the population mean weight of the bells was not 20lbs, thus leading us to ship product that did not conform to our 20lb specification. This is why hypothesis testing should be done while considering other information and context if at all possible. If a single hypothesis test yields an unexpected result, then another sample should be taken and the test should be repeated. It is highly unlikely that two Type I or Type II errors would occur in successive samples from the same population. The \\(\\mathbf{\\alpha}\\) significance level in a hypothesis test gives the probability of making a Type I error. If you do a hypothesis test at the \\(\\alpha = .01\\) significance level, you have a 0.01 probability (in other words, a 1% chance) of rejecting a null hypothesis that is actually true. To restate that, 1% of the time, a Type I error will happen. Example 1 A shampoo company has introduced a new formula of a particular shampoo. In the past, customers have been regularly surveyed about the shampoo, asked to rate the product on a scale of 1 to 10, and the shampoo has scored a mean of 6.2. An analyst at the company would like to test whether the mean satisfaction score changed after the new formula was introduced. \\[H_{0}: \\mu = 6.2 \\\\ H_{A}: \\mu \\neq 6.2\\] Suppose the analyst did this hypothesis test and, based on the sample, rejected \\(H_0\\). However, in reality the population mean customer satisfaction score is 6.2 (i.e. a value that makes the Null Hypothesis true), this would be a Type I Error. If the analyst did a hypothesis test and did not reject \\(H_0\\), but in reality the actual population mean customer satisfaction score was 5.1 (i.e. a value that means the Null Hypothesis is not true), this would be a Type II Error. Example 2 A human resources manager is interested in testing the effectiveness of a new training program. Prior to the training, employees could handle an average of 11.65 cases per day. The HR manager would like to determine whether the training has increased the average number of cases employees can handle per work day. \\[H_{0}:\\\\ H_{A}:\\] If the HR manager did a hypothesis test and did not reject \\(H_0\\), but the actual population mean of cases per day was 12, what type of error would this be? What if the actual population mean was 11, and the HR manager rejected the null hypothesis? What if the HR manager rejected the null hypothesis based on the sample, and the true population mean was 12.8? Example 3 A manufacturer makes test tubes for laboratory use. These test tubes must hold exactly 10 milliliters (ml) of liquid. If the production line goes out of adjustment, then it may produce test tubes that are on average too large or too small. A quality control analyst would like to check whether the production line is running properly. \\[H_{0}:\\\\ H_{A}:\\] What if the analyst rejected the null hypothesis based on the sample, and the true population mean was actually 10 ml? What if the analyst rejected the null hypothesis based on the sample, and the true population mean was actually 10.06? What if the analyst did not reject the null hypothesis, and the true population mean was actually 10ml? "],
["chapter-10.html", "Chapter 10", " Chapter 10 Inference About Means and Proportions with Two Populations "],
["notation-definitions.html", "Notation &amp; Definitions", " Notation &amp; Definitions In Chapter 10, we learn how to make inferences about two populations – either about the difference between two population means, or the difference between two population proportions. We draw those inferences by taking a sample from each population, and then using the sample data to do hypothesis testing or to calculate confidence intervals. In each problem, we will treat one population as Population 1, and the other as Population 2, and those numbers become subscripts in our notation (see below). Sample 1 is the sample taken from Population 1, and Sample 2 is the sample taken from Population 2. Population Parameters Sample Statistics \\(\\mu_{1}\\) mean of population 1 \\(\\bar{x}_{1}\\) mean of sample 1 \\(\\mu_{2}\\) mean of population 2 \\(\\bar{x}_{2}\\) mean of sample 2 \\(\\sigma_{1}\\) standard deviation of population 1 \\(s_{1}\\) standard deviation of sample 1 \\(\\sigma_{2}\\) standard deviation of population 2 \\(s_{2}\\) standard deviation of sample 2 \\(p_{1}\\) proportion of population 1 \\(\\bar{p}_{1}\\) proportion of sample 1 \\(p_{2}\\) proportion of population 2 \\(\\bar{p}_{2}\\) proportion of sample 2 \\(\\mu_{1}-\\mu_{2}\\) difference between two population means \\(n_{1}\\) the size of sample 1 \\(p_{1}-p_{2}\\) difference between two population proportions \\(n_{2}\\) the size of sample 2 "],
["hypothesis-tests-about-the-difference-between-population-means.html", "Hypothesis Tests about the Difference between Population Means", " Hypothesis Tests about the Difference between Population Means 1. Formulating the Hypotheses There are three possible forms of hypotheses. They each correspond to a different question you might want to ask about the difference between two population means, \\(\\mu_{1} - \\mu_{2}.\\) Note: in each of the hypotheses, \\(D_0\\) is a number. It is the hypothesized difference between the two population means. \\(D_0\\) should be set to 0, unless you are testing for a specific numerical difference between the means (like a difference of 5 or 12 or 178). If \\(\\mu_1 - \\mu_2 = 0\\), then there is no difference between the means - they are equal. Hypotheses for Hypothesis Tests about the Difference between Two Population Means, \\(\\mathbf{\\mu_1}-\\mathbf{\\mu_2}\\) Lower Tail Test Upper Tail Test Two-Tailed Test \\(H_0:\\mu_1 - \\mu_2 \\geq D_0\\) \\(H_A: \\mu_1 - \\mu_2 &lt; D_0\\) \\(H_0: \\mu_1 - \\mu_2 \\leq D_0\\) \\(H_A: \\mu_1 - \\mu_2 &gt; D_0\\) \\(H_0: \\mu_1 - \\mu_2 = D_0\\) \\(H_A: \\mu_1 - \\mu_2 \\neq D_0\\) Answers questions about If \\(\\mathbf{D_0 = 0}\\), then this test determines whether the mean of population 1, \\(\\mu_1\\), is less than the mean of population 2, \\(\\mu_2\\). If \\(\\mathbf{D_0 \\neq 0}\\), then this test determines whether the difference between the mean of population 1 and the mean of population 2 is less than \\(D_0\\). If \\(\\mathbf{D_0 = 0}\\), then this test determines whether the mean of population 1, \\(\\mu_1\\), is greater than the mean of population 2, \\(\\mu_2\\). If \\(\\mathbf{D_0 \\neq 0}\\), then this test determines whether the difference between the mean of population 1 and the mean of population 2 is greater than \\(D_0\\). If \\(\\mathbf{D_0 = 0}\\), then this test determines whether the mean of population 1, \\(\\mu_1\\), is different from the mean of population 2, \\(\\mu_2\\). If \\(\\mathbf{D_0 \\neq 0}\\), then this test determines whether the difference between the mean of population1 and the mean of population 2 is not equal to \\(D_0\\). Example 1 Suppose you have two populations and you want to know whether the mean of population 1 is lower than the mean of population 2. This corresponds to a lower tail test. No specific numerical difference between the two populations is asked for, so we set \\(D_0\\) to 0 and the hypotheses would be: \\[H_0: \\mu_1-\\mu_2\\geq 0 \\\\ H_A: \\mu_1 - \\mu_2 &lt; 0\\] Example 2 Suppose you have two populations, and you want to know whether the difference between their means is 25 or not. (Of course, a hypothesis test can only prove that the difference is NOT 25, not that it IS 25). This question matches with a two-tailed test. It asks for a specific numerical difference between the means of 25, so we set \\(D_0\\) to 25 and the hypotheses are: \\[H_0: \\mu_1 - \\mu_2 = 25 \\\\ H_A: \\mu_1 - \\mu_2 \\neq 25 \\] NOTE: The procedure explained in this handout is based on drawing two random, independent samples - one from each population - and taking the difference between the means of those samples, \\(\\bar{x}_1 - \\bar{x}_2\\). 2. Choosing and Calculating the Test Statistics The sample statistic of interest in hypothesis tests about the difference between two population means is the difference between the two sample means, \\(\\bar{x}_1 - \\bar{x}_2\\). We will need to determine how probable a given difference between two sample means is in order to decide whether or not to reject \\(H_0\\). To do that, we will calculate a test statistic using information from both samples, and sometimes from both populations. For hypothesis testing about the difference between two population means, the correct choice of test statistic depends on the sampling distribution of \\(\\bar{x}_1 - \\bar{x}_2\\), which in turn depends on what information you have about the variability in the underlying populations. The population standard deviations, \\(\\sigma_1\\) and \\(\\sigma_2\\), are sometimes known from prior knowledge, historical information, or past experience. On the other hand, if \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, then we use the information about variability of the samples: the standard deviations measured on the samples themselves, \\(s_1\\) and \\(s_2\\). When \\(\\mathbf{\\sigma_1}\\) and \\(\\mathbf{\\sigma_2}\\) are KNOWN Under the assumption that \\(H_0\\) is true as an equality, the sampling distribution of the difference between the two sample means, \\(\\bar{x}_1 - \\bar{x}_2\\), follows the \\(z\\) distribution. Therefore, we standardize the samples against the \\(z\\) distribution by calculating the following \\(z\\) test statistic: \\[z_{test} = \\frac{{(\\bar{x}}_1 - {\\bar{x}}_2) - D_0}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\ \\frac{\\sigma_2^2}{n_2}}}\\] where \\(\\bar{x}_1\\) = the mean of sample 1 \\(\\bar{x}_2\\) = the mean of sample 2 \\(D_0\\) = the hypothesized difference between the means (from \\(H_0\\), \\(H_A\\)) \\(\\sigma_1\\) = the standard deviation of population 1 \\(\\sigma_2\\) = the standard deviation of population 2 \\(n_1\\) = the sample size of sample 1 \\(n_2\\) = the sample size of sample 2 When \\(\\mathbf{\\sigma_1}\\) and \\(\\mathbf{\\sigma_2}\\) are UNKNOWN Under the assumption that \\(H_0\\) is true as an equality, the sampling distribution of the difference between the two sample means, \\(\\bar{x}_1 - \\bar{x}_2\\), follows the \\(t\\) distribution with \\(degrees\\ of\\ freedom\\) given by the equation below. Therefore, we standardize the samples against the \\(t\\) distribution by calculating the following \\(t\\) test statistic: \\[t_{test} = \\frac{{(\\bar{x}}_1 - {\\bar{x}}_2) - D_0}{\\sqrt{\\frac{s_1^2}{n_1} + \\ \\frac{s_2^2}{n_2}}}\\] where \\(\\bar{x}_1\\) = the mean of sample 1 \\(\\bar{x}_2\\) = the mean of sample 2 \\(D_0\\) = the hypothesized difference between the means (from \\(H_0\\), \\(H_A\\)) \\(s_1\\) = the standard deviation of sample 1 \\(s_2\\) = the standard deviation of sample 2 \\(n_1\\) = the sample size of sample 1 \\(n_2\\) = the sample size of sample 2 and the degrees of freedom are: \\(df = \\frac{\\left( \\frac{s_1^2}{n_1} + \\frac{s_{2}^2}{n_2} \\right)^2}{\\frac{1}{n_1 - 1}\\left( \\frac{s_1^2}{n_1} \\right)^2 + \\frac{1}{n_2 - 1}\\left( \\frac{s_2^2}{n_2} \\right)^2}\\) NOTE: if the degrees of freedom has decimal places, ALWAYS round DOWN to the nearest whole number. 3. Deciding whether or not to reject \\(\\mathbf{H_0}\\) There are two approaches to deciding whether or not to reject the Null: In the p-value approach, we compare the p-value of our test statistic to the \\(\\alpha\\) significance level and reject \\(H_0\\) if the p-value is less than or equal to the \\(\\alpha\\) significance level. In the critical value approach, we compare the test statistic to a critical value – this can be done with a diagram in which we use the critical value(s) to construct a rejection region or regions; if the test statistic is in a rejection region, we reject \\(H_0\\) and accept \\(H_A\\). Or, this step can be accomplished by following the mathematical rules given in the tables below. When to Reject \\(\\mathbf{H_0}\\) for z-test statistics For a Lower Tail Test For an Upper Tail Test For a Two-Tail Test p-value approach Look up the LT p-value of \\(z_{test}\\) If the LT p-value \\(\\leq \\alpha\\), reject \\(H_0\\) and accept \\(H_A\\). If the LT p-value &gt; \\(\\alpha\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. Look up the UT p-value of \\(z_{test}\\) If the UT p-value \\(\\leq \\alpha\\), reject \\(H_0\\) and accept \\(H_A\\). If the UT p-value &gt; \\(\\alpha\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. The two-tailed p-value is two times the one tailed p-value of \\(z_{test}\\). If the 2T p-value \\(\\leq \\alpha\\), reject \\(H_0\\) and accept \\(H_A\\). If the 2T p-value &gt; \\(\\alpha\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. CV approach If \\(z_{test} \\leq -z_{\\alpha}\\), reject \\(H_0\\) and accept \\(H_A\\). If \\(z_{test} &gt; -z_{\\alpha}\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. If \\(z_{test} \\geq z_{\\alpha}\\), reject \\(H_0\\) and accept \\(H_A\\). If \\(z_{test} &lt; z_{\\alpha}\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. If \\(z_{test} \\leq -z_{\\alpha/2}\\) OR \\(z_{test} \\geq z_{\\alpha/2}\\), reject \\(H_0\\) and accept \\(H_A\\). If \\(-z_{\\alpha/2} &lt; z_{test} &lt; z_{\\alpha/2}\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. NOTES: 1. \\(z_{test}\\) is the Test Statistic 2. \\(z_{\\alpha}, -z_{\\alpha}, and\\ z_{\\alpha/2}\\) are, respectively, UT, LT, and 2T Critical Values When to Reject \\(\\mathbf{H_0}\\) for t-test statistics For a Lower Tail Test For an Upper Tail Test For a Two-Tail Test p-value approach Look up the LT p-value of \\(t_{test}\\) If the LT p-value \\(\\leq \\alpha\\), reject \\(H_0\\) and accept \\(H_A\\). If the LT p-value &gt; \\(\\alpha\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. Look up the UT p-value of \\(t_{test}\\) If the UT p-value \\(\\leq \\alpha\\), reject \\(H_0\\) and accept \\(H_A\\). If the UT p-value &gt; \\(\\alpha\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. The two-tailed p-value is two times the one tailed p-value of \\(t_{test}\\). If the 2T p-value \\(\\leq \\alpha\\), reject \\(H_0\\) and accept \\(H_A\\). If the 2T p-value &gt; \\(\\alpha\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. CV approach If \\(t_{test} \\leq -t_{\\alpha}\\), reject \\(H_0\\) and accept \\(H_A\\). If \\(t_{test} &gt; -t_{\\alpha}\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. If \\(t_{test} \\geq t_{\\alpha}\\), reject \\(H_0\\) and accept \\(H_A\\). If \\(t_{test} &lt; t_{\\alpha}\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. If \\(t_{test} \\leq -t_{\\alpha/2}\\) OR \\(t_{test} \\geq t_{\\alpha/2}\\), reject \\(H_0\\) and accept \\(H_A\\). If \\(-t_{\\alpha/2} &lt; t_{test} &lt; t_{\\alpha/2}\\), do not reject \\(H_0\\); \\(H_A\\) is unsupported. NOTES: 1. \\(t_{test}\\) is the Test Statistic 2. \\(t_{\\alpha}, -t_{\\alpha}, and\\ t_{\\alpha/2}\\) are Critical Values 3. The correct t distribution to use when determining the p-value or the Critical Value depends on the degrees of freedom. 4. Interpreting the Test (Note: This explanation of interpretation holds for ALL hypothesis tests.) We start every hypothesis test with a question about the parameter of interest, so we must end every hypothesis test with the answer to that question. In other words, we must interpret the conclusion of our test in terms of the original question. Remember: in hypothesis testing you can never prove the null hypothesis. You can only prove the alternative hypothesis: when you reject the null and accept the alternative, then at your given \\(\\alpha\\) level of significance you may conclude that \\(H_A\\) is true. If you do not reject \\(H_0\\), then you must conclude that \\(H_A\\) is unsupported by the evidence. This gives us a clear guideline for how to interpret hypothesis tests: always look at the alternative hypothesis! In all that follows, you would substitute the actual words and numbers from your hypothesis test for the symbols. Notice that each interpretation simply states the alternative hypothesis in words, and says either that it is true or that it is unsupported by the evidence. How to Interpret a Hypothesis Test about the Difference between Two Population Populations, \\(\\mathbf{\\mu_1 - \\mu_2}\\) When you For a Lower Tail Test For an Upper Tail Test For a Two-Tailed Test Reject \\(\\mathbf{H_0}\\) If \\(\\mathbf{D_0 = 0}\\): at the \\(\\alpha\\) significance level, we can conlude that the mean population 1, \\(\\mu_1\\), is less than the mean of population 2, \\(\\mu_2\\). If \\(\\mathbf{D_0 \\neq 0}\\): at the \\(\\alpha\\) significance level, we can conclude that the difference between the mean of population 1, \\(\\mu_1\\), and the mean of population 2, \\(\\mu_2\\), is less than \\(D_0\\). If \\(\\mathbf{D_0 = 0}\\): at the \\(\\alpha\\) significance level, we can conclude that the mean of population 1, \\(\\mu_1\\), is greater than the mean of population 2, \\(\\mu_2\\). If \\(\\mathbf{D_0 \\neq 0}\\): at the \\(\\alpha\\) significance level, we can conclude that the difference between the mean of population 1, \\(\\mu_1\\), and the mean of population 2, \\(\\mu_2\\), is greater than \\(D_0\\). If \\(\\mathbf{D_0 = 0}\\): at the \\(\\alpha\\) significance level, we can conclude that the mean of population 1, \\(\\mu_1\\), is different than the mean of population 2, \\(\\mu_2\\). If \\(\\mathbf{D_0 \\neq 0}\\): at the \\(\\alpha\\) significance level, we can conclude that the difference between the mean of population 1, \\(\\mu_1\\), and the mean of population 2, \\(\\mu_2\\), is not equal to \\(D_0\\). Do not reject \\(\\mathbf{H_0}\\) If \\(\\mathbf{D_0 = 0}\\): at the \\(\\alpha\\) significance level, we cannot conlude that the mean population 1, \\(\\mu_1\\), is less than the mean of population 2, \\(\\mu_2\\). If \\(\\mathbf{D_0 \\neq 0}\\): at the \\(\\alpha\\) significance level, we cannot conclude that the difference between the mean of population 1, \\(\\mu_1\\), and the mean of population 2, \\(\\mu_2\\), is less than \\(D_0\\). If \\(\\mathbf{D_0 = 0}\\): at the \\(\\alpha\\) significance level, we cannot conclude that the mean of population 1, \\(\\mu_1\\), is greater than the mean of population 2, \\(\\mu_2\\). If \\(\\mathbf{D_0 \\neq 0}\\): at the \\(\\alpha\\) significance level, we cannot conclude that the difference between the mean of population 1, \\(\\mu_1\\), and the mean of population 2, \\(\\mu_2\\), is greater than \\(D_0\\). If \\(\\mathbf{D_0 = 0}\\): at the \\(\\alpha\\) significance level, we cannot conclude that the mean of population 1, \\(\\mu_1\\), is different than the mean of population 2, \\(\\mu_2\\). If \\(\\mathbf{D_0 \\neq 0}\\): at the \\(\\alpha\\) significance level, we cannot conclude that the difference between the mean of population 1, \\(\\mu_1\\), and the mean of population 2, \\(\\mu_2\\), is different from \\(D_0\\). NOTES: 1. \\(\\mu_1\\) is the true value of the mean of population 1 \\(\\mu_2\\) is the true value of the mean of population 2 2. \\(D_0\\) is the hypothesized difference between the two population means, \\(\\mu_1 - \\mu_2\\) Assumptions Underlying Hypothesis Tests about the Difference between Two Population Means All hypothesis tests use sampling distributions to determine the probability of sample statistics – that is how we determine whether or not to reject the null hypothesis. In order for us to be confident that our choice of sampling distribution for any given test really is the way the sample statistic is distributed, certain assumptions must be met. If the assumptions are not met – that is, if any given assumption is not true – then we cannot rely on the results of the hypothesis tests. They may mislead us, give us the wrong answers, and cause us to draw the wrong conclusions. For the hypothesis tests presented above about the difference between two population means, \\(\\mathbf{\\mu_{1} - \\mu_{2}}\\): Both samples must be random and independent of one another If both populations are normally distributed, then the sample sizes can be small If both populations are not normally distributed, or if one or more of the distributions are unknown, then the sample sizes must both be greater than or equal to 30 \\((n_1 \\geq 30\\ and\\ n_{2} \\geq 30)\\) When \\(\\mathbf{\\sigma_1}\\) and \\(\\mathbf{\\sigma_2}\\) are unknown (that is, when you are using the t distribution): It is recommended to have equal sample sizes, although this is not strictly necessary. If the sample sizes are nearly equal, then having sample sizes that satisfy \\(n_1 + n_2 \\geq 20\\) are adequate. if the populations have skewed distributions or have outliers, the sample sizes must be larger "],
["hypothesis-tests-about-the-difference-between-two-population-proportions.html", "Hypothesis Tests about the Difference between Two Population Proportions", " Hypothesis Tests about the Difference between Two Population Proportions 1. Formulating the Hypotheses There are three possible forms of hypotheses. They each correspond to a different question you might want to ask about the true value of the difference between two population proportions, \\(p_1 - p_2\\). Hypotheses for Hypothesis Tests about the Difference between Two Population Proportions, \\(\\mathbf{p_1}-\\mathbf{p_2}\\) Lower Tail Test Upper Tail Test Two-Tailed Test \\(H_0:p_1 - p_2 \\geq 0\\) \\(H_A: p_1 - p_2 &lt; 0\\) \\(H_0: p_1 - p_2 \\leq 0\\) \\(H_A: p_1 - p_2 &gt; 0\\) \\(H_0: p_1 - p_2 = 0\\) \\(H_A: p_1 - p_2 \\neq 0\\) Answers questions about If the true proportion of population 1, \\(p_1\\), is less than the true proportion of population 2, \\(p_2\\). If the true proportion of population 1, \\(p_1\\), is greater than the true proportion of population 2, \\(p_2\\). If the true proportion of population 1, \\(p_1\\), is different from the true proportion of population 2, \\(p_2\\). NOTE: The procedure explained in this handout is based on drawing two random, independent samples - one from each population - and taking the difference between the proportions of those samples, \\(\\bar{p}_1 - \\bar{p}_2\\). 2. Choosing and Calculating the Test Statistics NOTE: The logic is the same as all previous hypothesis tests: we are looking to see whether our sample values contradict the null hypothesis. The sample statistic of interest in hypothesis tests about the difference between two population proportions is the difference between the two sample proportions, \\({\\bar{p}}_1 - {\\bar{p}}_2.\\) In order to decide whether or not to reject \\(H_{0},\\) we need to use a sampling distribution of \\({\\bar{p}}_1 - {\\bar{p}}_2\\) to determine the probability of a given difference between two sample proportions, under the assumption that \\(H_0\\) is true as an equality – that is, assuming that the two proportions are equal. If the sample statistic \\({\\bar{p}}_1 - {\\bar{p}}_2\\) is less probable than the \\(\\alpha\\) significance level, then it contradicts the null hypothesis (\\(H_0\\)) and we will reject \\(H_0\\). Under the assumption that \\(H_0\\) is true as an equality, the sampling distribution of \\({\\bar{p}}_1 - {\\bar{p}}_2\\) follows the \\(z\\) distribution. Therefore, we have to standardize the sample information against the \\(z\\) distribution by calculating the \\(z\\) test statistic as follows: Calculating the \\(\\mathbf{z}\\) Test Statistic for Hypothesis Tests about \\(\\mathbf{p_1 - p_2}\\) First, calculate the pooled estimator, \\(\\mathbf{\\bar{p}}\\): \\[\\bar{p} = \\frac{n_1{\\bar{p}}_{1} + n_2{\\bar{p}}_{2}}{n_1 + n_2}\\] Second, calculate the \\(z\\) test statistic: \\[z_{test} = \\frac{{\\bar{p}}_1 - {\\bar{p}}_2}{\\sqrt{\\bar{p}\\left( 1 - \\bar{p} \\right)\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}}\\] where \\(\\bar{p}_1\\) = the proportion of sample 1 \\(\\bar{p}_2\\) = the proportion of sample 2 \\(\\bar{p}\\) = the pooled estimator (calculated in Step 1) \\(n_1\\) = the sample size of sample 1 \\(n_2\\) = the sample size of sample 2 3. Deciding whether or not to reject \\(\\mathbf{H_0}\\) The rules for z test statistics remain the same regardless of the context in which the z test statistic is used. See the rejection rules for z test statistics given above. 4. Interpreting the Test (Note: This explanation of interpretation holds for ALL hypothesis tests.) We start every hypothesis test with a question about the parameter of interest, so we must end every hypothesis test with the answer to that question. In other words, we must interpret the conclusion of our test in terms of the original question. Remember: in hypothesis testing you can never prove the null hypothesis. You can only prove the alternative hypothesis: when you reject the null and accept the alternative, then at your given \\(\\alpha\\) level of significance you may conclude that \\(H_A\\) is true. If you do not reject \\(H_0\\), then you must conclude that \\(H_A\\) is unsupported by the evidence. This gives us a clear guideline for how to interpret hypothesis tests: always look at the alternative hypothesis! In all that follows, you would substitute the actual words and numbers from your hypothesis test for the symbols. Notice that each interpretation simply states the alternative hypothesis in words, and says either that it is true or that it is unsupported by the evidence. How to Interpret a Hypothesis Test about the Difference between Two Population Proportions, \\(\\mathbf{p_1 - p_2}\\) When you For a Lower Tail Test For an Upper Tail Test For a Two-Tailed Test Reject \\(\\mathbf{H_0}\\) At the \\(\\alpha\\) significance level, we can conclude that \\(p_1\\) is less than \\(p_2\\). At the \\(\\alpha\\) significance level, we can conclude that \\(p_1\\) is greater than \\(p_2\\). At the \\(\\alpha\\) significance level, we can conclude that \\(p_1\\) is different from \\(p_2\\). Do not reject \\(\\mathbf{H_0}\\) At the \\(\\alpha\\) significance level, we cannot conclude that \\(p_1\\) is less than \\(p_2\\). At the \\(\\alpha\\) significance level, we cannot conclude that \\(p_1\\) is greater than \\(p_2\\). At the \\(\\alpha\\) significance level, we cannot conclude that \\(p_1\\) is different from \\(p_2\\). NOTES: \\(p_1\\) is the true value of the proportion of population 1 \\(p_2\\) is the true value of the proportion of population 2 Assumptions Underlying Hypothesis Tests About \\(\\mathbf{p_1-p_2}\\) In order to use this hypothesis test: The samples must be random and independent The following conditions must hold true: \\[n_1\\bar{p}_1 \\geq 5 \\\\ n_1(1 - \\bar{p}_1) \\geq 5 \\\\ n_2\\bar{p}_2 \\geq 5 \\\\ n_2(1 - \\bar{p}_2) \\geq 5\\] where \\(n_1\\) = the sample size of sample 1 \\(\\bar{p}_1\\) = the proportion of sample 1 \\(n_2\\) = the sample size of sample 2 \\(\\bar{p}_2\\) = the proportion of sample 2 "],
["formulating-hypotheses-what-is-the-difference.html", "Formulating Hypotheses: What is the difference?", " Formulating Hypotheses: What is the difference? Example 1 An article suggests that Business School students take more credit hours on average per semester than students in the liberal arts and sciences. An analyst wishes to see whether this is true at Wayne State. How should the null and alternative hypotheses be formulated? Use Business School students as Population 1. Example 2 Is there any difference between Mac users and PC users when it comes to mean customer satisfaction? Formulate the null and alternative hypotheses that could be used to test this question. Use Mac users as Population 1. Example 3 Sprint advertises that its prices for cell phone service are lower than Verizon. Formulate the null and alternative hypotheses to test Sprint’s claim. Use Sprint customers as Population 1. Example 4 An analyst suspects that, on average, iPhone users download 2 apps per month more than Android users. If the analyst wants to test for this difference, what should the null and alternative hypotheses be? (HINT: the test must determine whether the difference between the mean apps downloaded by iPhone users and Android users is 2 or not). Use iPhone users as Population 1. Hypothesis Tests about the Difference between Two Population Means: Exercises Exercise 1 Using data from Chapter 10, Exercise 7 (although changing the question asked) Consumer Reports uses a survey of readers to obtain customer satisfaction ratings for the nation’s largest retailers (Consumer Reports, March 2012). Each survey respondent is asked to rate a specified retailer in terms of six factors: quality of products, selection, value, checkout efficiency, service, and store layout. An overall satisfaction score summarizes the rating for each respondent with 100 meaning the respondent is completely satisfied in terms of all six factors. From past experience, Consumer Reports customer satisfaction scores have had a population standard deviation, regardless of store, of 12. We assume that customer satisfaction scores are normally distributed. Two independent, random samples were taken. A random sample of 25 Target customers had a mean customer satisfaction score of 79. A random sample of 30 Walmart customers had a mean customer satisfaction score of 71. Conduct and interpret a hypothesis test to determine whether the population mean customer satisfaction score for Target is higher than the population mean customer satisfaction score for Walmart at the \\(\\alpha = .01\\) significance level. Use Target customers as Population 1. Exercise 2 A winery was interested in the difference between white wine and red wine drinkers when it comes to average weekly wine consumption. Two random, independent samples were taken. A random sample of 35 white wine drinkers drank a mean of 17.6 ounces of wine per week with a standard deviation of 4.2. A random sample of 38 red wine drinkers drank a mean of 14.7 ounces of wine per week with a standard deviation of 3.64. Conduct and interpret a hypothesis test to determine if the mean weekly wine consumption of white wine drinkers differs from the mean weekly wine consumption of red wine drinkers at the \\(\\alpha = .05\\) significance level. Consider the white wine drinkers to be Population 1. Hypothesis Testing for the Difference between Two Population Proportions: Exercise A heart rate monitor manufacturer offers a service for users to upload their workout data and analyze it using a web-based app. The manufacturer is interested in different groups of athletes and how they compare in using this upload service. The question is: do triathletes and pure runners differ in terms of the proportion that uses the upload service? Random, independent samples of triathletes and pure runners were taken. In a sample of 292 triathletes, 79.45% used the upload service. In a sample of 315 pure runners, 72.38% used the upload service. Conduct and interpret a hypothesis test to determine whether triathletes and pure runners differ in the proportion that uses the upload service for their workout data. Use an \\(\\alpha = .01\\) significance level. Assign triathletes as Population 1 and pure runners as Population 2. "],
["confidence-intervals.html", "Confidence Intervals", " Confidence Intervals Hypothesis tests are one way to draw inferences about population parameters from sample statistics. Hypothesis tests can answer questions about whether parameters are equal to or not equal to one other, greater than one another, or less than one another. Confidence interval estimates are another way to draw inferences about population parameters from sample statistics. Confidence intervals give us information about what the difference between two population parameters actually is, to a specified degree of certainty. In the case of difference in means, the confidence interval estimate gives a range that contains the true difference between two population means. Confidence intervals are always calculated at a given level of confidence, which is a percentage that is related to the \\(\\alpha\\) significance level. The percent confidence is \\(100(1-\\alpha)\\). This table shows the conversions between some typical \\(\\alpha\\) significance levels and the corresponding confidence levels: \\(\\mathbf{\\alpha}\\) significance level Confidence level (%) 0.10 90% 0.05 95% 0.01 99% You can calculate these conversions yourself. For example, if \\(\\alpha = \\ 0.01,\\) the confidence level is: \\[100(1 - \\alpha) = 100(1 - 0.01) = 100(0.99) = .99\\%\\] A confidence interval consists of two parts: a point estimate plus or minus a margin of error. A point estimate is a single number that estimates a population parameter. The margin of error determines the range around the point estimate. In the equations that follow, the point estimate is the part before the \\(\\pm\\) and the margin of error is the part following it. The point estimate minus the margin of error is the lower bound (LB) of the confidence interval, and the point estimate plus the margin of error is the upper bound (UB). Confidence intervals are usually reported in square brackets, with the bounds separated by a comma, as in [LB, UB]. "],
["confidence-intervals-for-the-difference-between-two-population-means.html", "Confidence Intervals for the Difference between Two Population Means", " Confidence Intervals for the Difference between Two Population Means For the difference between two population means, choosing the correct formula for the confidence interval estimate depends on whether \\(\\sigma_1\\) and \\(\\sigma_2\\) - the population standard deviations for population 1 and 2 - are KNOWN or UNKNOWN. If \\(\\mathbf{\\sigma_1, \\sigma_2}\\) are KNOWN, the \\(100(1 - \\alpha)\\%\\) confidence interval for the difference between two population means, \\(\\mu_{1} - \\mu_{2},\\) is given by: \\[\\bar{x}_1 - \\bar{x}_2\\ \\pm z_{\\alpha/2}\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\] where \\(\\bar{x}_1\\) = the mean of sample 1 \\(\\bar{x}_2\\) = the mean of sample 2 \\(z_{\\alpha/2}\\) = the positive critical value of \\(z\\) at \\(\\frac{\\alpha}{2}\\) \\(\\sigma_1\\) = the standard deviation of population 1 \\(\\sigma_2\\) = the standard deviation of population 2 \\(n_1\\) = the sample size of sample 1 \\(n_2\\) = the sample size of sample 2 If \\(\\mathbf{\\sigma_1, \\sigma_2}\\) are UNKNOWN, the sample standard deviations \\(s_1\\) and \\(s_2\\) must be used instead, and the \\(100(1 - \\alpha)\\%\\) confidence interval for the difference between two population means, \\(\\mu_{1} - \\mu_{2},\\) is given by: \\[\\bar{x}_1 - \\bar{x}_2\\ \\pm \\ t_{\\alpha/2}\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\] where \\(t_{\\alpha/2}\\) has the degrees of freedom: \\[df = \\frac{\\left( \\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}} \\right)^{2}}{\\frac{1}{n_{1} - 1}\\left( \\frac{s_{1}^{2}}{n_{1}} \\right)^{2} + \\frac{1}{n_{2} - 1}\\left( \\frac{s_{2}^{2}}{n_{2}} \\right)^{2}}\\] (NOTE: Always round the degrees of freedom DOWN to the nearest whole number, no matter what) and \\(\\bar{x}_1\\) = the mean of sample 1 \\(\\bar{x}_2\\) = the mean of sample 2 \\(t_{\\alpha/2}\\) = the positive critical value of \\(t\\) at \\(\\frac{\\alpha}{2}\\) \\(s_1\\) = the standard deviation of sample 1 \\(s_2\\) = the standard deviation of sample 2 \\(n_1\\) = the sample size of sample 1 \\(n_2\\) = the sample size of sample 2 The template for the interpretation of a confidence interval for \\(\\mu_{1} - \\mu_{2}\\) is: With ___ % confidence, we can conclude that the true difference between \\(\\mathbf{\\mu_1}\\) and \\(\\mathbf{\\mu_2}\\) is between [Lower Bound] and [Upper Bound]. IMPORTANT: If the interval contains zero, then there is no statistically significant difference between the two means. As usual, you should fill in the meaning for the notation above, referring to the specific interval you are interpreting. "],
["confidence-intervals-for-the-difference-between-two-proportions.html", "Confidence Intervals for the Difference Between Two Proportions", " Confidence Intervals for the Difference Between Two Proportions A confidence interval estimate for the difference between two population proportions, \\(p_1 - p_2\\), is a range of numbers that contains the true difference between the two population proportions, to a given level of confidence. BE CAREFUL: confidence interval calculations for the difference between two population proportions DO NOT use the pooled estimator, \\(\\bar{p}\\), that is used in hypothesis testing for difference in proportions. The \\(100(1 - \\alpha)\\%\\) confidence interval estimate for the difference between two population proportions, \\(p_1 - p_2\\), is given by: \\[\\bar{p}_1 - \\bar{p}_2\\ \\pm \\ z_{\\alpha/2}\\sqrt{\\frac{\\bar{p}_1(1 - \\bar{p}_1)}{n_1} + \\frac{\\bar{p}_2\\left(1 - \\bar{p}_2 \\right)}{n_2}}\\ \\] where where \\(\\bar{p}_1\\) = the proportion of sample 1 \\(\\bar{p}_2\\) = the proportion of sample 2 \\(z_{\\alpha/2}\\) = the positive critical value of \\(z\\) at \\(\\frac{\\alpha}{2}\\) \\(n_1\\) = the sample size of sample 1 \\(n_2\\) = the sample size of sample 2 Remember, the point estimate minus the margin of error is the lower bound (LB) of the confidence interval, and the point estimate plus the margin of error is the upper bound (UB). Confidence intervals are usually reported in square brackets, with the bounds separated by a comma, as in [LB, UB]. The template for the interpretation of a confidence interval for \\(p_1 - p_2\\) is: With ___ % confidence, we can conclude that the true difference between \\(\\mathbf{p_1}\\) and \\(\\mathbf{p_2}\\) is between [Lower Bound] and [Upper Bound]. IMPORTANT: If the interval contains zero, then there is no statistically significant difference between the two proportions. Confidence Intervals for Differences: Exercises Exercise 1 Do Android or iPhone users differ in how many apps they download per month? What is the difference between them? An analyst takes two independent, random samples: one of Android users and one of iPhone users. The sample of 50 Android users downloaded a mean of 2.4 apps per month. The sample of 55 iPhone users downloaded a mean of 3.7 apps per month. Historical data suggest that the population standard deviation for Android users is 1 app per month, and the population standard deviation for iPhone users is 1.2 apps per month. Calculate and interpret the 95% confidence interval estimate for the difference in mean apps downloaded per month between Android and iPhone users. Assign population 1 to be iPhone users. Exercise 2 An HR manager would like to know how much the mean number of overtime hours differs between employees that have children at home and those who don’t have children at home. The manager takes a random sample of 35 employees with children at home and finds that these employees work a mean of 4.2 overtime hours and have a standard deviation of 4 hours. An independent random sample of 40 employees without children at home has a mean of 6.5 overtime hours and a standard deviation of 5.9 hours. Calculate and interpret a 99% confidence interval estimate for the difference in mean overtime hours between these two groups. Use employees with children at home as Population 1. The t-distribution appropriate for use in this instance has 68 degrees of freedom. (On your own time, calculate out the degrees of freedom – remember to always round down when calculating degrees of freedom this way!!) Exercise 3 A heart rate monitor manufacturer offers a service for users to upload their workout data and analyze it using a web-based app. The manufacturer is interested in different groups of athletes and how they compare in using this upload service. The company would like to know: what is the difference between the proportions of triathletes and cyclists who upload their workout data? Random, independent samples of triathletes and cyclists were taken. In a sample of 301 triathletes, 239 used the upload service. In a sample of 278 cyclists, 191 used the upload service. Construct and interpret a 90% confidence interval estimate for the difference between the proportions of triathletes and cyclists that use the upload service. Use Triathletes as Population 1. Can you confirm that there is indeed a difference between the proportions, at the 90% confidence level? If so, which group has a higher proportion who uploads their data? "],
["chapter-11.html", "Chapter 11", " Chapter 11 Inferences About Population Variances "],
["notation-definitions-1.html", "Notation &amp; Definitions", " Notation &amp; Definitions The standard deviation and the variance are measures of variability. They measure how much the values of a variable in a given sample or population vary above and below the mean. A population or sample with values that are closer to the mean would have a lower variance and a lower standard deviation than a population or sample with values more spread out around the mean. Population Parameters Sample Statistics \\(\\sigma\\) standard deviation of a population the lowercase Greek letter “sigma” \\(s\\) standard deviation of a sample \\(\\sigma^{2}\\) variance of a population “sigma squared” \\(s^2\\) variance of a sample The standard deviation is the square root of the variance: \\(\\sigma = \\sqrt{\\sigma^{2}}\\ and\\ s = \\sqrt{s^{2}}.\\) Likewise, the variance is the standard deviation squared, exactly how it appears in the notation. Example If the standard deviation of a sample is 12, what is the variance of the sample? We are given that \\(s\\) = 12. The sample variance is \\(s^2\\), so we can calculate the variance: \\(s^2=12^2=144\\). Thus, the variance of the sample is 144 and is denoted with \\(s^2\\). NOTE: problem descriptions could refer to either a variance OR a standard deviation. You must treat them accordingly in the equations for this chapter. If you notate the values properly, then you will know when a value needs to be squared and when it does not. Each of these letters can be subscripted to refer to specific populations and samples, like this: Population Parameters with subscripts Sample Statistics with subscripts \\(\\sigma_{1}\\) standard deviation of population 1 \\(s_{1}\\) standard deviation of sample 1 (from population 1) \\(\\sigma_{2}\\) standard deviation of population 2 \\(s_{2}\\) standard deviation of sample 2 (from population 2) \\(\\sigma_{1}^{2}\\) variance of population 1 \\(s_{1}^{2}\\) variance of sample 1 (from population 1) \\(\\sigma_{2}^{2}\\) variance of population 2 \\(s_{2}^{2}\\) variance of sample 2 (from population 2) This chapter will introduce two new test statistics, with corresponding sampling distributions and a host of critical values. The notation we will use is: Chi-Square \\(\\chi^2\\) \\[\\chi^{2}\\] This is the Greek letter “chi” squared. Pronounced \"kai square\" \\[\\chi_{test}^{2}\\] A chi-square test statistic \\[\\chi_{\\alpha,LT}^{2}\\] A critical value of chi-square at \\(\\alpha\\) in the lower tail. The critical value in a lower-tail test with a \\(\\chi^{2}\\) test statistic. \\[\\chi_{\\alpha,UT}^{2}\\] A critical value of chi-square at \\(\\alpha\\) in the upper tail. The critical value in an upper-tail test with a \\(\\chi^{2}\\) test statistic. \\[\\chi_{\\alpha/2,LT}^{2}\\] A critical value of chi-square at \\(\\alpha/2\\) in the lower tail. The lower tail critical value in a two-tailed test with a \\(\\chi^{2}\\) test statistic. \\[\\chi_{\\alpha/2,UT}^{2}\\] A critical value of chi-square at \\(\\alpha/2\\) in the upper tail. The upper tail critical value in a two-tailed test with a \\(\\chi^{2}\\) test statistic. \\[F\\] \\[F_{test}\\] An \\(F\\) test statistic \\[F_{\\alpha}\\] An upper-tail critical value of \\(F\\) at \\(\\alpha\\). The critical value in an upper-tail test with an \\(F\\)test statistic. \\[F_{\\alpha/2}\\] A two-tailed critical value of \\(F\\) at \\(\\alpha/2\\). The critical value in a two-tailed test with an \\(F\\) test statistic. "],
["hypothesis-tests-about-a-single-population-variance.html", "Hypothesis Tests about a Single Population Variance", " Hypothesis Tests about a Single Population Variance Formulating the Hypotheses: There are three possible forms of hypotheses. They each correspond to a different question you might want to ask about the true value of the population variance, \\(\\sigma^{2}.\\) \\(Note:\\ in\\ each\\ of\\ the\\ hypotheses,\\ \\sigma_{0}^{2}\\text{\\ is\\ a\\ number.\\ \\ It\\ is\\ a\\ value\\ that\\ }\\sigma^{2}\\text{\\ hypothesized\\ to\\ be.}\\) Hypotheses for Hypothesis Tests about a Single Population Variance, \\(\\sigma^{2}\\) Lower Tail Test Upper Tail Test Two-Tailed Test $\\(H_{0}:\\sigma^{2} \\ge | H_{0}:\\ \\sigma^{2} | leq \\sigma_{0}^{2}\\) \\[H_{A}:\\ \\sigma^{2 } &lt; \\sigma_{0}^{2}\\] q {0}^{2}$$ | } = {0}^{2}$ $ | \\[H_{A}:\\ \\sigma^{2 } &gt; \\sigma_{0}^{2}\\] | $$ | $$H_{0}:\\ \\sigma^{2 $ \\[ H_{A}:\\ \\sigma^{2} \\ neq \\sigma_{0}^{2}\\] Answers Questions About: If the true population variance, \\(\\sigma^{2},\\) is less than a given number, \\(\\sigma_{0}^{2}\\) If the true population variance, \\(\\sigma^{2},\\) is greater than a given number, \\(\\sigma_{0}^{2}\\) If the true population variance, \\(\\sigma^{2},\\) is different from a given number, \\(\\sigma_{0}^{2}\\) Upper Tail and Lower Tail Tests are called one-tailed or directional tests. Two-tailed tests are called non-directional tests. The Test Statistic: Under the assumption that \\(H_{0}\\) is true as an equality, the sampling distribution of the sample variance, \\(s^{2},\\ \\)follows the \\({\\text{chi}\\text{-}square\\ (\\chi}^{2})\\ distribution\\) distribution. Therefore, we standardize our sample against the \\(\\chi^{2}\\) distribution by calculating the following \\(\\chi_{\\text{test}}^{2}\\) test statistic: \\[\\chi_{\\text{test}}^{2} = \\frac{\\left( n - 1 \\right)s^{2}}{\\sigma_{0}^{2}}\\] where \\[{s^{2} = the\\ sample\\ variance\\ (or\\ the\\ sample\\ standard\\ deviation\\ squared) }{\\sigma_{0}^{2} = the\\ hypothesized\\ population\\ variance\\ \\left( \\text{from\\ the\\ hypotheses} \\right) }{n = the\\ sample\\ size}\\] and the degrees of freedom are \\(df = n - 1\\) NOTE: some problems will give you the variances (\\(s^{2}\\text{\\ and\\ }\\sigma_{0}^{2})\\), in which case you simply plug the values into the equation as is. However, some problems will give you standard deviations (\\(\\text{s\\ and\\ }\\sigma_{0}),\\) in which case you would need to square those values in this equation. Deciding whether or not to Reject \\(\\mathbf{H}_{\\mathbf{0}}\\): There are two approaches to deciding whether or not to reject the Null: In the p-value approach, we compare the p-value of our test statistic to the \\(\\mathbf{\\alpha}\\) significance level and reject \\(H_{0}\\) if the p-value is less than or equal to the \\(\\alpha\\) significance level. In the critical value approach, we compare the test statistic to a critical value – this can be done with a diagram in which we use the critical value(s) to construct a rejection region or regions; if the test statistic is in a rejection region, we reject \\(H_{0}\\) and accept \\(H_{A}.\\) Or, this step can be accomplished by following the mathematical rules given in the tables below. REMINDER: you can never accept the null hypothesis. Remember the swans. When to Reject \\(\\mathbf{H} _{\\mathbf{0}}\\) for \\(\\mathbf{\\tex t{Chi}}\\text{- }\\mathbf{Squar e\\ (}\\mathbf{\\ chi}^{\\mathbf{ 2}}\\mathbf{)}\\) Test Statistics: For a Lower Tail Test: For an Upper Tail Test: For a Two-Tailed Test: p-value approach: Calculate the lower tail \\(p\\text{- }\\text{value}\\) of \\(\\chi_{\\te xt{test}}^{2}\\) If the \\(\\text{LT\\ p}\\ text{-}value \\ leq \\ \\alpha,\\) then reject \\(H_{0}\\) and accept \\(H_{A}.\\) If the \\(\\text{LT\\ p}\\text{-}valu e &gt; \\ \\alpha,\\) then do not reject \\(H_{0}\\). \\(H_{A}\\) is unsupported. Calculate the upper tail \\(p\\text{- }\\text{value}\\) of \\(\\chi_{\\te xt{test}}^{2}\\) If the \\(\\text{UT\\ p}\\ text{-}value \\ leq \\ \\alpha,\\) then reject \\(H_{0}\\) and accept \\(H_{A}.\\) If the \\(\\text{UT\\ p}\\text{-}valu e &gt; \\ \\alpha,\\) then do not reject \\(H_{0}\\). \\(H_{A}\\) is unsupported. The two-tailed \\(p\\text{- }\\text{value}\\) is two times the one-tailed p-value of \\(\\chi_{\\tex t{test}}^{2}.\\) If the \\(2T\\ p\\ text{-}value \\ leq \\ \\alpha,\\) then reject \\(H_{0}\\) and accept \\(H_{A}.\\) If the \\(2T\\ p\\text{-}valu e &gt; \\ \\alpha,\\) then do not reject \\(H_{0}\\). \\(H_{A}\\) is unsupported. Critical Value: Approach If $ {}^{2} {,L T}^{2}, $then reject \\(H_{0}\\ \\)and accept \\(H_{A}\\) If \\(\\chi_ {\\text{test}}^ {2} &gt; \\chi_{\\a lpha,LT}^{2}\\), then do not reject \\(H_{0}\\t ext{.\\ }H_{A}\\) is unsupported. If $ {}^{2} {,U T}^{2}, $then reject \\(H_{0}\\) and accept \\(H_{A}\\) If \\(\\chi_ {\\text{test}}^ {2} &lt; \\chi_{\\a lpha,UT}^{2}\\), then do not reject \\(H_{0}\\t ext{.\\ }H_{A}\\) is unsupported. If \\(\\chi_{ \\text{test}}^{ 2} \\leq \\chi_{ \\alpha/2,LT}^{ 2}\\text{\\ OR}\\) \\(\\chi_{\\tex t{test}}^{2} \\ geq \\chi_{\\alp ha/2,UT}^{2}\\), then reject \\(H_{0}\\) and accept \\(H_{A}\\). If \\(\\chi_ {\\alpha/2,LT}^ {2} &lt; \\ \\chi_{ \\text{test}}^{ 2} &lt; \\chi_{\\al pha/2,UT}^{2}\\) then do not reject \\(H_{0}\\t ext{.\\ }H_{A}\\) is unsupported. NOTES: \\(\\chi_{\\text{t est}}^{2}\\ \\)is a Test Statistic \\(\\chi_{ \\alpha,LT}^{2} ,\\ \\chi_{\\alph a,UT}^{2},\\ \\c hi_{\\alpha/2,L T}^{2},\\ and\\ \\chi_{\\alpha/2 ,UT}^{2}\\ \\)are Critical Values \\(\\chi^{2}\\) is based on degrees of freedom. Be sure to calculate the appropriate degrees of freedom for the test you are performing. Interpreting the test: (Note: This explanation of interpretation holds for ALL hypothesis tests.) We start every hypothesis test with a question about the parameter of interest, so we must end every hypothesis test with the answer to that question. In other words, we must interpret the conclusion of our test in terms of the original question. Remember: in hypothesis testing you can never prove the null hypothesis. You can only prove the alternative hypothesis: when you reject the null and accept the alternative, then at your given \\(\\alpha\\) level of significance you may conclude that \\(H_{A}\\) is true. If you do not reject \\(H_{0},\\ \\)then you must conclude that \\(H_{A}\\) is unsupported by the evidence. This gives us a clear guideline for how to interpret hypothesis tests: always look at the alternative hypothesis! In all that follows, you would substitute the actual words and numbers from your hypothesis test for the symbols. Notice that each interpretation simply states the alternative hypothesis in words, and says either that it is true or that we cannot conclude that it is true. How to Interpret a Hypothesis Test about a Single Population Variance, \\(\\mathb f{\\sigma}^{\\ma thbf{2}}\\): When you: For a Lower Tail Test: For an Upper Tail Test: For a Two-Tailed Test: Reject \\(\\mathbf{H} _{\\mathbf{0}}\\) At the \\(\\text{α\\ }\\)significance level, we can conclude that \\(\\ sigma^{2}\\ \\)is less than \\(\\s igma_{0}^{2}\\). At the \\(\\text{α\\ }\\)significance level, we can conclude that \\(\\ sigma^{2}\\ \\)is greater than \\(\\s igma_{0}^{2}\\). At the \\(\\text{α\\ }\\)significance level, we can conclude that \\(\\ sigma^{2}\\ \\)is different than \\(\\s igma_{0}^{2}\\). Do not reject \\(\\mathbf{H} _{\\mathbf{0}}\\) At the \\(\\text{α\\ }\\)significance level, we cannot conclude that \\(\\ sigma^{2}\\ \\)is less than \\(\\s igma_{0}^{2}\\). At the \\(\\text{α\\ }\\)significance level, we cannot conclude that \\(\\ sigma^{2}\\ \\)is greater than \\(\\s igma_{0}^{2}\\). At the \\(\\text{α\\ }\\)significance level, we cannot conclude that \\(\\ sigma^{2}\\ \\)is different than \\(\\s igma_{0}^{2}\\). NOTES: \\(\\ sigma^{2}\\ \\)is the true value of the population variance \\(\\ sigma_{0}^{2}\\) is a number that is the hypothesized value of \\(\\sigma^{2}\\) Assumptions Underlying Hypothesis Tests about a Single Population Variance All hypothesis tests use sampling distributions to determine the probability of sample statistics. In order for us to be confident that our choice of sampling distribution for any given test really is the way the sample statistic is distributed, certain assumptions must be met. If the assumptions are not met – that is, if any given assumption is not true – then we cannot rely on the results of the hypothesis tests. They may mislead us, give us the wrong answers, and cause us to draw the wrong conclusions. When making inferences about a single population variance using \\(\\chi^{2}\\): The underlying population must be normally distributed. If it is not normal, then the sampling distribution is unknown, so we cannot use \\(\\chi^{2}\\)as the sampling distribution. The sample must be random. If it is not random, then the sampling distribution is unknown, so we cannot use \\(\\chi^{2}\\)as the sampling distribution. "],
["hypothesis-tests-about-two-population-variances.html", "Hypothesis Tests about Two Population Variances", " Hypothesis Tests about Two Population Variances Formulating the Hypotheses: There are two possible forms of hypotheses: upper tail and two-tailed. Whenever a directional question is asked (that is, a less-than or greater-than question), we will formulate the hypothesis test as an upper tail test. The two forms correspond to different questions you might want to ask about the true values of two population variances, \\(\\sigma_{1}^{2}\\text{\\ and\\ }\\sigma_{2}^{2}.\\) \\[{Note:\\ in\\ each\\ hypothesis,\\ \\sigma_{1}^{2} = \\ the\\ variance\\ of\\ population\\ 1,and }{\\text{\\ \\ }\\sigma_{2}^{2} = the\\ variance\\ of\\ population\\ 2}\\] Hypotheses for Hypothesis Tests about Two Population Variances, \\(\\sigma_{1}^ {2}\\text{\\ and\\ }\\sigma_{2}^{2}\\) Upper Tail Test Two-Tailed Test \\[H_{0}:\\ \\sig ma_{1}^{2} \\leq \\sigma_{2}^{2}\\] \\[H_{A}:\\ \\ sigma_{1}^{2} &gt; \\sigma_{2}^{2}\\] \\[H_{0}:\\ \\ sigma_{1}^{2} = \\sigma_{2}^{2}\\] \\[H_{A}:\\ \\sig ma_{1}^{2} \\neq \\sigma_{2}^{2}\\] Answers questions about: If the true population variance of population 1, \\(\\mathbf{\\si gma}_{\\mathbf{1}}^{2},\\) | gma}{ is greater than the true population variance of population 2, \\(\\mathbf{\\si gma}_{2}^{2}.\\) | gma}{2}^{2}.$ NOTE: This test also tells you if the variance of population 2, \\(\\mathbf{\\si gma}_{2}^{2}\\), | is less than the variance of population 1, \\(\\mathbf{\\si gma}_{\\mathbf{1}}^{2}\\). | If the true population variance of population 1, \\(\\mathbf{\\si athbf{1}}^{2},\\) is different from the true population variance of population 2, $\\mathbf{ The Upper Tail Test is a one-tailed or directional test. Two-tailed tests are called non-directional tests. The Test Statistic: To properly formulate the test statistic, you MUST use the larger sample variance as \\(s_{1}^{2}\\) and the smaller sample variance as \\(s_{2}^{2}\\). You can use this information to label the populations: the sample with the larger sample variance is sample 1 from Population 1, and the sample with the smaller variance is sample 2 from Population 2. By doing this, you will ensure that the test statistic is in the upper tail of the F distribution, and the critical values on the F table will work properly. For hypothesis testing about two population variances, the sample statistics of interest are the sample variances, \\(s_{1}^{2}\\text{\\ and\\ }s_{2}^{2}.\\) Under the assumption that \\(H_{0}\\) is true as an equality, the sampling distribution of the ratio of the sample variances follows the F distribution. Therefore, we standardize our samples against the F distribution by calculating the following F test statistic: \\[F_{\\text{test}} = \\ \\frac{s_{1}^{2}}{s_{2}^{2}}\\ \\] \\[with\\ Numerator\\ degrees\\ of\\ freedom = df_{1} = \\ n_{1} - 1\\] \\[\\ and\\ Denominator\\ degrees\\ of\\ freedom = df_{2} = \\ n_{2} - 1\\] where \\[{s_{1}^{2} = the\\ sample\\ variance\\ of\\ sample\\ 1 }{s_{2}^{2} = the\\ sample\\ variance\\ of\\ sample\\ 2 }{n_{1} = the\\ sample\\ size\\ of\\ sample\\ 1 }{n_{2} = the\\ sample\\ size\\ of\\ sample\\ 2}\\] NOTE: some problems will give you the variances (\\(s_{1}^{2}\\text{\\ and\\ }s_{2}^{2})\\), in which case you simply plug the values into the equation as is. However, some problems will give you standard deviations (\\(s_{1}\\text{\\ and\\ }s_{2}),\\) in which case you would need to square those values in this equation. Deciding whether or not to Reject \\(\\mathbf{H}_{\\mathbf{0}}\\): There are two approaches to deciding whether or not to reject the Null: In the p-value approach, we compare the p-value of our test statistic to the \\(\\mathbf{\\alpha}\\) significance level and reject \\(H_{0}\\) if the p-value is less than or equal to the \\(\\alpha\\) significance level. In the critical value approach, we compare the test statistic to a critical value – this can be done with a diagram in which we use the critical value(s) to construct a rejection region or regions; if the test statistic is in a rejection region, we reject \\(H_{0}\\) and accept \\(H_{A}.\\) Or, this step can be accomplished by following the mathematical rules given in the tables below. REMINDER: you can never accept the null hypothesis. Remember the swans. The Hypothesis Testing rejection rules for \\(F\\) test statistics are: When to Reject \\(\\mat hbf{H}_{\\mathbf{0}}\\) if the Test Statistic is \\(\\mathbf{F}\\) For an Upper Tail Test: For a Two-Tailed Test: p-value approach: Calculate the upper tail \\(p\\ text{-}\\text{value}\\) of \\(F_{\\text{test}}\\) If the \\(\\ text{UT\\ p}\\text{-}v alue \\leq \\ \\alpha,\\) then reject \\(H_{0}\\) and accept \\(H_{A}.\\) If the \\(\\text{UT\\ }p\\tex t{-}value &gt; \\alpha,\\) then do not reject \\(H_{0} \\text{.\\ }H_{A}\\ \\)is unsupported. The two-tailed \\(p\\t ext{-}\\text{value\\ o f\\ }F_{\\text{test}}\\) is 2 x upper tail p-value of \\(F_{\\text{test}}\\) If the \\(2T\\ p\\text{-}v alue \\leq \\ \\alpha,\\) then reject \\(H_{0}\\) and accept \\(H_{A}.\\) If the \\(2T\\ p\\tex t{-}value &gt; \\alpha,\\) then do not reject \\(H_{0} \\text{.\\ }H_{A}\\ \\)is unsupported. Critical Value Approach: If \\(F_{\\text{test}} \\ge q F_{\\alpha},\\ \\)then reject \\(H_{0}\\) and accept \\(H_{A}.\\) If \\(F_{\\text{test}} &lt; F_{\\alpha},\\ \\)then do not reject \\(H_{0}\\text{.\\ }\\) \\(H_{A}\\ \\)is unsupported. If \\(F _{\\text{test}} \\geq F_{\\alpha/2},\\ \\)then reject \\(H_{0}\\) and accept \\(H_{A}.\\) If \\(F_{\\text{test}} &lt; F_{\\alpha/2},\\ \\)then do not reject \\(H_{0}\\text{.\\ }\\) \\(H_{A}\\ \\)is unsupported. NOTES: $ F_{} $is a Test Statistic \\(F_{\\alpha}\\) and \\(F_{\\alpha/2}\\) are Critical Values. The degrees of freedom are \\(numera tor\\ df = n_{1} - 1\\) and \\(denomina tor\\ df = n_{2} - 1\\) Interpreting the test: (Note: This explanation of interpretation holds for ALL hypothesis tests.) We start every hypothesis test with a question about the parameter of interest, so we must end every hypothesis test with the answer to that question. In other words, we must interpret the conclusion of our test in terms of the original question. Remember: in hypothesis testing you can never prove the null hypothesis. You can only prove the alternative hypothesis: when you reject the null and accept the alternative, then at your given \\(\\alpha\\) level of significance you may conclude that \\(H_{A}\\) is true. If you do not reject \\(H_{0},\\ \\)then you must conclude that \\(H_{A}\\) is unsupported by the evidence. This gives us a clear guideline for how to interpret hypothesis tests: always look at the alternative hypothesis! In all that follows, you would substitute the actual words and numbers from your hypothesis test for the symbols. Notice that each interpretation simply states the alternative hypothesis in words, and says either that it is true or that we cannot conclude that it is true. How to Interpret a Hypothesis Test about Two Population Variances, \\(\\m athbf{\\sigma}_{\\math bf{1}}^{2}\\ | mathbf{\\text{\\ and\\ }}\\sigma_{\\ | mathbf{2}}^{\\mathbf{ 2}}\\mathbf{\\ }\\): | When you: For an Upper Tail Test: For a Two-Tailed Test: Reject \\(\\mat hbf{H}_{\\mathbf{0}}\\) This result can be interpreted two ways. Choose the interpretation that is appropriate in the context of the problem: At the \\(\\te xt{α\\ }\\)significance level, we can conclude that \\(\\mathb f{\\sigma}_{\\mathbf{1 }}^{2}\\ \\)is | greater than \\(\\ma thbf{\\sigma}_{\\mathb f{2}}^{2}\\). | At the \\(\\te xt{α\\ }\\)significance level, we can conclude that \\(\\mathb f{\\sigma}_{\\mathbf{2 }}^{2}\\ \\)is | less than \\(\\ma thbf{\\sigma}_{\\mathb f{1}}^{2}\\). | At the \\(\\te xt{α\\ }\\)significance level, we can conclude that \\(\\sigma_{1}^{2}\\ \\)is different than \\(\\sigma_{2}^{2}\\). Do not reject \\(\\mat hbf{H}_{\\mathbf{0}}\\) This result can be interpreted two ways. Choose the interpretation that is appropriate in the context of the problem: At the \\(\\te xt{α\\ }\\)significance level, we cannot conclude that \\(\\mathb f{\\sigma}_{\\mathbf{1 }}^{2}\\ \\)is | greater than \\(\\ma thbf{\\sigma}_{\\mathb f{2}}^{2}\\). | At the \\(\\te xt{α\\ }\\)significance level, we cannot conclude that \\(\\mathb f{\\sigma}_{\\mathbf{2 }}^{2}\\ \\)is | less than \\(\\ma thbf{\\sigma}_{\\mathb f{1}}^{2}\\). | At the \\(\\te xt{α\\ }\\)significance level, we cannot conclude that \\(\\sigma_{1}^{2}\\ \\)is different than \\(\\sigma_{2}^{2}\\). NOTES: \\(\\sigma_{1}^{2}\\ \\)is the true variance of population 1; \\(\\sigma_{2}^{2}\\ \\)is the true variance of population 2 Assumptions Underlying Hypothesis Tests about Two Population Variances All hypothesis tests use sampling distributions to determine the probability of sample statistics. In order for us to be confident that our choice of sampling distribution for any given test really is the way the sample statistic is distributed, certain assumptions must be met. If the assumptions are not met – that is, if any given assumption is not true – then we cannot rely on the results of the hypothesis tests. They may mislead us, give us the wrong answers, and cause us to draw the wrong conclusions. When making inferences about two population variances using \\(F\\): The underlying populations must both be normally distributed. If they are not normal, then the sampling distribution is unknown, so you cannot use F as the sampling distribution. Both samples must be random, and they must be independent of one another. If they are not random and independent, then the sampling distribution is unknown, so you cannot use F as the sampling distribution. "],
["the-chi-square-distribution.html", "The Chi-Square Distribution", " The Chi-Square Distribution The \\(\\chi^{2}\\) test statistics and \\(\\chi^{2}\\) sampling distributions are used for inferences about a single population variance (Ch 11), goodness of fit tests (Ch 12), and tests of independence (Ch 12). The Chi-Square distribution: is a probability distribution, so the area under the curve is 1 has degrees of freedom (df). The calculation for degrees of freedom will depend on the type of hypothesis test or confidence interval. is ALWAYS positive and is NOT symmetrical Implication: Critical Values in the upper tail and lower tail will be two different positive numbers The mean of a chi-square distribution is equal to its degrees of freedom. How is this relevant? If \\(\\chi_{test}^{2}\\mathbf{&gt; df}\\), then the test statistic is in the upper tail If \\(\\chi_{test}^{2}\\mathbf{&lt; df}\\), then the test statistic is in the lower tail The Chi-Square table contains Critical Values. Use the column headings marked “Area in Lower Tail” for LT critical values, and “Area in Upper Tail” for UT critical values. Excel is used to calculate p-values for Chi-Square test statistics. The Hypothesis Testing rejection rules for \\(\\chi^{2}\\) test statistics are listed in Ch 11: Handout #2, Part 1. Exercise: Practice using Chi-Square Exercise 1: a) If df = 20 and \\(\\alpha = 0.10\\), what is the critical value of Chi-square for an upper tail test? b) Suppose the test-statistic in this upper tail test is \\(\\chi_{test}^{2} = 31.4\\). Use the critical value approach to decide whether or not to reject the null hypothesis. c) Suppose the test-statistic in this upper tail test is \\(\\chi_{test}^{2} = 31.4\\). Use the p-value approach to decide whether or not to reject the null hypothesis. Exercise 2: a) If df = 45 and \\(\\alpha = 0.05\\), what is the critical value of Chi-Square for a lower tail test? b) Suppose that the test-statistic in this lower tail test is \\(\\chi_{test}^{2} = 46.1\\). Use the critical value approach to decide whether or not to reject the null hypothesis. c) Suppose that the test-statistic in this lower tail test is \\(\\chi_{test}^{2} = 46.1\\). Use the p-value approach to decide whether or not to reject the null hypothesis. Exercise 3: a) If df = 16 and \\(\\alpha = 0.05\\), what are the critical values of Chi-square for a two tailed test? b) Suppose that the test-statistic in this two-tailed test is \\(\\chi_{test}^{2} = 27.88.\\) Use the critical value approach to decide whether or not to reject the null hypothesis. c) Suppose that the test-statistic in this two-tailed test is \\(\\chi_{test}^{2} = 27.88.\\) Use the p-value approach to decide whether or not to reject the null hypothesis. "],
["exercise-hypothesis-tests-about-a-single-population-variance.html", "Exercise: Hypothesis Tests about a Single Population Variance", " Exercise: Hypothesis Tests about a Single Population Variance Example 1: A cranberry juice manufacturing company has designed its bottle filling process with a variance of 0.0225 ounces2. It conducts regular quality control tests to see whether the process is running according to this design specification. To that end, a quality control analyst takes a random sample of 25 bottles of cranberry juice and finds this sample has a variance of 0.04 ounces2. The population of bottle contents is assumed to be normal. Conduct and interpret a hypothesis test at the \\(\\alpha = 0.05\\) significance level to determine whether the population variance of the contents of the bottles is different from 0.0225. Is there evidence that the process is out of adjustment? Example 2: Kombucha is a sparkling, fermented tea which naturally contains some alcohol. In order to prevent kombucha from being regulated as an alcoholic beverage, makers must keep their tea from exceeding the legal limit for alcohol percentage. In order to stay legal, the manufacturer needs the population variance of their kombucha to be less than 0.00985. The population is normally distributed. The manufacturer takes a random sample of 66 bottles of kombucha and calculates a sample variance of 0.00624. Conduct and interpret a hypothesis test to determine whether the population variance of alcohol percentage in the kombucha is less than 0.00985 at an \\(\\alpha = 0.01\\) significance level. Is the kombucha brewer staying within the legal limit? "],
["the-f-distribution.html", "The F distribution", " The F distribution The F test statistic and F sampling distribution are used for inferences about two population variances (Ch 11), analysis of variance tests (Ch 13), and regression analysis (Ch 14-15). The \\(F\\) distribution: A probability distribution, so the area under the curve is 1 Has TWO separate degrees of freedom The numerator degrees of freedom \\((df_{1})\\), which is based on the numerator in the test statistic. The denominator degrees of freedom \\((df_{2})\\), which is based on the denominator of the test statistic. is ALWAYS positive and is NOT symmetrical In hypothesis tests about two population variances, we will construct our test statistic to be in the upper tail, so there will be one critical value \\({(F}_{\\alpha})\\) for the directional test (upper tail) and one critical value \\((F_{\\alpha/2})\\) for the two-tailed test. The F table contains a selection of Critical Values. Excel is used to calculate p-values for F test statistics. The Hypothesis Testing rejection rules for \\(F\\) test statistics are listed in Ch 11: Handout #2, Part 2. Exercise: Practice using F Exercise 1: Consider an upper tail test at an \\(\\alpha = .05\\) significance level, with \\(numerator\\ degrees\\ of\\ freedom\\ df = 25\\ and\\ denominator\\ degrees\\ of\\ freedom\\ df = 29.\\) What is the critical value? Suppose the test statistic is \\(F_{test} = 3.1\\). By the &gt; Critical Value approach, decide whether or not to reject the null &gt; hypothesis. Suppose the test statistic is \\(F_{test} = 3.1\\). By the &gt; p-value approach, decide whether or not to reject the null &gt; hypothesis. Exercise 2: Consider a two-tailed test at an \\(\\alpha = .05\\) significance level, with \\(numerator\\ degrees\\ of\\ freedom\\ df = 6\\ and\\ denominator\\ degrees\\ of\\ freedom\\ df = 7.\\) What is the critical value? Suppose that the test statistic is \\(F_{test} = 4.82\\). By the Critical Value approach, decide whether or not to reject the null hypothesis. Suppose that the test statistic is \\(F_{test} = 4.82.\\) By the p-value approach, decide whether or not to reject the null hypothesis. "],
["exercise-hypothesis-tests-about-two-population-variances.html", "Exercise: Hypothesis Tests about Two Population Variances", " Exercise: Hypothesis Tests about Two Population Variances Example 1: Our transport company would like to prove that we have lower variability in delivery times than our competitor. An analyst at our company obtains two random, independent samples. A random sample of 41 of our deliveries had a variance in delivery time of 2.56. A random sample of 61 of our competitor’s deliveries had a variance in delivery time of 4.15. Delivery times for both companies are normally distributed. Conduct and interpret a hypothesis test to determine whether we have a lower population variance than our competitor at an \\(\\alpha = 0.01\\) significance level. Example 2: A manufacturer of bathroom scales makes two models, Model A and Model B. Reliability, which is the extent to which a measurement is consistent (i.e. the same) when repeated under similar conditions, is an important characteristic of a bathroom scale. A scale that is more reliable can command a higher price. (HINT: Consider what reliability means in terms of the variance) The manufacturer conducted a series of tests in which it weighed a standard 150lb weight on the two scales. It then sampled the weights displayed on the scales. These were independent, random samples, and the populations are assumed to be normally distributed. The sample size for Model A was 21 weigh-ins and the sample variance was 0.16. The sample size for Model B was 26 weigh-ins and the sample variance was 0.0625. Conduct and interpret a hypothesis test to determine whether the population variances for these two scales differ at the \\(\\alpha = .05\\) significance level. Based on your analysis, which scale should have a higher price? "],
["chapter-12.html", "Chapter 12", " Chapter 12 Tests of Goodness of Fit, Independence "],
["multinomial-probability-distributions.html", "Multinomial Probability Distributions", " Multinomial Probability Distributions Goodness of Fit tests are hypothesis tests that determine whether or not a categorical variable has a particular probability distribution across its categories. Goodness-of-Fit tests employ the concept of a multinomial probability distribution, which is a probability distribution that gives the probabilities of three or more categorical outcomes. A multinomial probability distribution for k categorical outcomes is simply a list of probabilities, one per category of the variable, like this: \\[p_{1} = P_{1} = probability\\ of\\ an\\ outcome\\ being\\ in\\ category\\ 1\\] \\[p_{2} = P_{2} = probability\\ of\\ an\\ outcome\\ being\\ in\\ category\\ 2\\] \\[\\ldots\\] \\[p_k = P_k = probability\\ of\\ an\\ outcome\\ being\\ in\\ category\\ k\\] Category probabilities (\\(P_{1},P_{2},\\ldots,\\ P_{k}\\)) must always be between 0 and 1; that is, \\(0 \\leq P_{i} \\leq 1\\). Therefore, if probabilities are expressed as percentages in a problem, those percentages must be divided by 100 to get probabilities. In a properly formulated multinomial probability distribution, the sum of all of the category probabilities is 1. Example 1: Suppose you were studying a population of cats at an animal shelter. The variable you are interested in is color. The cats at the shelter come in three colors: tabby, black, and calico. 25% of the cats are tabby, 60% of the cats are black, and 15% of the cats are calico. Construct the multinomial probability distribution of color for this population. \\[i\\] Category Probability 1 Tabby 2 Black 3 Calico A multinomial probability distribution can be used to calculate expected frequencies. Expected frequencies are the number of outcomes per category that we expect to find in a sample drawn from a population with the given distribution. To calculate the expected frequencies for a sample of size \\(n\\), use the following equation: \\[e_{i} = n(P_{i})\\] where \\[e_i = the\\ expected\\ frequency\\ for\\ the\\ i^{th}\\ category \\\\ n = the\\ sample\\ size \\\\ P_i = the\\ probability\\ of\\ an\\ outcome\\ being\\ in\\ category\\ i\\] NOTE: Expected frequencies often come out with decimals. You should NOT round them to whole numbers. Keep them at four decimals unless they come out even to fewer decimal places. Example 2: Consider the multinomial probability distribution from Example 1. What are the expected frequencies for a random sample of size 50 drawn from this population? In other words, how many cats in the sample are expected to be tabby, how many black, and how many calico, theoretically? \\[i\\] Categoryi Probability \\[P_i\\] Expected Frequency \\[e_i\\] 1 Tabby 2 Black 3 Calico Example 3: a) A given variable has eight categories. What is the multinomial probability distribution for this variable if the probability is the same for all eight categories? b) What is the multinomial probability distribution if the variable has five categories and the probability is the same for all five categories? "],
["goodness-of-fit-test-for-a-single-categorical-variable.html", "Goodness-of-Fit Test for a Single Categorical Variable", " Goodness-of-Fit Test for a Single Categorical Variable Goodness-of-Fit tests test whether or not a categorical variable fits a particular multinomial probability distribution. A multinomial probability distribution gives the probabilities for three or more categorical outcomes of a given variable. The multinomial probability distribution for a variable with \\(\\text{k\\ }\\)categorical outcomes is: \\[p_1 = P_1 = probability\\ of\\ an\\ outcome\\ being\\ in\\ category\\ 1 \\\\ p_2 = P_2 = probability\\ of\\ an\\ outcome\\ being\\ in\\ category\\ 2 \\\\ \\ldots \\\\ p_k = P_k = probability\\ of\\ an\\ outcome\\ being\\ in\\ category\\ k\\] where \\[P_1,P_2,\\ldots,P_k\\ are\\ probabilities\\ between\\ 0\\ and\\ 1\\] \\[k = the\\ number\\ of\\ categories\\ in\\ the\\ variable\\] Formulating the Hypotheses: There is only one set of hypotheses for goodness-of-fit tests, and it incorporates a multinomial probability distribution. You must fill in the probabilities \\((P_1,\\ P_2,\\ \\ldots,\\ P_k)\\) for each problem. Hypotheses for Goodness-of-Fit Tests \\[H_{0}:The\\ probability\\ distribution\\ is\\ \\text{p}_{\\mathbf{1}} = P_{1},\\ \\tex t{p}_{\\mathbf{2}} = P_{2},\\ \\ldots,\\ \\text{p}_{\\mathbf{k}} = P_{k}\\] \\[H_{A}:The\\ prob ability\\ distribution\\ is\\ not\\ \\text{p}_{\\mathbf{1}} = P_{1},\\ \\tex t{p}_{\\mathbf{2}} = P_{2},\\ \\ldots,\\ \\text{p}_{\\mathbf{k}} = P_{k}\\] Answers questions about: If the population distribution of a given categorical variable fits \\(\\text{p}_{\\mathbf{1}} = P_{1},\\ \\te xt{p}_{\\mathbf{2}} = P_{2},\\ \\ldots,\\ \\text{p}_{\\mathbf{k}} = P_{k}\\) or not. NOTE: \\(P_1,P_2,\\ldots,P_k\\ are\\ probabilities\\ between\\ 0\\ and\\ 1\\) \\(k = the\\ number\\ of\\ categories\\) The Test Statistic: Here is the idea behind Goodness of Fit tests: if the null hypothesis is true, then the multinomial probability distribution in \\(H_{0}\\ \\)truly describes the population. This means that the observed frequencies from the sample should be the same as (or very close to) the expected frequencies calculated from the \\(H_{0}\\ \\)probability distribution. (For example, if 25% of the population fits into Category A, then about 25% of the sample should fit into Category A.) Large differences between the observed frequencies and the expected frequencies are evidence that the underlying population is NOT distributed according to the hypothetical multinomial probability distribution. Some variation between observed and expected frequencies will occur due to random chance, so we need to use our hypothesis testing techniques to see if the differences we see are statistically significant. The \\(\\text{α\\ significance\\ level}\\) sets a threshold at which we determine the sample distribution to be different enough from what we would expect under the null hypothesis, to contradict the null hypothesis. The size of the difference between the observed and expected frequencies is quantified in our \\(\\chi^{2}\\ \\)test statistic. When the test statistic is large enough to reject the null, we can conclude that the population is not distributed according to the hypothetical multinomial probability distribution. The test statistic for a Goodness-of-Fit Test is: \\[\\chi_{test}^{2} = \\sum_{i = 1}^{k}\\frac{\\left(f_{i} - e_{i}\\right)^{2}}{e_{i}}\\] \\[where\\] \\[e_i = nP_i = the\\ expected\\ frequency\\ for\\ category\\ i \\\\ f_i = the\\ observed\\ sample\\ frequency\\ for\\ category\\ i \\\\ k = the\\ number\\ of\\ categories \\\\ n = sample\\ size\\] \\[and\\] \\[the\\ degrees\\ of\\ freedom\\ are\\ df = k - 1\\] Deciding whether or not to Reject \\(H_0\\): Only large discrepancies between the observed and expected frequencies constitute evidence against the null hypothesis, so all Goodness-of-Fit tests are upper tail tests. (If the test statistic was in the lower tail, that would mean there were only small discrepancies between observed and expected). When to Reject \\(H_0\\) in a Goodness-of-Fit Test +==================================+==================================+ | | Always an Upper Tail Test: | +———————————-+———————————-+ | p-value approach: | Calculate the upper tail | | | \\(p-value\\) of | | | \\(\\chi_{test}^{2}\\) | | | | | | If the | | | \\(p-value \\leq \\alpha\\), | | | then reject \\(H_{0}\\) and accept | | | \\(H_{A}\\). | | | | | | If the | | | \\(p-value &gt; \\alpha\\), then | | | do not reject \\(H_{0}\\). \\(H_{A}\\) | | | is unsupported. | +———————————-+———————————-+ | Critical Value: Approach | Look up the UT Critical Value of | | | \\(\\chi^{2}\\), which is | | | \\(\\chi_{\\alpha,UT}^{2}\\) | | | | | | If | | | \\(\\chi_{test}^{2} \\geq \\chi_{\\alpha,UT}^{2}\\) | | | , $then | | | reject \\(H_{0}\\)and accept | | | \\(H_{A}\\). | | | | | | If | | | \\(\\chi_{\\text{tes | | | t}}^{2} &lt; \\chi_{\\alpha,UT}^{2}\\), | | | then do not reject | | | \\(H_{0}\\text{.\\ }H_{A}\\) is | | | unsupported. | +———————————-+———————————-+ | NOTES: | | | | | | 1) \\(\\chi_{\\text{test}}^{2}\\ \\)is | | | a Test Statistic | | | | | | 2) \\(\\chi_{\\alpha,UT}^{2}\\) is an | | | Upper Tail Critical Value | | | | | | 3) \\(\\chi^{2}\\) is based on | | | degrees of freedom. The | | | degrees of freedom in | | | Goodness-of-Fit Tests is | | | | | | \\[df = k - 1\\ where\\ k\\ i | | | s\\ the\\ number\\ of\\ categories\\] | | +———————————-+———————————-+ Interpreting the test: (Note: This explanation of interpretation holds for ALL hypothesis tests.) We start every hypothesis test with a question about the parameter of interest, so we must end every hypothesis test with the answer to that question. In other words, we must interpret the conclusion of our test in terms of the original question. Remember: in hypothesis testing you can never prove the null hypothesis. You can only prove the alternative hypothesis: when you reject the null and accept the alternative, then at your given \\(\\alpha\\) level of significance you may conclude that \\(H_{A}\\) is true. If you do not reject \\(H_{0},\\ \\)then you must conclude that \\(H_{A}\\) is unsupported by the evidence. This gives us a clear guideline for how to interpret hypothesis tests: always look to the alternative hypothesis! In all that follows, you must substitute the actual words and numbers from your hypothesis test for the symbols. Notice that each interpretation simply states the alternative hypothesis in words, and says either that we can or cannot conclude that it is true. How to Interpret a Goodness-of-Fit Test: When you: Interpretation: Reject \\(\\mathbf{H}_{\\mathbf{0}}\\) At the \\(\\text{α\\ }\\)significance level, we can conclude that the probability distribution of \\[*the variable*\\] is not \\(\\text{p}_{1} = P_{1},\\ \\text{p}_{2} = P_{2},\\ \\ldots,\\ \\text{p}_{k} = P_{k}\\). Do not reject \\(\\mathbf{H}_{\\mathbf{0}}\\) At the \\(\\text{α\\ }\\)significance level, we cannot conclude that the probability distribution of \\[*the variable*\\] is different from \\(\\text{p}_{1} = P_{1},\\ \\text{p}_{2} = P_{2},\\ \\ldots,\\ \\text{p}_{k} = P_{k}\\). NOTES: When \\(H_{0}\\ \\)is rejected, you can then look at the distribution of outcomes in the sample for information about the true population distribution. Comparing the expected frequencies to the observed frequencies is often helpful to learn about how the actual distribution differs from the \\(H_{0}\\) distribution. Assumptions Underlying These Hypothesis Tests All hypothesis tests use sampling distributions to determine the probability of sample statistics. In order for us to be confident that our choice of sampling distribution for any given test really is the way the sample statistic is distributed, certain assumptions must be met. If the assumptions are not met – that is, if any given assumption is not true – then we cannot rely on the results of the hypothesis tests. They may mislead us, give us the wrong answers, and cause us to draw the wrong conclusions. For Goodness-of-Fit Tests, the only assumption that must be satisfied is that the expected frequency for each category must be greater than five: \\[e_{i} \\geq 5\\] Exercise: A bike shop sells bikes with frames made from four different materials: carbon fiber, aluminum, steel, and titanium. In the past, the share of sales by type of frame was 17% carbon fiber, 53% aluminum, 12% steel, and 18% titanium. The bike shop decided to use a Goodness-of-Fit test at the \\(\\alpha = 0.05}\\) significance level to see whether the probability of sales by frame type has changed from what it was in the past. In a random sample of 250 bike sales, the following frequencies were observed: Category Frame Type Number of bikes sold 1 Carbon Fiber 66 2 Aluminum 128 3 Steel 16 4 Titanium 40 Start by formulating the null and alternative hypotheses: From the null hypothesis, fill in the Hypothesized Probability column. From the sample data, fill in the Observed Frequency column. For each row, the Expected Frequency is \\(e_{i} = n\\left(P_{i}\\right)\\). Frame ma terial Ca tegory Hypoth esized Proba bility Observed Fre quency Expected Fre quency Di fference Sq uared/ Expected Fre quency \\[(i)\\] \\[\\left( P_{i} \\ right)\\] \\[\\left( f_{i} \\ right)\\] \\[\\left( e_{i} \\ right)\\] \\[\\ frac{{(f _{i}\\ - \\ e_{i} )}^{2}}{ e_{i}}\\] Carbon Fiber 1 Aluminum 2 Steel 3 Titanium 4 \\[\\sum_ {}^{}\\ma thbf{f}_ {\\mathbf {i}}\\mat hbf{=}\\] \\[ \\chi_{\\t ext{test }}^{2} = \\ \\sum_ {i = 1}^ {4}{\\fra c{{(f_{i }\\ - \\ e_{i})}^ {2}}{e_{ i}} =}\\] The sum of the values in the last column is the \\(\\chi^{2}\\) test statistic. The test statistic and the critical value have k – 1 degrees of freedom, where k is the number of categories. Now you are ready to complete the hypothesis test. (left blank for hypothesis test) Because we have determined that the distribution has changed from what it was in the past, the bike shop now wants more details. Which categories have seen the biggest changes? What adjustments should be made to inventory? \\[CHART\\] "],
["test-of-independence-for-two-categorical-variables.html", "Test of Independence for Two Categorical Variables", " Test of Independence for Two Categorical Variables Tests of Independence show whether or not there is a relationship between two categorical variables measured on the same population. If two categorical variables are: • independent of one another, the two variables are not related • not independent of one another, the two variables are related (i.e. dependent) Formulating the Hypotheses: The default assumption, which forms the null hypothesis in this type of test, is that the two variables are independent of one another. If you can reject the null hypothesis, that means the two variables are related. There is only one set of hypotheses for tests of independence. The two categorical variables are \\[*Variable 1*\\] and \\[*Variable 2*\\], and you should fill in what those variables are in the context of the problem when doing a test of independence. It doesn’t matter which one is which, but you must maintain consistency in labeling throughout the problem. Hypotheses for Tests of Independence \\(H_{0}:\\left\\lbrack Variable\\ 1 \\right\\rbrack\\ text{\\ is\\ independent\\ of\\ }\\left\\lbrack Variable\\ 2 \\right\\rbrack\\) \\(H_{A}:\\left\\lbrack Variable\\ 1 \\right\\rbrack\\text{ \\ is\\ not\\ independent\\ of\\ }\\left\\lbrack Variable\\ 2 \\right\\rbrack\\) Answers questions about: Whether or not \\[*Variable 1*\\] is related to \\[*Variable 2*\\] NOTE: \\[*Variable 1*\\] and \\[*Variable 2*\\] must both be categorical variables The Test Statistic: The logic is the similar to a Goodness of Fit test. The expected frequencies are the counts we expect to get for each joint category in the sample if the two variables are independent (that is, if the null hypothesis is true). Observed frequencies are the actual frequencies in the random sample. Some variation between observed and expected frequencies will occur due to random chance, so we use our hypothesis testing techniques and set a threshold (the \\(\\text{α\\ significance\\ level}\\)) at which we consider the differences between observed and expected counts to be large enough to contradict the null hypothesis. If the differences between the expected and observed frequencies are large, then the sample contradicts the null hypothesis and it is rejected, with the conclusion that \\(H_{A}\\) is true: the variables are not independent. The size of the difference between the observed and expected frequencies is quantified in our \\(\\chi^{2}\\ \\)test statistic, which is calculated according to this formula: \\[\\chi_{\\text{test}}^{2} = \\sum_{i}^{}{\\sum_{j}^{}\\frac{\\left( f_{\\text{ij}} - e_{\\text{ij}} \\right)^{2}}{e_{\\text{ij}}}}\\] where \\[{f_{\\text{ij}} = the\\ observed\\ frequencies\\ from\\ the\\ sample }{e_{\\text{ij}} = the\\ expected\\ frequencies\\ if\\ the\\ variables\\ are\\ independent}\\] \\[\\text{and}\\] \\[the\\ degrees\\ of\\ freedom\\ are\\ df = \\left( r - 1 \\right)\\left( c - 1 \\right)\\] where \\[r = the\\ number\\ of\\ categories\\ in\\ Variable\\ 1\\] \\[c = the\\ number\\ of\\ categories\\ in\\ Variable\\ 2\\] We will use a contingency table with \\(i\\) rows and \\(\\text{j\\ }\\)columns to begin our calculation of this test statistic. Deciding whether or not to Reject \\(\\mathbf{H}_{\\mathbf{0}}\\): Only large differences between the observed and expected frequencies constitute evidence against the null hypothesis, so all Tests of Independence are upper tail tests. The two possible decisions are: \\(\\text{Reject\\ }H_{0}\\text{\\ and\\ accept\\ }H_{A}\\) \\(\\text{Do\\ not\\ reject\\ }H_{0}\\text{\\ and\\ conclude\\ that\\ }H_{A}\\text{\\ is\\ unsupported.}\\) REMINDER: you can never accept the null hypothesis. Remember the swans. When to Reject \\(\\mathbf {H}_{\\mathbf{0}}\\mathbf{\\ }\\)in Tests of Independence: Always an Upper Tail Test: p-value approach: Calculate the upper tail \\(p\\text{-}\\text{value}\\) of \\(\\chi_{\\text{test}}^{2}\\) If the \\(p\\text{-}value \\leq \\ \\alpha,\\) then reject \\(H_{0}\\) and accept \\(H_{A}.\\) If the \\(p\\text{-}value &gt; \\alpha\\), then do not reject \\(H_{0}\\). \\(H_{A}\\) is unsupported. Critical Value: Approach Look up the UT Critical Value of \\(\\chi^{2}\\), which is \\(\\chi_{\\alpha,UT}^{2}\\) If \\(\\chi_{\\text{test}}^{2} \\ geq \\chi_{\\alpha,UT}^{2},\\ \\)then reject \\(H_{0}\\)and accept \\(H_{A}\\). If \\(\\chi_{\\text{tes t}}^{2} &lt; \\chi_{\\alpha,UT}^{2}\\), then do not reject \\(H_{0}\\text{.\\ }H_{A}\\) is unsupported. NOTES: \\(\\chi_{\\text{test}}^{2}\\ \\)is a Test Statistic \\(\\chi_{\\alpha,UT}^{2}\\) is a Critical Value \\(\\chi^{2}\\) is based on degrees of freedom. The degrees of freedom in Tests of Independence is \\[d f = \\left( r - 1 \\right)\\left( c - 1 \\right)\\text{\\ where\\ r\\ is \\ the\\ number\\ of\\ categories}\\] \\[in\\ Variable \\ 1\\ and\\ c\\ is\\ the\\ number\\ of \\ categories\\ in\\ Variable\\ 2.\\] Interpreting the test: (Note: This explanation of interpretation holds for ALL hypothesis tests.) We start every hypothesis test with a question about the parameter of interest, so we must end every hypothesis test with the answer to that question. In other words, we must interpret the conclusion of our test in terms of the original question. Remember: in hypothesis testing you can never prove the null hypothesis. You can only prove the alternative hypothesis: when you reject the null and accept the alternative, then at your given \\(\\alpha\\) level of significance you may conclude that \\(H_{A}\\) is true. If you do not reject \\(H_{0},\\ \\)then you must conclude that \\(H_{A}\\) is unsupported by the evidence. This gives us a clear guideline for how to interpret hypothesis tests: always look to the alternative hypothesis! In all that follows, you would substitute the actual words and numbers from your hypothesis test for the symbols. Notice that each interpretation simply states the alternative hypothesis in words, and says either that it is true or that it is unsupported by the evidence. How to Interpret a Test of Independence: When you: Interpretation: Reject \\(\\mathbf{H}_{\\mathbf{0}}\\) At the \\(\\text{α\\ }\\)significance level, we can conclude that \\[*Variable 1*\\] is not independent of \\[*Variable 2*\\]. \\[*Variable 1*\\] and \\[*Variable 2*\\] are related. Do not reject \\(\\mathbf{H}_{\\mathbf{0}}\\) At the \\(\\text{α\\ }\\)significance level, we cannot conclude that \\[*Variable 1*\\] and \\[*Variable 2*\\] are related. NOTES: When \\(H_{0}\\ \\)is rejected, you can: look at the distribution of outcomes in the sample for information about the relationship between \\[*Variable 1*\\] and \\[*Variable 2*\\]. compare the expected frequencies to the observed frequencies to learn about how the two variables are related. Assumptions Underlying These Hypothesis Tests All hypothesis tests use sampling distributions to determine the probability of sample statistics. In order for us to be confident that our choice of sampling distribution for any given test really is the way the sample statistic is distributed, certain assumptions must be met. If the assumptions are not met – that is, if any given assumption is not true – then we cannot rely on the results of the hypothesis tests. They may mislead us, give us the wrong answers, and cause us to draw the wrong conclusions. For Tests of Independence, the only assumption that must be satisfied is that the expected frequency for each combination of categories must be greater than five: \\[e_{\\text{ij}} \\geq 5\\] Exercise. An analyst for a chain of coffee shops suspects that a customer’s drink choice is related to the time of day when the customer comes in. The three drink choices the analyst would like to consider are Hot Coffee, Iced Coffee, and Specialty Drinks. The analyst takes a random sample of 503 customers and then splits the customers into two groups: those who came in before 11am and those who came in after 11am. Of the customers who came in before 11am, 90 ordered Hot Coffee, 79 ordered Iced Coffee, and 72 ordered Specialty Drinks. Of the customers who came in after 11am, 67 ordered Hot Coffee, 74 ordered Iced Coffee, and 121 ordered Specialty Drinks. Conduct a Test of Independence to find out whether drink choice is related to time of visit at the α = .01 significance level. First, fill in the Observed Frequencies from the sample data, then calculate the Row and Column Totals. Second, calculate the expected frequency, \\(e_{\\text{ij}}\\), for each combination of categories: \\[e_{\\text{ij}} = \\ \\frac{(Row\\ i\\ Total)(Column\\ j\\ Total)}{n}\\] where i is the row number, j is the column number, and n is the sample size. Contingency Table: \\[\\mathbf{f}_{\\mathbf{\\text{ij}}}\\] Before 11am After 11am Row Total Hot Coffee Iced Coffee Specialty Drink Column Total NOTE: there are three categories of Drink Choice, so r = 3. There are two categories of Time of Day, so c = 2. You will need this information later to calculate degrees of freedom for the \\(\\chi^{2}\\) test statistic and critical value. Third, fill in the following table. The sum of the last column is the \\(\\chi_{\\text{test}}^{2}\\) test statistic: Drink Choice Time of Visit Observed Frequency Expected Frequency Difference Squared/Expected Frequency \\[\\text{ij}\\] \\[f_{\\text{ij}}\\] \\[e_{\\text{ij}}\\] \\[\\frac{{{(f}_{\\text{ij}} - \\ e_{\\text{ij}})}^{2}}{e_{\\text{ij}}}\\] 11 Hot Coffee Before 11am 12 Hot Coffee After 11am 21 Iced Coffee Before 11am 22 Iced Coffee After 11am 31 Specialty Drinks Before 11am 32 Specialty Drinks After 11am \\[\\chi_{\\text{test}}^{2} = \\sum_{i}^{}{\\sum_{j}^{}\\frac{{{(f}_{\\text{ij}} - \\ e_{\\text{ij}})}^{2}}{e_{\\text{ij}}}} = \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\] The \\(\\chi^{2}\\) test statistic and critical value for a test of independence has (r – 1)(c – 1) degrees of freedom (see note from previous page). Now you are ready to complete your hypothesis test. \\[CHART\\] "],
["chapter-13.html", "Chapter 13", " Chapter 13 Experimental Design and Analysis of Variance "],
["experimental-design.html", "Experimental Design", " Experimental Design An experimental design is a plan and a process for gathering data in which the researcher controls, modifies, or manipulates at least one variable. The purpose of gathering data in any research design, including experimental design, is to test hypotheses. In a study with an experimental design, the researcher begins with an idea about cause and effect. The researcher wants to test whether a treatment causes a response. To do this, each treatment is applied to a different group, and the mean responses of the groups are measured and compared to each other. Terminology: A factor is an explanatory variable, also called an Independent Variable (IV). It is the variable that the researcher controls, modifies, or manipulates to see what happens. The treatments are different levels or categories of the factor. Experimental units are the objects or subjects whose response the researcher is measuring. Experimental units are assigned to treatment groups which are groups of experimental units receiving a particular treatment. A response variable is the Dependent Variable (DV). It is what the researcher expects to vary with different treatments. A response is a single measurement of the response variable for a given experimental unit. Random assignment is the process of randomly assigning experimental units to treatment groups, or randomly assigning treatments to experimental units. Random assignment makes the groups equivalent at the outset of the study…. so any changes you see in the groups after applying the treatments, can be attributed to the treatments themselves. Two types of experimental design: Fully randomized experimental design: The treatments are randomly assigned to the groups, or, equivalently, the groups are randomly assigned to the treatments. In either case, the treatments are randomly assigned by the researcher. This is the key component of a fully randomized experimental design. The researcher imposes changes or otherwise manipulates the treatments that the groups receive. Can prove cause and effect. This is because the process of randomly assigning treatment groups makes the groups equivalent at the outset. Everything other than the treatment that could affect the response is randomly distributed among the groups. Therefore, any difference in the mean response between treatment groups must be due to the treatment itself. In a fully randomized experimental design, it is appropriate to call the Independent Variable (the factor) the cause and the Dependent Variable (the response) the effect. This is because you can prove cause and effect with a fully randomized experiment. Nonrandomized experimental design: also called quasi-experimental design. The treatments are not randomly assigned by the researcher, but different groups still receive different treatments. Cannot prove cause and effect. Both experimental designs contrast with another type of study design: observational design. In observational designs, outcomes are observed and measured for different groups, but there is no assignment of treatments to different groups. Researchers using observational designs look at ongoing behavior – they do not impose any changes in any variables. These studies are useful to illuminate associations and relationships between variables but they cannot prove cause and effect. In order for observational studies to have validity, they must use random sampling from the population of interest. In both nonrandomized experimental design and observational design, there is no way to completely rule out the possibility that there are pre-existing differences between treatment groups that explain differing responses. This is why these types of studies do not prove cause and effect. Excercise: xperimental Design: Exercise A local grocery store would like to increase sales on produce. This grocery store tracks purchases at the customer level. The grocery store decided that an effective way to boost sales of produce might be to redesign the weekly ad that customers receive. In the current design (Weekly Ad A), produce is on an inside page, the pictures are small, and the emphasis is on price. In the second design (Weekly Ad B), the produce is featured on the front page, beautifully photographed in full color, and there are recipes that incorporate produce. In the third design (Weekly Ad C) is the same as Design B except that there are no recipes. The grocery store takes a random sample of customers. These customers are randomly assigned to receive one of the three Weekly Ad designs. The customers’ purchases are tracked and their weekly spending on produce is measured. 1) What is the factor in this experiment? 2) What are two other terms for factor? 3) What are the treatments? How many treatments are there? 4) How many groups of customers will there be in this experiment? 5) What is the response variable in this experiment? 6) What is another term for response variable? 7) What are the experimental units? 8) Give an example of a response in this study. 9) Is this a fully randomized experiment? How do you know? 10) Suppose that differences in Weekly Spending on Produce are observed among the groups after they receive the Ads. Can the researcher conclude that the Weekly Ad Design caused those differences? "],
["introduction-to-analysis-of-variance-anova.html", "Introduction to Analysis of Variance (ANOVA)", " Introduction to Analysis of Variance (ANOVA) ANOVA is a hypothesis testing procedure that allows us to test the equality of three or more population means in one hypothesis test. ANOVA is used to test whether the mean response differs between treatment groups in fully randomized and quasi-experimental designs. It is also used to test between-group differences in observational studies when the samples are random. Formulating the Hypotheses: There is only one general form of hypotheses for ANOVA. It is adapted to each particular ANOVA depending on how many groups there are in a particular analysis. NOTE: \\(k = the\\ number\\ of\\ groups\\) Hypotheses for Analysis of Variance (ANOVA) \\(H_{0}:\\mu_{1} = \\ \\mu_{2} = \\ldots = \\ \\mu_{k}\\) \\[H_{A}:Not\\ all\\ of\\ the\\ means\\ are\\ equal\\] Answers questions about: Whether at least one group has a different mean than the others. NOTE: \\(\\mu_{1} ,\\mu_{2},\\ldots,\\mu_{k}\\text{\\ are\\ the\\ means\\ of\\ the\\ dependent\\ }\\left( \\text{response} \\right)\\text{\\ variable\\ for\\ each\\ group.}\\) \\(k = the\\ number\\ of\\ groups\\) the alternative hypothesis may be stated as \\(H_{A}:At\\ least\\ two\\ of\\ the\\ population\\) \\(\\text{means\\ are\\ not\\ equal}\\). These two forms of \\(H_{A}\\) are equivalent to one another. The Test Statistic: In order to understand the test statistic, we need to know something about the process behind ANOVA. ANOVA estimates the variance of the dependent variable two different ways, and compares those variance estimates using an F test statistic. The two variance estimates are: The between-treatments estimate of the variance, also called the Mean Square due to Treatments (MSTR) The MSTR is the variance estimated under the assumption that \\(H_{0}\\ \\)is true The within-treatments estimate of the variance, also called the Mean Square due to Error (MSE) The MSE is the variance estimated under the assumption that \\(H_{0}\\ \\)is false The test statistic in ANOVA procedures is \\(F\\), and it is the ratio of the two variance estimates: the MSTR and the MSE. The by-hand calculation of this test statistic is the subject of its own handout. The equation is: \\[F_{\\text{test}} = \\frac{\\text{MSTR}}{\\text{MSE}}\\] with \\[{numerator\\ degrees\\ of\\ freedom = df_{1} = k - 1 }{denominator\\ degrees\\ of\\ freedom = df_{2} = n_{T} - k}\\] where \\[{k = the\\ number\\ of\\ groups }{n_{T} = total\\ sample\\ size\\ (of\\ all\\ the\\ groups)}\\] Deciding whether or not to Reject \\(\\mathbf{H}_{\\mathbf{0}}\\): ANOVA \\(F\\) tests are always one-tailed, upper tail tests. The reason for this is due to the nature of the two variance estimates: the MSTR and the MSE. Here is how ANOVA works. If \\(H_{0}\\) is actually true and all the means of the groups are equal, then both the MSTR and the MSE will yield similar estimates of the true variance. Therefore, the ratio of MSTR and MSE (which is \\(F)\\ \\)will be a small number – too small to cause us to reject \\(H_{0}.\\) If, on the other hand, \\(H_{0}\\) is actually false, and the means of the groups are not all equal, the MSTR will vastly overestimate the true variance, while the MSE will estimate the true variance accurately. Therefore, the ratio of the MSTR and MSE (which is \\(F)\\ \\)will be a large number – large enough to cause us to reject \\(H_{0}.\\) We only want to reject the null when the difference between MSTR and MSE is large, hence the use of the upper tail test. When to Reject \\(\\mathbf{H}_{\\mathbf{0}}\\) in ANOVA (the Test Statistic is \\(\\mathbf {F}_{\\mathbf{\\text{test}}}\\)) Always an Upper Tail Test: p-value approach: Calculate the upper tail \\(p\\text{-}\\text{value}\\) of \\(F_{\\text{test}}.\\) If the \\(\\text{UT\\ p}\\text{-}value \\leq \\ \\alpha,\\) then reject \\(H_{0}\\) and accept \\(H_{A}.\\) If the \\(\\tex t{UT\\ p}\\text{-}value &gt; \\alpha\\), then do not reject \\(H_{0}\\text{.\\ }H_{A}\\) is unsupported. Critical Value Approach: If \\(F_{\\tex t{test}} \\geq F_{\\alpha},\\ \\)then reject \\(H_{0}\\) and accept \\(H_{A}.\\) If \\(F_{\\text{test}} &lt; F_{\\alpha}\\), then do not reject \\(H_{0}\\text{.\\ }H_{A}\\) is unsupported. NOTES: \\(F_{\\text{test}}\\ \\)is the Test Statistic \\(F_{\\alpha}\\) is the upper tail Critical Value (from the \\(F\\) table). The \\(\\text{numerator\\ degrees\\ of\\ freedom} = k - 1\\), \\(\\text{denominator\\ de grees\\ of\\ freedom} = n_{T} - k\\) where \\[{k = the\\ number\\ of\\ groups }{n_{T} = total\\ sample\\ size\\ (of\\ all\\ the\\ groups)}\\] Interpreting the test: How to Interpret an ANOVA: When you: Interpretation: Reject \\(\\mathbf{H}_{\\mathbf{0}}\\) At the \\(\\text{α\\ }\\)significance level, we can conclude that not all of the population means are equal. Could also be stated: At the \\(\\text{α\\ }\\)significance level, we can conclude that at least one of the populations has a different mean than the others. Do not reject \\(\\mathbf{H}_{\\mathbf{0}}\\) At the \\(\\text{α\\ }\\)significance level, we cannot conclude that any of the population means are different from one another. NOTES: When \\(H_{0}\\ \\)is rejected, you can then perform comparisons of pairs of means from the ANOVA to determine which one is different (or which ones are different) Assumptions Underlying ANOVA Tests ANOVA relies on three assumptions: For each population, the response variable (dependent variable) is normally distributed. Each treatment group in an experiment represents a potentially different population, and in each population, the response variable, also called the dependent variable, must be normally distributed. This holds true for groups in observational studies as well. The variance of the response variable is the same for all of the populations. The observations must be independent. In experimental designs, this assumption is satisfied by the use of random assignment to place experimental units into treatment groups. In observational designs, this assumption is satisfied by using random sampling. "],
["calculating-the-anova-test-statistic-and-summary-table.html", "Calculating the ANOVA Test Statistic and Summary Table", " Calculating the ANOVA Test Statistic and Summary Table We calculate the F test statistic from the sample data. In ANOVA, this is no easy task. It involves a series of new equations and careful attention to detail. NOTE: A balanced design is one in which all the groups are the same size (all have the same \\(n\\)). An unbalanced design is one in which at least one group is a different size than the others. The book and MindTap use two different sets of equations for calculating the \\(F_{\\text{test}}\\) in ANOVA, and one set only works if you have a balanced design. The formulas in this handout will work properly no matter which design is being used, so I recommend you just stick with these. For the following equations: There are \\(k\\) treatment groups. Each group has a sample size, called \\(n_{j}\\) for the jth treatment group (e.g. the second treatment group has a sample size of \\(n_{2}\\)). Keep this notation in mind. The \\(\\mathbf{\\text{F\\ }}\\)test statistic has a numerator and a denominator that must be calculated separately. The numerator of the F test statistic is the MSTR: Recall: the MSTR is the Mean Square due to Treatments, also called the between-treatments estimate of the variance. This is the estimate of the true variance under the assumption that \\(H_{0}\\) is true, which would mean that all the group population means were equal. Here are the four steps to calculate the \\(\\mathbf{\\text{MSTR}}\\): \\(First,\\ calculate\\ the\\ total\\ sample\\ size,n_{T}\\ :\\) \\[\\text{\\ \\ }n_{T} = n_{1} + n_{2} + \\ldots + n_{k}\\] NOTE: The total sample size, \\(n_{T}\\ \\)is just the sum of the sample sizes of each group \\(Second,\\ calculate\\ the\\ overall\\ sample\\ mean,\\overset{̿}{\\overline{x}}\\ :\\ \\) \\[\\overset{̿}{\\overline{x}} = \\ \\frac{n_{1}{\\overline{x}}_{1} + \\ n_{2}{\\overline{x}}_{2} + \\ldots + n_{k}{\\overline{x}}_{k}}{n_{T}}\\ \\] \\[{\\text{where\\ }n_{1},\\ n_{2},\\ldots,n_{k} = sample\\ size\\ for\\ each\\ group\\ }{{\\overline{x}}_{1},\\ {\\overline{x}}_{2},\\ldots,{\\overline{x}}_{k} = the\\ sample\\ mean\\ for\\ each\\ group}\\] NOTE: \\(\\overset{̿}{\\overline{x}} = the\\ overall\\ sample\\ mean,\\ is\\ also\\ called\\ the\\ Grand\\ Mean,\\ or\\ x\\text{-}\\text{triple}\\text{-}\\text{bar}\\) 3) \\(Third,\\ calculate\\ the\\ Sum\\ of\\ Squares\\ due\\ to\\ Treatments,\\ SSTR:\\) \\[SSTR = \\ \\sum_{j = 1}^{k}{n_{j}\\ ({\\overline{x}}_{j} - \\ \\overset{̿}{\\overline{x}}})^{2}\\] $${n_{j} = sample size for group j }{{}_{j} = the sample mean for group j }{k = the number of groups}$$ 4) \\(Finally,\\ use\\ the\\ SSTR\\ to\\ calculate\\ the\\ MSTR:\\) \\[MSTR = \\ \\frac{\\text{SSTR}}{k - 1}\\] \\[\\text{where\\ k} = the\\ number\\ of\\ groups \\] The denominator of the \\(\\mathbf{F}\\) test statistic is the \\(\\mathbf{\\text{MSE}}\\): The MSE is the Mean Square due to Error, also called the within-treatments estimate of the variance. This is the estimate of the true variance under the assumption that \\(H_{0}\\) is false, which would mean that at least one group population mean was different than the others. Here are the two steps to calculate the \\(\\mathbf{\\text{MSE}}\\): First, calculate the Sum of Squares due to Error, \\(\\text{SSE}\\): \\[SSE = \\sum_{j = 1}^{k}{(n_{j} - 1)(s_{j}^{2})}\\] \\[{\\text{where\\ }n_{j} = the\\ sample\\ size\\ of\\ group\\ j }{s_{j}^{2} = the\\ sample\\ variance\\ of\\ group\\ j }{k = the\\ number\\ of\\ groups}\\] 2) Second, use the \\(\\text{SSE}\\) to calculate the \\(\\text{MSE}\\): \\[MSE = \\frac{\\text{SSE}}{n_{T} - k}\\] Now calculate the test statistic, \\(\\mathbf{F}_{\\mathbf{\\text{test}}}\\): \\[\\mathbf{F}_{\\mathbf{\\text{test}}}\\mathbf{=}\\frac{\\mathbf{\\text{MSTR}}}{\\mathbf{\\text{MSE}}}\\] with \\[{\\mathbf{numerator\\ degrees\\ of\\ freedom = d}\\mathbf{f}_{\\mathbf{1}}\\mathbf{= k - 1} }{\\mathbf{denominator\\ degrees\\ of\\ freedom = d}\\mathbf{f}_{\\mathbf{2}}\\mathbf{=}\\mathbf{n}_{\\mathbf{T}}\\mathbf{- k}}\\] In the process of calculating the ANOVA test statistic, we organize all of the results of our calculations into an ANOVA table. ANOVA tables may be labeled with different terminology (e.g. the book uses different labels than Excel) but the numbers always represent the same calculations. ANOVA Summary Table: Equations *Source of Vari ation** Sum of Sq uares Degrees of Fr eedom Mean S quare F p- value C ritical value of F Treat ments \\[SST R = \\ \\ sum_{j = 1}^{k }{n_{j} \\ (\\ove rline{x _{j}} - \\ \\ove rset{̿}{ \\overli ne{x}}} )^{2}\\] \\[df _{1} = k - 1\\] \\[M STR = \\ \\frac{ \\text{S STR}}{k - 1}\\] \\[F _{\\text {test}} = \\fra c{\\text {MSTR}} {\\text{ MSE}}\\] Upper tail p-value for \\[F_{ \\text{t est}}\\] \\[F _{\\alph a,UT}\\] Error $ \\(SSE = \\sum_{j = 1}^{ k}{(n_{ j} - 1) (s_{j}^ {2})}\\)$ $ \\(df_{2} = n_{T } - k\\)$ \\[M SE = \\f rac{\\te xt{SSE} }{n_{T} - k}\\] Total \\[SST = SSTR + SSE\\] \\[n_{T } - 1\\] ANOVA Summary Table: Simplified *Source of Vari ation** Sum of Sq uares Degrees of Fr eedom Mean S quare F p- value C ritical value of F Treat ments $ \\(\\text{ SSTR}\\)$ nu merator $ df_{1}$ $ \\(\\text{ MSTR}\\)$ \\[F_{ \\text{t est}}\\] Upper tail p-value for \\[F_{ \\text{t est}}\\] \\[F _{\\alph a,UT}\\] Error \\[\\text {SSE}\\] deno minator $ df_{2}$ \\[\\text {MSE}\\] Total \\[SST = SSTR + SSE\\] \\[n_{T } - 1\\] Exercise: ANOVA Exercise 1. Let’s suppose that the grocery store from Chapter 13: Handout #1 decided to perform the experiment. The objective of the experiment is to see whether any one of the three different ad designs causes people to buy more produce. The store randomly samples customers from its database, and then randomly assigns 22 of those customers to receive Weekly Ad A, 21 of the customers to receive Weekly Ad B, and 20 of the customers to receive Weekly Ad C. Weekly produce purchases by each customer are tracked, and the sample mean and variance of weekly produce spending for each group are calculated. For each group, the weekly spending on produce is normally distributed, and the population variances are assumed to be equal. j Treatment \\[n_{j}\\] Sample Mean \\[{\\overline{x}}_{j}\\] Sample Variance \\[{s_{j}}^{2}\\] 1 Weekly Ad A 25.11 9.21 2 Weekly Ad B 28.51 10.05 3 Weekly Ad C 29.20 9.91 Use ANOVA to test whether the means for the three treatment groups are equal at an \\(\\alpha\\ = \\ 0.05\\) significance level. Use your results to fill in the ANOVA table. ANOVA Table for Exercise 1: Grocery Store Ads Source of Variation Sum of Squares Degrees of Freedom Mean Square F Critical value of F Treatments Error Total Exercise 2. In a study with a fully randomized experimental design, four treatments were applied to randomly assigned groups of experimental units. The first treatment group had 12 experimental units, the second treatment group had 9 experimental units, the third treatment group had 9 experimental units, and the fourth treatment group had 14 experimental units. Given the following ANOVA table, test whether the population means are equal at the α = 0.01 significance level. Source of Variation Sum of Squares Degrees of Freedom Mean Square F Critical value of F Treatments Error 50 Total 65 "],
["fishers-least-significant-difference-a-multiple-comparison-procedure.html", "Fisher’s Least Significant Difference: a Multiple Comparison Procedure", " Fisher’s Least Significant Difference: a Multiple Comparison Procedure If the null hypothesis in an ANOVA is rejected, then you may conclude that not all the population means are equal. However, ANOVA by itself cannot tell you which of the means are different. It could be only one group that has a different mean with the rest being equal, or two groups, or all of the groups could have different means. ANOVA is silent in that respect. Consider the case where there are four groups in an ANOVA analysis, so the null and alternative hypotheses are: \\(H_{0}:\\ \\mu_{1} = \\mu_{2} = \\mu_{3} = \\mu_{4}\\) \\(H_{A}:Not\\ all\\ the\\ population\\ means\\ are\\ equal\\) If the null is rejected, then the question becomes: which mean is not equal to the others? Are they all different, or is only one or two different from the others? This is where multiple comparison procedures come in. Multiple comparison procedures are only used after ANOVA confirms a difference, to find out which groups have a different mean from the others. Multiple comparison procedures test pairs of means, and you have to run one test per pair. Mathematically, we need to ask: \\(\\text{is\\ }\\mu_{1} \\neq \\mu_{2}?\\ \\text{is\\ }\\mu_{2} \\neq \\mu_{3}?\\ \\text{is\\ }\\mu_{3} \\neq \\mu_{4}?\\) is \\(\\mu_{1} \\neq \\mu_{3}?\\ \\text{is\\ }\\mu_{2} \\neq \\mu_{4}?\\text{is}\\ \\mu_{1} \\neq \\mu_{4}?\\ \\ \\)Notice how those questions look like alternative hypotheses in a two tailed test? Well, that is exactly what they are used for in multiple comparisons. Fisher’s Least Significant Difference (Fisher’s LSD) procedure is commonly used for multiple comparisons. Fisher’s LSD is a special type of two-tailed t test. There are three different methods to use Fisher’s LSD: a traditional t test; a modified t test; and, a confidence interval. All three methods will come to the same conclusion, because they are all different ways of using the same information. All three ways are presented in this handout and in 13.3 in the book, but in this class only METHOD 2 will be used on the homework and on exams. Method 1: Fisher’s LSD as a series of traditional t tests: Take any two groups from the ANOVA (here they are labeled \\(i\\) and \\(j\\) just to differentiate them). Set an \\(\\alpha\\) significance level. The hypotheses are: \\(H_{0}:\\ \\mu_{i} = \\mu_{j}\\) \\(H_{A}:\\ \\mu_{i} \\neq \\mu_{j}\\) The test statistic is: \\(t_{\\text{test}} = \\frac{{\\overline{x}}_{i}\\ - \\ {\\overline{x}}_{j}}{\\sqrt{\\text{MSE}\\left( \\frac{1}{n_{i}}\\ + \\ \\frac{1}{n_{j}} \\right)}}\\) \\[\\text{where\\ }\\] \\({\\overline{x}}_{i} = the\\ sample\\ mean\\ of\\ group\\ i\\) \\[{\\overline{x}}_{j} = the\\ sample\\ mean\\ of\\ group\\ j\\] \\[MSE = Mean\\ Square\\ Error\\ from\\ the\\ ANOVA\\] \\[n_{j} = sample\\ size\\ of\\ the\\ j^{\\text{th}}\\text{\\ group}\\] and the degrees of freedom are \\(df_{2} = n_{T} - k\\) from the ANOVA The rejection rules are: \\(p\\text{-}value\\ approach:\\ Reject\\ H_{0}\\text{\\ if\\ }2T\\ p\\text{-}value \\leq \\ \\alpha\\) \\(Critical\\ Value\\ approach:Reject\\ H_{0}\\text{\\ if\\ }t_{\\text{test}} \\leq - t_{\\frac{\\alpha}{2}}\\text{\\ or\\ }t_{\\text{test}} \\geq t_{\\frac{\\alpha}{2}}\\ \\) \\[with\\ df = n_{T}\\ –k\\] \\[n_{T} = total\\ sample\\ size,\\ from\\ the\\ ANOVA\\] \\[k = the\\ number\\ of\\ groups\\ in\\ the\\ ANOVA\\] NOTE: the df here are the same df from the \\(\\text{MSE}\\) in the ANOVA If the null hypothesis is rejected, then the two group means \\(\\mu_{i}\\text{\\ and\\ }\\mu_{j}\\ \\)are different. If the null hypothesis is not rejected, then there is no evidence that \\(\\mu_{i}\\text{\\ and\\ }\\mu_{j}\\ \\)are different. Conduct one test for each possible pair of means from the ANOVA Method 2: Fisher’s LSD as a series of modified t tests: (This is the one we will use in this course) Take any two groups from the ANOVA (here they are labeled \\(i\\) and \\(j\\) just to differentiate them). Set an \\(\\alpha\\) significance level. The hypotheses are: \\(H_{0}:\\ \\mu_{i} = \\mu_{j}\\) \\(H_{A}:\\ \\mu_{i} \\neq \\mu_{j}\\) The test statistic is: \\({\\overline{x}}_{i}\\ - \\ {\\overline{x}}_{j}\\) \\(\\text{where}\\) \\({\\overline{x}}_{i} = the\\ sample\\ mean\\ of\\ group\\ i\\) \\({\\overline{x}}_{j} = the\\ sample\\ mean\\ of\\ group\\ j\\) The Least Significant Difference is: \\(LSD = t_{\\alpha/2}\\sqrt{\\text{MSE}\\left( \\frac{1}{n_{i}}\\ + \\ \\frac{1}{n_{j}} \\right)}\\text{\\ where\\ the\\ }2T\\ t_{\\alpha/2}\\text{\\ Critical\\ Value\\ has\\ }df_{2} = n_{T} - k\\) \\[\\text{and}\\] \\[{n_{i} = sample\\ size\\ of\\ the\\ i^{\\text{th}}\\text{\\ group} }{n_{j} = sample\\ size\\ of\\ the\\ j^{\\text{th}}\\text{\\ group} }{n_{T} = the\\ total\\ sample\\ size\\ from\\ the\\ ANOVA }{k = the\\ number\\ of\\ groups\\ in\\ the\\ ANOVA}\\] and the rejection rule is: \\(\\text{If\\ }\\left| {\\overline{x}}_{i}\\ - \\ {\\overline{x}}_{j} \\right| \\geq LSD\\ then\\ reject\\ H_{0}\\text{\\ and\\ accept\\ }H_{A}\\text{.\\ }\\) \\(\\text{If\\ }\\left| {\\overline{x}}_{i}\\ - \\ {\\overline{x}}_{j} \\right| &lt; LSD\\ then\\ \\text{do\\ not}\\text{\\ reject\\ }H_{0}\\text{\\ and\\ conclude\\ }H_{A}\\text{\\ is\\ unsupported.\\ \\ \\ }\\) If the null hypothesis is rejected, then the two group means \\(\\mu_{i}\\text{\\ and\\ }\\mu_{j}\\) are different. If the null hypothesis is not rejected, then there is no evidence that \\(\\mu_{i}\\text{\\ and\\ }\\mu_{j}\\) are different. Conduct one test for each possible pair of means from the ANOVA. Note that this method is very efficient to use for multiple comparisons in balanced designs (i.e. those in which the groups are all the same size). In balanced designs, you only have to compute the LSD once because it will be the same no matter which two groups you are testing. Then you can compute the difference between each pair of means and compare the absolute value of each difference to the LSD to see whether to reject the null. In unbalanced designs (i.e. those in which the groups are not all the same size), you will have to compute the LSD individually for each test. Method 3: Fisher’s LSD as a series of confidence intervals: Take any two groups from the ANOVA (here they are labeled \\(i\\) and \\(j\\) just to differentiate them). Set an α significance level. The hypotheses are: \\(H_{0}:\\ \\mu_{i} = \\mu_{j}\\) \\(H_{A}:\\ \\mu_{i} \\neq \\mu_{j}\\) Calculate a \\(100\\left( 1 - \\alpha \\right)\\%\\) confidence interval for the difference between \\(\\mu_{i}\\text{\\ and\\ }\\mu_{j}\\) using the following equation: \\({\\overline{x}}_{i}\\ - \\ {\\overline{x}}_{j}\\ \\pm LSD\\ where\\ \\) \\(LSD = t_{\\frac{\\alpha}{2}}\\sqrt{\\text{MSE}\\left( \\frac{1}{n_{i}}\\ + \\ \\frac{1}{n_{j}} \\right)}\\ \\ and\\ the\\ t\\ critical\\ value\\ has\\ df = n_{T} - k\\) The rejection rule is: \\(\\text{Reject\\ }H_{0}\\text{\\ if\\ the\\ confidence\\ interval\\ does\\ not\\ include\\ zero.}\\) If the null hypothesis is rejected, then the two group means \\(\\mu_{i}\\text{\\ and\\ }\\mu_{j}\\) are different. Conduct one test for each possible unique pair of means from the ANOVA Example. We are going to use Fisher’s LSD to test the groups from the Grocery Store Ad example on Ch 13: Handout #4. Part 1. Using Method 2, perform a Fisher’s LSD procedure to test whether the population mean produce spending for the group that received Ad A is different than Ad B. Use an \\(\\alpha = 0.05\\) significance level. If you can confirm a difference between the means, which group spent more on average? Example, part 2: Using Method 2, perform a Fisher’s LSD procedure to test whether the population mean produce spending for the group that received Ad A is different than Ad C. Use an \\(\\alpha = 0.05\\) significance level. If you can confirm a difference between the means, which group spent more on average? Example, part 3: Using Method 2, perform a Fisher’s LSD procedure to test whether the population mean produce spending for the group that received Ad B is different than Ad C. Use an \\(\\alpha = 0.05\\) significance level. If you can confirm a difference between the means, which group spent more on average? "],
["chapter-14.html", "Chapter 14", " Chapter 14 Simple Linear Regression "],
["introduction-1.html", "Introduction", " Introduction Simple linear regression is a statistical procedure that models the relationship between two variables. The two variables are the dependent variable (DV) called y, and the independent variable (IV), called x. A statistical model is a simplified picture of the relationship between variables that emphasizes key features and regular patterns. In simple linear regression, we model the relationship between two variables as a straight line. Why do we build statistical models? It is to be able to explain what is going on with the dependent variable, and to make predictions about the future based on data in the past. For example, we can model the relationship between a person’s level of education (x) and salary (y) to predict how much money they will make – in fact we could use regression on this problem and predict how much more people will make for each additional year of education they have. We can model the durability of a painted coating (y) based on the thickness of the coating (x). How thick does the coating have to be to hit our durability target? Statistical modeling can help with that. Regression in particular is for modeling linear relationships. Other statistical models exist to model other kinds of relationships. The Regression Model is a straight line defined by the following equation: \\[y = \\ \\beta_{0} + \\beta_{1}x + \\ \\epsilon\\] where \\[{y = the\\ dependent\\ variable }{x = the\\ independent\\ variable }{\\beta_{0} = the\\ population\\ intercept }{\\beta_{1} = the\\ population\\ slope\\ on\\ x,the\\ coefficient\\ on\\ x\\ }{\\epsilon = random\\ error }\\] The regression model describes the relationship between x and y in the population – so \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are population parameters. Since they are population parameters, they are not directly measured. Rather, we use sample data to estimate these population values, and draw conclusions about them using statistical inference, like confidence intervals and hypothesis tests. In regression, a random sample is taken from a population, and the data in the sample is used to calculate a linear equation, called the Estimated Regression Equation (ERE): \\[\\widehat{y} = b_{0} + b_{1}x\\] \\[{\\text{where\\ }\\widehat{y} = the\\ expected\\ or\\ predicted\\ value\\ of\\ y\\ \\left( \\text{the\\ mean\\ of\\ y} \\right) }{b_{0} = the\\ sample\\ intercept;the\\ estimate\\ of\\ \\beta_{0} }{b_{1} = the\\ sample\\ slope;the\\ estimate\\ of\\ \\beta_{1}}\\] The Estimated Regression Equation gives us a way to calculate the expected value of y at a given value of x. The expected value of y at a given x is the long-run mean of y: the mean value of y at a given x if many samples were taken from the same population. To state this more formally, \\(\\widehat{y}\\) is a point estimate for E(y), the expected value of y at a given x. It is often called the predicted value of y. A Refresher on the Equation of a Line The equation of a line is: \\[y = b + mx\\] \\[{\\text{where\\ b} = the\\ intercept }{m = the\\ slope\\ of\\ the\\ line}\\] The slope of the line: \\(m = \\frac{\\text{change\\ in\\ y}}{\\text{change\\ in\\ x}}\\) When graphing a line, the vertical axis is the y axis, and the horizontal axis is the x axis. Hence, the slope is often described as \\(m = \\frac{\\text{rise}}{\\text{run}}\\) because it describes how much the line rises up the y axis as it runs along the x axis. Example 1: Refresher 1. Graph the following equation. What is the slope? Interpret the slope by stating in words how much y changes (and in what direction) for a one unit change in x. \\[y = 1 + 0.5x\\] Example 2: Refresher 2. Graph the following equation. What is the slope? Interpret the slope by stating in words how much y changes (and in what direction) for a one unit change in x. \\[y = 8 - 2x\\] Example 3: Refresher 3. Graph the following equation. What is the slope? Interpret the slope in this equation by stating in words how much y changes (and in what direction) for a one unit change in x. \\[y = 5\\] And now back to regression… Now, data do not typically fall exactly on a nice straight line when graphed. Real-world data looks more like a cloud of points. Linear regression can only be used in cases where there is a plausible straight-line relationship between x and y. If the relationship between x and y is not linear, then you have to use some other technique to model it. Scatterplots which graph each data point are an important tool to recognize the relationships in your data. Here is how data that would typically be modeled with regression might look. Can you visualize the line that could model this data? Would it have a positive or a negative slope? Figure 1. \\[CHART\\] Here is another example of data that could be modeled with linear regression. Can you visualize the line that could model this data? Would it have a positive or a negative slope? Figure 2. \\[CHART\\] What about this one? Figure 3. \\[CHART\\] Now let’s look at some examples of data you would NOT model with linear regression. Do you see the nonlinear pattern? Figure 4. \\[CHART\\] If you modeled the data in Figure 4 with linear regression, the regression procedure would give you a straight line with a positive slope. But that straight line would misrepresent the actual relationship, which would cause your predictions to be way off! You as the researcher are responsible for avoiding this mistake – no computer is going to tell you not to make it. (NOTE: there are ways to transform data like this so that it can be modeled – if you continue studying statistics, you will find out about those). For now, protect yourself against such errors by graphing scatterplots of your data. Figure 5. \\[CHART\\] This is a dangerous pattern, because regression would model this as a horizontal line, or something close to that. And a horizontal line (which means the slope is zero) indicates there is no relationship between x and y. But there clearly is a relationship here – it is just not a linear relationship. Let’s look at some examples: \\[y = \\ \\beta_{0} + \\beta_{1}x + \\ \\epsilon\\] The estimated regression equation (ERE) is the relationship between x and y in the sample: \\[\\widehat{y} = b_{0} + b_{1}x\\] Remember, \\(\\widehat{y}\\) is the average \\(y\\) for a given \\(x\\), and is called the expected or predicted value of \\(y\\). Example 4. Regression could be used to investigate the relationship between weekly sales and weekly advertising. To use regression to model this relationship, first designate the DV and the IV. \\[{y = Weekly\\ Sales\\ \\left( \\$ \\right) }{x = Weekly\\ Advertising\\ \\left( \\$ \\right)}\\] Then take a random sample of weeks and measure sales and advertising in the prior week for each. The graph of the data looks like this. Do you expect a positive or a negative slope on the regression line?: \\[CHART\\] The sample data is used to calculate the following values: \\[{Intercept:\\ b}_{0} = 267.30\\] \\[Slope:b_{1} = 4.51\\] \\[95\\%\\ confidence\\ interval\\ for\\ the\\ population\\ slope,\\ \\beta_{1} = \\lbrack 3.35,\\ 5.66\\rbrack\\] a) Write down the Estimated Regression Equation (ERE). b) If $235 is spent on advertising, what would be the predicted sales? Interpret this value. c) What is the interpretation of the slope, \\(b_{1}?\\) d) Interpret the 95% confidence interval for the population slope, \\(\\beta_{1}\\) Example 5. The owners of a restaurant think that as the temperature of a certain drink increases, customer satisfaction will decrease. A random sample of customers who order this drink is taken, and the temperature of the drink along with the customer satisfaction rating (0 to 100) is measured. To use regression to model this relationship, first we designate the DV and the IV. \\[{y = Customer\\ Satisfaction\\ Rating }{x = Temperature\\ (℉)}\\] Here is the scatterplot of the data collected:\\[CHART\\] The sample data is used to calculate the following values: \\[{{Intercept:\\ b}_{0} = 105.2 }{{Slope:\\ b}_{1} = \\ - 1.42}\\] \\[95\\%\\ confidence\\ interval\\ for\\ the\\ population\\ slope,\\ \\beta_{1} = \\lbrack - 1.74,\\ - 1.09\\rbrack\\] a) Write down the Estimated Regression Equation (ERE) b) If the drink is served at \\(22℉\\), what would be the predicted customer satisfaction score? Interpret this value. c) What is the interpretation of the slope, \\(b_{1}?\\) d) Interpret the 95% confidence interval for the population slope, \\(\\beta_{1}\\) "],
["hypothesis-testing-in-simple-linear-regression.html", "Hypothesis Testing in Simple Linear Regression", " Hypothesis Testing in Simple Linear Regression The Regression Model, \\(\\mathbf{y =}\\mathbf{\\beta}_{\\mathbf{0}}\\mathbf{+}\\mathbf{\\beta}_{\\mathbf{1}}\\mathbf{x + \\epsilon}\\), gives the relationship between \\(\\text{x\\ }\\)and \\(y\\) in the population. From the sample, regression calculates the Estimated Regression Equation, \\(\\widehat{\\mathbf{y}}\\mathbf{=}\\mathbf{b}_{\\mathbf{0}}\\mathbf{+}\\mathbf{b}_{\\mathbf{1}}\\mathbf{x}\\), which gives the relationship between \\(\\text{x\\ }\\)and \\(y\\) in the sample. VERY IMPORTANT: \\(\\mathbf{\\ }\\mathbf{b}_{\\mathbf{1}}\\mathbf{\\text{\\ is\\ NOT\\ the\\ same\\ as\\ }}\\mathbf{\\beta}_{\\mathbf{1}}\\mathbf{\\text{\\ and\\ }}\\mathbf{b}_{\\mathbf{0}}\\mathbf{\\text{\\ is\\ NOT\\ the\\ same\\ as\\ }}\\mathbf{\\beta}_{\\mathbf{0}}\\mathbf{!!!!!!}\\) You must use hypothesis tests to learn about the \\(\\beta^{&#39;}s\\) from the \\(b&#39;s\\) The following table summarizes the possible relationships between \\(\\text{x\\ }\\)and \\(y:\\) Slope Coefficient Relationship between \\(\\mathbf{\\text{x\\ }}\\)and \\(\\mathbf{y}\\) \\[\\text{If\\ }\\beta_{1} &gt; 0\\] Positive linear relationship in the population \\[\\text{If\\ }\\beta_{1} &lt; 0\\] Negative linear relationship in the population \\[\\text{If\\ }\\beta_{1} = 0\\] No relationship in the population In order to conclude that there is a relationship between \\(\\text{x\\ }\\)and \\(y\\ \\)in the population, we need to confirm that \\(\\beta_{1} \\neq 0\\). But we cannot observe the population parameter \\(\\beta_{1}\\) – we only have the sample statistic \\(b_{1}.\\) How can we draw conclusions about a population parameter from a sample?? We have to use good old hypothesis testing! If the hypothesis test confirms a relationship between x and y, we typically say that “the relationship between x and y is statistically significant,” or, “there is a statistically significant relationship between x and y.” Hypothesis Test for Significance of the Population Slope \\(\\mathbf{\\beta}_{\\mathbf{1}}\\) Formulating the Hypotheses: There is only one form of hypotheses, because in regression the question is: is the population slope different from zero? If the slope of a line is zero, there is no relationship between x and y. If the slope is either positive or negative, then there is a relationship between x and y. Therefore, the question calls for a two-tailed test. Hypotheses about the Slope Coefficient \\(\\mathbf{\\beta}_{\\mathbf{1}}\\) in Regression Two-Tailed Test \\[H_{0}:\\ \\beta_{1} = 0\\] \\[H_{A}:\\ \\beta_{1} \\neq 0\\] Answers the question: Whether the population slope is different from zero. That is to say, whether there is a relationship between \\(\\text{x\\ and\\ y}\\) The Test Statistic: The sampling distribution of \\(b_{1}\\) is the \\(t\\) distribution, and so we use a \\(t\\) test statistic. The test statistic is: \\[t_{\\text{test}} = \\frac{b_{1}}{s_{b_{1}}}\\] \\[{b_{1} = the\\ sample\\ slope\\ coefficient,\\ from\\ the\\ regression\\ output }{s_{b_{1}} = the\\ standard\\ error\\ on\\ the\\ slope\\ coefficient,\\ from\\ the\\ regression\\ output}\\] and the degrees of freedom are \\(df = n - p - 1\\) \\[{\\text{where\\ n} = the\\ number\\ of\\ observations }{p = the\\ number\\ of\\ independent\\ variables\\ \\left( x^{&#39;}s \\right)\\text{\\ in\\ the\\ ERE}}\\] Deciding whether or not to Reject \\(\\mathbf{H}_{\\mathbf{0}}\\): For this Two-Tailed Test: p-value approach: The two-tailed \\(p\\text{-}\\text{value}\\) is two times the one-tailed probability of \\(t_{\\text{test}}.\\) If the \\(2T \\ p\\text{-}value \\leq \\ \\alpha,\\) then reject \\(H_{0}\\) and accept \\(H_{A}.\\) If the \\(2T\\ p\\text{-}value &gt; \\ \\alpha,\\) then do not reject \\(H_{0}\\). \\(H_{A}\\) is unsupported. NOTE: the two-tailed \\(\\mathbf{ p}\\text{-}\\mathbf{\\text{value}}\\) is reported by Excel in the Regression Output!! Critical Value Approach: If \\(t_{\\text{test}} \\ leq {- t}_{\\alpha/2}\\text{\\ OR}\\) \\(t_{ \\text{test}} \\geq t_{\\alpha/2}\\), then reject \\(H_{0}\\)and accept \\(H_{A}\\). If \\({- t}_{\\alpha/2} &lt; t_{\\text{test}} &lt; t_{\\alpha/2}\\), then do not reject \\(H_{0}\\). \\(H_{A}\\) is unsupported. NOTES: \\(t_{\\text{test}}\\ \\)is a Test Statistic \\(\\pm t_{\\alpha/2}\\) are Critical Values The degrees of freedom are \\(df = n - p - 1\\) Interpreting the test: In the following interpretations, you should substitute in the actual meaning of \\(\\text{x\\ and\\ y}\\). When you: The Interpretation is: Reject \\(\\mathbf{H}_{\\mathbf{0}}\\) At the \\(\\alpha\\) significance level, we can conclude that the population slope is different from zero. Therefore, the relationship between \\(\\text{x\\ and\\ y}\\) is statistically significant. Further: If \\(b_{1} &gt; 0,\\ \\)there is a positive linear relationship between \\(\\text{x\\ and\\ y.}\\) If \\(b_{1} &lt; 0,\\ \\)there is a negative linear relationship between \\(\\text{x\\ and\\ y.}\\) Do not reject \\(\\mathbf{H}_{\\mathbf{0}}\\) At the \\(\\alpha\\) significance level, we cannot conclude that the population slope is different from zero. Therefore, the relationship between \\(\\text{x\\ and\\ y}\\) is not statistically significant. "],
["simple-linear-regression-what-it-all-means.html", "Simple Linear Regression: What it all means", " Simple Linear Regression: What it all means Simple linear regression is a statistical modeling procedure that models the relationship between one Dependent Variable (DV), \\(y,\\) and one Independent Variable (IV), \\(\\text{x.}\\) A model is a simplified representation that captures important characteristics but leaves out many details. The model of the relationship between \\(x\\) and \\(\\text{y\\ }\\)in linear regression is a straight line. The linear relationship between \\(x\\) and \\(y\\) in the whole population of interest is represented in the Regression Model: \\[y = \\ \\beta_{0} + \\beta_{1}x + \\ \\epsilon\\] where \\[{y = the\\ dependent\\ variable }{x = the\\ independent\\ variable }{\\beta_{0} = the\\ population\\ intercept,\\ called\\ beta\\ nought\\ or\\ beta\\ zero }{\\beta_{1} = the\\ slope\\ on\\ x,the\\ coefficient\\ on\\ x,\\ called\\ beta\\ one }{\\epsilon = random\\ error }\\] The betas are population parameters, and as such, we cannot observe them directly. Instead, we take a random sample from the population and use the data from the sample to estimate the betas. That is what the regression procedure does: it analyzes all the data in the sample and uses it to calculate the Estimated Regression Equation: \\[\\widehat{y} = b_{0} + b_{1}x\\] \\[{\\text{where\\ }\\widehat{y} = the\\ expected\\ or\\ predicted\\ value\\ of\\ y\\ (the\\ mean\\ of\\ y)\\ }{b_{0} = the\\ sample\\ intercept;the\\ estimate\\ of\\ \\beta_{0} }{b_{1} = the\\ sample\\ slope;the\\ estimate\\ of\\ \\beta_{1}}\\] Regression output contains many different quantities, all of which are calculated from the data: that is, from the individual values of \\(x\\) and \\(y\\) in the observations in the data set. The data set will look like this, although the columns do not have to be in this order: Observation (\\(\\mathbf{i}\\)) Dependent Variable (\\(\\mathbf{y }_{\\mathbf{i}}\\)) Independent Variable (\\(\\mathbf{x }_{\\mathbf{i}}\\)) 1 \\[y_{1}\\] \\[x_{1}\\] 2 \\[y_{2}\\] \\[x_{2}\\] … … … \\[n\\] \\[y_{n}\\] \\[x_{n}\\] The Observation column – which may be labeled by Case or Subject, or whatever is appropriate for the study – is a unique identifier for each observation in the dataset. Every observation in the dataset consists of measurements for each variable. For example, if this dataset was for a regression investigating income and years of education, each observation would be an individual person (\\(i\\)), and for each person, income (\\(y_{i}\\)) and years of education (\\(x_{i}\\)) would be measured and recorded. From the values of \\(y_{i}\\) and \\(x_{i}\\) for each observation \\(i\\), we can calculate all of the values in the regression output. Linear regression calculates the estimated regression equation by employing something called the Least Squares Criterion, which minimizes the squared vertical distance between each observed value of \\(y\\) in the sample (each \\(y_{i}\\)) and the predicted \\(y\\) from the estimated regression equation (each \\({\\widehat{y}}_{i}\\)) at every value of \\(x_{i}\\). This vertical distance between observed and predicted (\\(y_{i} - {\\widehat{y}}_{i}\\)) is a very important quantity in regression, called the Residual. Mathematically, the Least Squares Criterion is stated: \\[Least\\ Squares\\ Criterion = min\\sum_{}^{}\\left( y_{i} - {\\widehat{y}}_{i} \\right)^{2}\\] So, the Least Squares Criterion minimizes the sum of the squared residuals. Conceptually, the estimated regression equation that linear regression calculates is the line that is as close as possible to all the points in the data set at once: that is what it means to satisfy the Least Squares Criterion. For linear regression with more than one independent variable, matrix algebra or calculus is required to calculate the estimated regression equation. The equations simplify in the case of Simple Linear Regression, and so we could easily calculate the equation, and in fact all the regression output, by hand – if we only had the time. For now, let’s concentrate on learning what the numbers in the regression output mean. Interpreting Linear Regression Output NOTE: in many of the interpretations, I refer to the DV as \\(y\\) and the IVs as \\(x,\\ x_{1},\\ x_{2}\\ \\)etc. When actually interpreting a regression, these variables have meaning and the meaning should be substituted in for these placeholder variables. In the Regression Statistics table (in order of importance/informativeness): The Coefficient of Determination, \\(\\mathbf{R}^{\\mathbf{2}}\\mathbf{,\\ }\\)(called R Square in Excel) gives the proportion of the variability in the dependent variable that is explained by the independent variable or variables. \\(R^{2}\\) varies from 0 to 1, and when interpreted, it is converted to a percentage. In Simple Linear Regression: \\(R^{2}\\) measures the proportion of the variability in \\(\\text{y\\ }\\)that is explained by \\(x\\). Example: If \\(R^{2} = 0.7231,\\) then the interpretation is: 72.31% of the variability in \\(y\\) is explained by \\(\\text{x.}\\) In Multiple Regression: \\(R^{2}\\) measures the proportion of the variability in \\(\\text{y\\ }\\)that is explained by all of the independent variables, \\(x_{1},\\ x_{2},\\ \\ldots,\\ x_{p}\\). Example: If \\(R^{2} = 0.4979,\\) then the interpretation is: 49.79% of the variability in \\(y\\) is explained by \\(x_{1},\\ x_{2},\\ldots,\\ x_{p}.\\) The Standard Error of the Estimate, s, often called the Root Mean Square Error (RMSE), measures the accuracy of the predictions made by the Estimated Regression Equation. It is the average distance that the observed \\(y\\) values in the sample fall from the regression line. Interpretation: the average error we would make using the estimated regression equation to predict \\(y\\). In other words: if we used the estimated regression equation to predict \\(y\\), we would be off by s on average. Example: If \\(s = 4\\ units,\\) then the interpretation is: \\(4\\ units\\) is the average error we would make if we used the estimated regression equation to predict \\(\\text{y.}\\) Another way to state the same thing is: If we used the estimated regression equation to predict y, we would be off by 4 units on average. The Standard Error of the Estimate is in the same units as \\(y\\), which makes it very easy to understand and interpret. The lower the Standard Error of the Estimate, the more accurate the predictions, so the lower the better! Given two regression models that predict the same DV measured in the same units, the Standard Error of the Estimate can be used to choose between models: whichever one has the lower Standard Error of the Estimate is the better model, because the predictions made with it will be more accurate. Adjusted \\(\\mathbf{R}^{\\mathbf{2}}\\mathbf{\\ (notated\\ }\\mathbf{R}_{\\mathbf{A}}^{\\mathbf{2}}\\mathbf{)}\\) is used as a model comparison statistic in multiple regression. When comparing two models that predict the same DV in the same units, the one with the higher Adjusted \\(\\mathbf{R}^{\\mathbf{2}}\\) is the better model. NOTE: it is not appropriate to use regular old \\(R^{2}\\) to compare two models, because adding another IV to a regression model will increase \\(R^{2}\\), whether that IV explains anything or not. Adjusted \\(\\mathbf{R}^{\\mathbf{2}}\\), on the other hand, will only increase if the additional IV adds explanatory power, and will actually decrease if it does not. The Correlation Coefficient (called Multiple R in Excel) is denoted \\(R_{\\text{xy}}\\) in simple linear regression and \\(R_{ŷy}\\) in multiple regression. It gives a descriptive measure of the strength of the linear association between \\(y\\) and \\(x\\) (in simple linear regression) and between \\(y\\) and all the \\(x\\text{&#39;}s\\) in multiple regression. The closer to 1, the stronger the association. Values close to 0 indicate that x and y are not linearly related. No rules are set in stone about what constitutes a “strong” or “weak” relationship. A common rule of thumb: &lt; 0.25: weak linear association; between 0.25 and 0.75: moderate linear association; &gt; 0.75: strong linear association The ANOVA table: recall: the sample data is our best representation of the underlying population, so if our model is good at predicting the sample values, we can infer it will also be good at predicting population values The ANOVA table in the regression output compares two different methods of predicting the sample data in an effort to help us decide if our regression model is any good. Premise: there are two alternative models you could use to predict \\(y\\). One model includes information about \\(x\\) and one does not: First, you could use the Estimated Regression Equation to predict \\(y\\) (in other words, predict \\({\\widehat{y}}_{i}\\) \\(\\ \\)at each \\(x_{i}\\)) Second, you could use the next best option, which is to just use \\(\\overline{y}\\) to predict \\(y\\) (in other words, predict \\(\\overline{y}\\) (the mean of y) at each \\(x_{i}\\)) This is also called the ‘Intercept only model’ because the equation of \\(\\text{y\\ }\\)does not include any \\(x\\) variables, thus being only an intercept, and representing a horizontal line at \\(y = \\ \\overline{y}\\). In the ANOVA table, you calculate the total errors you would make if you predicted the actual observed data using each of these models, square those errors, and sum them. In this way, we can compare the two possible models and decide whether our regression model is worth using. The Sum of Squares due to Error, \\(\\mathbf{\\text{SSE}},\\ \\)sometimes called the Residual Sum of Squares, is the total amount of prediction error we would make using the estimated regression equation to predict the observed sample data. It is the sum of the squared residuals. A residual is the difference between the observed and predicted \\(\\mathbf{y}_{\\mathbf{i}}\\) at a given \\(\\mathbf{x}_{\\mathbf{i}}\\text{.\\ }\\)In other words: \\[\\mathbf{\\text{At\\ any\\ given\\ }}\\mathbf{x}_{\\mathbf{i}}\\mathbf{,\\ the\\ residual =}\\mathbf{y}_{\\mathbf{i}}\\mathbf{-}{\\widehat{\\mathbf{y}}}_{\\mathbf{i}}\\] The \\(SSE = \\sum_{}^{}\\left( y_{i} - {\\widehat{y}}_{i} \\right)^{2}\\), and has degrees of freedom \\(df_{2} = n - p - 1\\) Note: the sum of the squared residuals is the quantity that is minimized by the Least Squares Criterion! The Total Sum of Squares, \\(\\mathbf{SST,}\\ \\)is the total amount of prediction error we would make if we used \\(\\overline{y}\\) (the mean of \\(y_{i}\\)) to predict the observed sample data. The \\(SST = \\sum_{}^{}\\left( y_{i} - \\overline{y} \\right)^{2}\\), and has degrees of freedom \\(df = n - 1\\) The Sum of Squares due to Regression, \\(\\mathbf{SSR,}\\) is how much we will reduce prediction error by using the Estimated Regression Equation instead of the mean to predict the observed sample data. The \\(SSR = \\sum_{}^{}\\left( {\\widehat{y}}_{i} - \\overline{y} \\right)^{2}\\), and has degrees of freedom \\(df_{1} = p\\) The Mean Square Error, \\(\\mathbf{MSE,}\\) is the estimate of the variance of \\(\\epsilon\\). Remember \\(\\epsilon?\\) It is the random error term in the Regression Model: \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\) The \\(MSE = \\frac{\\text{SSE}}{n - p - 1}\\) The Coefficients table: The \\(\\mathbf{\\text{sample\\ slope\\ coefficients}}\\), \\(\\mathbf{b}_{\\mathbf{1}}\\mathbf{,\\ }\\mathbf{b}_{\\mathbf{2}}\\mathbf{,\\ldots,\\ }\\mathbf{b}_{\\mathbf{p}}\\) , quantify the magnitude and direction of the relationship between each IV and the DV. They are point estimates, respectively, of \\(\\beta_{1},\\ \\beta_{2},\\ldots,\\ \\beta_{p}.\\) In Simple Linear Regression, the interpretation of \\(\\mathbf{b}_{\\mathbf{1}}\\) is: If \\(\\mathbf{b}_{\\mathbf{1}}\\mathbf{&gt; 0}:\\) For every one unit increase in \\(x\\), \\(\\text{y\\ }\\)is predicted to increase by \\(b_{1}\\) on average. If \\(\\mathbf{b}_{\\mathbf{1}}\\mathbf{&lt; 0}:\\) For every one unit increase in \\(x\\), \\(\\text{y\\ }\\)is predicted to decrease by \\(b_{1}\\) on average. In Multiple Regression, each slope is interpreted. The interpretation for \\(b_{1}\\) is given here, and the interpretation of the other slopes follows the same pattern. The interpretation of \\(\\mathbf{b}_{\\mathbf{1}}\\) is: If \\(\\mathbf{b}_{\\mathbf{1}}\\mathbf{&gt; 0}:\\) For every one unit increase in \\(x\\), \\(\\text{y\\ }\\)is predicted to increase by \\(b_{1}\\) on average, holding all other independent variables constant. If \\(\\mathbf{b}_{\\mathbf{1}}\\mathbf{&lt; 0}:\\) For every one unit increase in \\(x\\), \\(\\text{y\\ }\\)is predicted to decrease by \\(b_{1}\\) on average, holding all other independent variables constant. The \\(\\mathbf{sample\\ intercept,\\ }\\mathbf{b}_{\\mathbf{0}},\\) is the value of \\(\\text{y\\ }\\)when \\(x = 0.\\) This is not always interpretable in the context of a given regression. Interpretability depends on whether the value \\(x = 0\\ \\)has substantive meaning. The \\(\\mathbf{\\text{confidence\\ intervals\\ for\\ the\\ slope\\ coefficients}}\\), \\(\\mathbf{\\beta}_{\\mathbf{1}}\\mathbf{,\\ }\\mathbf{\\beta}_{\\mathbf{2}}\\mathbf{,\\ \\ldots,\\ }\\mathbf{\\beta}_{\\mathbf{p}}\\), and the \\(\\mathbf{intercept,\\ }\\mathbf{\\beta}_{\\mathbf{0}}\\mathbf{,}\\) are based on the \\(\\alpha\\) significance level and are interpreted similarly to the other confidence intervals we have encountered: For a given slope coefficient, say \\(\\mathbf{\\beta}_{\\mathbf{1}}\\mathbf{,}\\) the interpretation is: We can be \\(\\_\\_\\_\\%\\ \\)confident that the true value of the population slope coefficient for \\(x_{1}\\) is between \\[lower bound\\] and \\[upper bound\\]. For the intercept, \\(\\mathbf{\\beta}_{\\mathbf{0}}\\), the interpretation is: We can be \\(\\_\\_\\_\\%\\ \\)confident that the true value of the population intercept is between \\[lower bound\\] and \\[upper bound\\]. "],
["simple-linear-regression-an-example.html", "Simple Linear Regression: An Example", " Simple Linear Regression: An Example We begin with a research question defining what we are investigating with regression: Does the time students spend on MindTap help explain (that is to say, predict) their grades? We will use an \\(\\alpha = 0.01\\) significance level for this regression analysis. This question defines our variables and the relationship that we are investigating. Here the variables are: Dependent variable: \\(y = Overall\\ MindTap\\ Score\\ (\\%\\ points)\\) Independent variable: \\(x = Time\\ spent\\ logged\\ on\\ (hrs)\\) The dataset collected is a random sample of \\(n = 24\\) students enrolled in BA 3400. For each student, time spent logged on to MindTap (in hours) and overall MindTap score (in points) was recorded. Here is the dataset: Student # Hours Score 1 20.85 88.80 2 10.65 85.40 3 25.72 99.60 4 7.75 68.10 5 18.28 78.40 6 11.62 75.10 7 17.30 78.50 8 15.03 84.30 9 9.60 77.20 10 10.93 90.40 11 14.02 82.20 12 15.25 91.10 13 17.72 98.50 14 9.57 71.90 15 14.60 86.00 16 16.45 85.60 17 6.77 63.00 18 12.07 85.90 19 13.00 83.20 20 22.00 97.30 21 4.92 81.80 22 14.67 86.10 23 22.00 87.00 24 24.90 90.50 After the data collection step, the next thing to do is to graph a scatterplot of Scores vs Time Spent. In the scatterplot, we are chiefly concerned with non-linear patterns – if we see those, then we cannot use linear regression to analyze this data. We may also be able to identify a trend in the data that will give us an idea about the relationship between these two variables. Here is the scatterplot: \\[CHART\\] Do you see any nonlinear patterns? Do you see a trend in the data? What sign do you expect on the slope of the regression line? At this point, we can run the regression in Excel (Demo). See the next page for the results! SUMMARY OUTPUT DV: MindTap Score ————————- ——————- —————— ———- ———– —————— ————- ————— ————— Regression Statistics Multiple R 0.6917 R Square 0.4785 Adjusted R Square 0.4548 Standard Error 6.6202 Observations 24 ANOVA df SS MS F Significance F Regression 1 884.5979 884.5979 20.1839 0.0002 Residual 22 964.1917 43.8269 Total 23 1848.79 Coefficients Standard Error t Stat P-value Lower 95% Upper 95% Lower 99.0% Upper 99.0% Intercept 67.4709 3.9186 17.2182 2.97E-14 59.3443 75.5976 56.4254 78.5165 Hours 1.1151 0.2482 4.4927 0.0002 0.6004 1.6299 0.4155 1.8148 Using the Regression Output Equations Roadmap, identify \\(b_{0}\\) and \\(b_{1}\\) on this output. Then write the Estimated Regression Equation (Remember, the ERE is \\(\\widehat{\\mathbf{y}}\\mathbf{=}\\mathbf{b}_{\\mathbf{0}}\\mathbf{+}\\mathbf{b}_{\\mathbf{1}}\\mathbf{x}\\)): Before we can use this equation for prediction or interpret the slope, we must confirm that there is a statistically significant relationship between MindTap Score (y) and Time Spent (x) in the population. (Remember, if the slope of a line is zero, then there is no relationship between x and y. If the slope of a line is a number other than zero, then there is a relationship between x and y.) Here is the Coefficients Table again: Coefficients Standard Error t Stat P-value Lower 95% Upper 95% Lower 99.0% Upper 99.0% Intercept 67.4709 3.9186 17.2182 2.97E-14 59.3443 75.5976 56.4254 78.5165 Hours 1.1151 0.2482 4.4927 0.0002 0.6004 1.6299 0.4155 1.8148 The sample slope \\(b_{1}\\) is 1.1151 which is definitely not zero… so that means there is a relationship between Hours and Score, right? Not necessarily! That sample slope just reflects what is going on in this sample of 24 students. We have to use this sample slope to prove that the population slope \\(\\mathbf{\\beta}_{\\mathbf{1}}\\) is not zero. If proven, then that would mean there is a statistically significant relationship between Hours and Score in the whole population of BA 3400 students, not just in these 24 students that were in the sample. That is what we need to prove before using the regression to explain or predict anything. Perform the hypothesis test detailed in Ch 14: Handout #2 to determine whether there is a statistically significant relationship between Hours and Score in the population: Now that we have confirmed that Hours and Score are related in the population, we can use our Estimated Regression Equation (ERE) to predict student scores, and we can interpret the slope \\(b_{1}.\\) First, let’s write down the ERE again: If 11 hours are spent on MindTap, what is the predicted score? Interpret this value. Interpret the slope \\(b_{1}.\\) Report and interpret the confidence interval for the population slope \\(\\beta_{1}.\\) Since we are doing this regression at the \\(\\alpha = 0.01\\) significance level, which is a 99% confidence level, we should report and interpret the 99% confidence interval: But there is more to regression than that… The existence of a significant relationship between y and x is a necessary step – after all, if you cannot confirm a relationship between the IV and the DV, then the regression is no good for anything – but it is not sufficient to stop there. We need a way to judge the quality of the model. How well does it fit the data? Does it make accurate predictions? How much of the variation in y does x explain? Such questions are answered in the Regression Statistics table. But to understand the Regression Statistics table, we need to understand how the regression line (the estimated model) is calculated. Linear regression calculates the estimated regression equation which minimizes the squared vertical distance between each observed value of \\(y\\) in the sample (each \\(y_{i}\\)) and the regression line, at every value of \\(x_{i}\\) in the dataset. This vertical distance between observed and predicted y is a very important quantity in regression, called the Residual and it is calculated by taking the difference between the observed and predicted values of y for a given observation: \\[\\text{At\\ a\\ given\\ value\\ of\\ }x_{i},\\ the\\mathbf{\\ residual =}\\mathbf{y}_{\\mathbf{i}}\\mathbf{-}{\\widehat{\\mathbf{y}}}_{\\mathbf{i}}\\] Mathematically speaking, the regression line satisfies the Least Squares Criterion, which is: \\[Least\\ Squares\\ Criterion = min\\sum_{}^{}\\left( y_{i} - {\\widehat{y}}_{i} \\right)^{2}\\] So, the regression procedure minimizes the sum of the squared residuals. Conceptually, the estimated regression equation that linear regression calculates is the line that is as close as possible to all the points in the data set at once: that is what it means to satisfy the Least Squares Criterion. The regression line that satisfies the Least Squares Criterion is called the best-fit line. Let’s calculate some residuals for the MindTap data, to gain insight into what regression is doing and thereby understand what the output tells us. Here is part of the MindTap dataset, along with the graph of the data with the regression line: Student # Hours \\[(x_{i})\\] Score \\[(y_{i})\\] 1 20.85 88.80 2 10.65 85.40 3 25.72 99.60 4 7.75 68.10 5 18.28 78.40 … … … \\[CHART\\] Find Student #4 on the graph. Calculate the residual for this student. Find Student #3 on the graph. Calculate the residual for this student. The regression procedure uses the residuals for all 24 students in its placement of the regression line. It takes all the residuals into account at once, squares them (why?), sums them, and places the line where that sum is the smallest it can be. This sum of the squared residuals is extremely important, then! And that is why it is reported in the ANOVA table in the regression output. It is called the Sum of Squares due to Error or the Residual Sum of Squares, and it is notated as the SSE. \\[\\mathbf{\\text{SSE}} = \\sum_{}^{}\\left( y_{i} - {\\widehat{y}}_{i} \\right)^{2}\\] \\[\\text{with\\ d}f_{2} = n - p - 1\\] Let’s take a look at the ANOVA table from the MindTap regression output. (Refer to the Regression Output Equations Roadmap throughout). Where is the SSE and its degrees of freedom? ANOVA df SS MS F Significance F Regression 1 884.5979 884.5979 20.1839 0.0002 Residual 22 964.1917 43.8269 Total 23 1848.79 The other values in the ANOVA table are also important in what we are building towards – that is, assessing the quality of this model. The Total Sum of Squares, \\(\\mathbf{SST,}\\ \\)is the total amount of prediction error we would make if we used \\(\\overline{y}\\) (the mean of \\(y_{i}\\)) to predict the observed sample data. Where is the SST and its df in the ANOVA table? The \\(SST = \\sum_{}^{}\\left( y_{i} - \\overline{y} \\right)^{2}\\), and has degrees of freedom \\(df = n - 1\\) The Sum of Squares due to Regression, \\(\\mathbf{SSR,}\\) is how much we will reduce prediction error by using the Estimated Regression Equation instead of the mean to predict the observed sample data. Where is the SSR and its df in the ANOVA table? The \\(SSR = \\sum_{}^{}\\left( {\\widehat{y}}_{i} - \\overline{y} \\right)^{2}\\), and has degrees of freedom \\(df_{1} = p\\) The Mean Square Error, \\(\\mathbf{MSE,}\\) is the estimate of the variance of \\(\\epsilon\\). Remember \\(\\epsilon?\\) It is the random error term in the Regression Model: \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\). Where is the MSE in the ANOVA table? The \\(MSE = \\frac{\\text{SSE}}{n - p - 1}\\) Putting this all together to explain the ANOVA table in regression: recall: the sample data is our best representation of the underlying population, so if our model is good at predicting the sample values, we can infer it will also be good at predicting population values The ANOVA table in the regression output compares two different methods of predicting the sample data in an effort to help us assess the quality of our regression model. We assess the regression by comparing its predictions to the next best alternative Premise: there are two different models you could use to predict \\(y\\). One model includes information about \\(x\\) and one does not: First, you could use the Estimated Regression Equation to predict \\(y\\) (in other words, predict \\({\\widehat{y}}_{i}\\) \\(\\ \\)at each \\(x_{i}\\)) Second, you could use the next best option, which is to just use the mean of \\(\\overline{y}\\) to predict \\(y\\) at each \\(x_{i}\\) This is also called the ‘Intercept only model’ because the equation of \\(\\text{y\\ }\\)does not include any \\(x\\) variables, thus being only an intercept, and representing a horizontal line at \\(y = \\ \\overline{y}\\). The ANOVA table reports the total errors made when predicting the actual observed data using each of these models. In this way, we can compare the two possible models and decide whether our regression model is worth using. So now that we have more understanding of what is in the ANOVA table, we should not be surprised to see all of these values showing up in the Regression Statistics table in the output, which contains the model fit stats we need. Here is the Regression Statistics table from the Regression Output Equations Roadmap: Regression Statistics Multiple R \\(\\text{\\ \\ \\ R}_{\\text{xy}}\\) or \\(R_{ŷy}\\ \\)= \\(\\sqrt{R^{2}}\\) →\\(\\text{~~ }R_{\\text{xy}}\\) &amp; \\(R_{ŷy}\\ \\): Correlation Coefficient R Square \\(\\text{ \\ \\ \\ R}^{2} =\\) $ $ →$ R^{2}$ : Coefficient of Determination Adjusted R Square \\[\\text{\\ \\ \\ R}_{A}^{2}\\] → \\(\\ R_{A}^{2}\\) \\(1 - \\lbrack\\ left( 1 - R^{2} \\right)\\left( \\frac{n\\ - 1}{ n\\ - p\\ - 1} \\right)\\rbrack\\) : Adjusted \\(R^{2}\\) Standard Error \\[\\ \\ \\ s\\ = \\sqr t{\\text{MSE}}\\] →\\(\\text {~~}\\text{s~}\\): Standard Error of the Estimate, or Root Mean Square Error (RMSE) *Observations** \\(\\text{\\ n}\\) → n = # observations Here is the Regression Statistics table from the MindTap regression output: Regression Statistics Multiple R 0.6917 R Square 0.4785 Adjusted R Square 0.4548 Standard Error 6.6202 Observations 24 blank on purpose Show the calculation of the Coefficient of Determination, \\(R^{2}.\\) Interpret this value. Show the calculation of the Standard Error of the Estimate, \\(\\text{s.}\\) Interpret this value. Show the calculation of the Correlation Coefficient, \\(R_{\\text{xy}}.\\) Interpret this value. "],
["chapter-15.html", "Chapter 15", " Chapter 15 Multiple Regression "],
["multiple-regression.html", "Multiple Regression", " Multiple Regression Multiple regression models the relationship between one dependent variable, \\(y\\), and two or more independent variables (IVs), which we will notate \\(x_{1},x_{2},\\ x_{3},\\ldots,\\ x_{p}.\\) (Note: here the subscripts on the \\(x\\)’s signify different independent variables, not individual observations of\\(\\text{\\ x}\\).) So, multiple regression is linear regression with more than one independent variable. Everything we have learned in Simple Linear Regression applies to Multiple Regression as well, because Simple Linear Regression is just the special case of multiple regression in which there is only one \\(x\\). The Regression Model describes the relationship between the DV and the IVs in the population, and it is given by the following equation: \\[y = \\ \\beta_{0} + \\beta_{1}x_{1} + \\ \\beta_{2}x_{2} + \\ldots + \\ \\beta_{p}x_{p} + \\ \\epsilon\\] where \\[{y = the\\ dependent\\ variable }{x_{1},x_{2},\\ldots,\\ x_{p} = the\\ independent\\ variables }{\\beta_{0} = the\\ population\\ intercept }{\\beta_{1},\\beta_{2},\\ldots\\ ,\\ \\beta_{p} = the\\ population\\ slopes,which\\ are\\ the\\ coefficients\\ on\\ the\\ x\\text{&#39;}\\text{s\\ } }{\\epsilon = random\\ error }{p = the\\ number\\ of\\ independent\\ variables}\\] The betas, (\\(\\beta_{0},\\beta_{1},\\ \\beta_{2},\\ \\ldots,\\ \\beta_{p}\\)), are population parameters. In other words, they are characteristics of the population as a whole, and so we cannot know their values without observing the entire population. When using regression, we take a random sample from the population of interest and we use that sample to estimate these parameters (the betas). That process results in the estimated regression equation (ERE): \\[\\widehat{y} = \\ b_{0} + \\ b_{1}x_{1} + \\ b_{2}x_{2} + \\ldots + \\ b_{p}x_{p}\\] where \\[{\\widehat{y} = the\\ expected\\ or\\ predicted\\ value\\ of\\ y\\ (the\\ mean\\ of\\ y) }{x_{1},x_{2},\\ldots,\\ x_{p} = the\\ independent\\ variables }{b_{0} = the\\ sample\\ intercept;the\\ estimate\\ of\\ \\beta_{0} }{b_{1},\\ b_{2},\\ldots,b_{p} = the\\ sample\\ slopes,which\\ are\\ the\\ coefficients\\ on\\ the\\ x^{&#39;}s;\\ }\\] \\[{\\text{the\\ }b^{&#39;}\\text{s\\ are\\ the\\ estimates\\ of\\ }\\beta_{1},\\beta_{2},\\ldots,\\ \\beta_{p} }{p = the\\ number\\ of\\ independent\\ variables}\\] Multiple regression is based on the idea that the relationship between \\(y\\) and each \\(x\\) is linear, so you still must examine the scatterplot of \\(y\\) against each \\(x\\) to check for non-linear patterns. If you see non-linear patterns, you cannot use linear regression to model the relationship without modifying the variables to make the relationship linear (techniques for this exist, but are beyond the scope of this class). The equations to calculate the coefficients in multiple regression require calculus or matrix algebra, so we will not be calculating these by hand. Instead, we will use Excel to produce output for multiple regression and interpret that output. See Chapter 14: Handout #3 – Interpreting Linear Regression Output which includes interpretations for Multiple Regression as well as Simple Linear Regression. "],
["hypothesis-testing-for-significance-in-multiple-regression.html", "Hypothesis Testing for Significance in Multiple Regression", " Hypothesis Testing for Significance in Multiple Regression The Regression Model, \\(\\mathbf{y =}\\mathbf{\\beta}_{\\mathbf{0}}\\mathbf{+}\\mathbf{\\beta}_{\\mathbf{1}}\\mathbf{x}_{\\mathbf{1}}\\mathbf{+ \\ }\\mathbf{\\beta}_{\\mathbf{2}}\\mathbf{x}_{\\mathbf{2}}\\mathbf{+ \\ldots +}\\mathbf{\\beta}_{\\mathbf{p}}\\mathbf{x}_{\\mathbf{p}}\\mathbf{+ \\epsilon}\\), gives the relationship between \\(\\text{y\\ }\\)and the \\(x&#39;s\\) in the population. From the sample, regression calculates the Estimated Regression Equation (ERE), \\(\\widehat{\\mathbf{y}}\\mathbf{=}\\mathbf{b}_{\\mathbf{0}}\\mathbf{+}\\mathbf{b}_{\\mathbf{1}}\\mathbf{x}_{\\mathbf{1}}\\mathbf{+}\\mathbf{b}_{\\mathbf{2}}\\mathbf{x}_{\\mathbf{2}}\\mathbf{+ \\ldots +}\\mathbf{b}_{\\mathbf{p}}\\mathbf{x}_{\\mathbf{p}}\\), which gives the relationship between \\(\\text{y\\ }\\)and the \\(x^{&#39;}s\\) in the sample. Hypothesis Tests About the Population Slopes Each population slope gives the relationship between its corresponding \\(x\\) and \\(y\\). The following table summarizes the possible relationships between \\(\\text{x\\ }\\)and \\(y:\\) Slope Coefficient Relationship between \\(\\mathbf{\\text{x\\ }}\\)and \\(\\mathbf{y}\\) \\[If\\ \\beta &gt; 0\\] Positive linear relationship \\[If\\ \\beta &lt; 0\\] Negative linear relationship \\[If\\ \\beta = 0\\] No relationship In order to conclude that there is a relationship between a given \\(\\text{x\\ }\\)and \\(y,\\) we need to confirm that the population slope coefficient on that \\(x\\) does not equal zero. In Chapter 14: Handout #2, we learned that we can use a two-tailed \\(t\\) test to confirm whether the population slope is different from zero. In multiple regression, we use that same procedure on each slope coefficient in turn. So, to confirm whether there is a relationship between \\(x_{1}\\) and \\(y\\), the hypotheses would be: \\[H_{0}:\\ \\beta_{1} = 0\\] \\[H_{A}:\\ \\beta_{1} \\neq 0\\] If \\(H_{0}\\) is rejected and \\(H_{A}\\) is accepted, then there is a relationship between \\(x_{1}\\) and \\(\\text{y.}\\) In other words, the relationship between \\(x_{1}\\) and \\(y\\) is statistically significant. Then, to confirm whether there is a relationship between \\(x_{2}\\) and \\(y\\), the hypotheses would be: \\[H_{0}:\\ \\beta_{2} = 0\\] \\[H_{A}:\\ \\beta_{2} \\neq 0\\] If \\(H_{0}\\) is rejected and \\(H_{A}\\) is accepted, then there is a relationship between \\(x_{2}\\) and \\(\\text{y.}\\) In other words, the relationship between \\(x_{2}\\) and \\(y\\) is statistically significant. . . . Finally, to confirm whether there is a relationship between \\(x_{p}\\) and \\(y\\), the hypotheses would be: \\[H_{0}:\\ \\beta_{p} = 0\\] \\[H_{A}:\\ \\beta_{p} \\neq 0\\] If \\(H_{0}\\) is rejected and \\(H_{A}\\) is accepted, then there is a relationship between \\(x_{p}\\) and \\(\\text{y.}\\) In other words, the relationship between \\(x_{p}\\) and \\(y\\) is statistically significant. Each slope coefficient must be tested separately. Luckily for us, Excel calculates the \\(t_{\\text{test}}\\) test statistic and two-tailed p-value for each slope coefficient and reports them in the regression output. We just need to compare the p-value to \\(\\alpha\\) in each case. If the p-value \\(\\leq \\ \\alpha\\), then we reject \\(H_{0}\\) and conclude there is a statistically significant relationship between the particular \\(x\\) and \\(y\\). Remember: when the null hypothesis is rejected in these hypothesis tests, the conclusion is that the relationship between the given \\(x\\) and \\(y\\) is statistically significant. Hypothesis Test for the Overall Significance of the Regression Model The \\(F\\) test reported in the ANOVA table in the regression output tests the overall significance of the regression model. It tests whether the model considered as a whole is significant. Whereas the \\(t\\) tests discussed above test for the significance of the relationship between each individual \\(x\\) and \\(y\\) separately, the \\(F\\) test shows whether \\(y\\) is related to the \\(\\text{set\\ of\\ all\\ }x^{&#39;}s\\), considered together. Technically speaking, the \\(F\\) test compares two different models for predicting \\(y:\\) the regression model, and the model in which the mean is used to predict \\(y\\) (called the intercept-only model). If you can reject the null hypothesis in the \\(F\\) test, then you know that using the regression to predict \\(y\\) at each \\(x\\) is an improvement over using the mean of \\(y\\) to predict \\(y\\) at each \\(x\\), and that the improvement is statistically significant. Formulating the Hypotheses: There is only one form of hypotheses, but the number of \\(\\beta&#39;s\\) in it depends on how many IVs you have in your regression model. Hypotheses about the Overall Significance of the Regression Model \\[H_{0}:\\ \\beta_{1} = \\beta_{2} = \\ldots = \\beta_{p} = 0\\] \\[H_{A}:\\ One\\ or\\ more\\ of\\ the\\ \\beta^{&#39;}\\text{s\\ is\\ not\\ zero}\\] Answers the question: Whether the population slope coefficients are jointly different from zero. That is to say, whether the regression model considered as a whole is statistically significant. The Test Statistic: The test statistic is: \\[F_{\\text{test}} = \\frac{\\text{MSR}}{\\text{MSE}}\\] \\[\\text{MSR} = the\\ Mean\\ Square\\ due\\ to\\ Regression\\] with (numerator) degrees of freedom \\(df_{1} = p\\) \\[\\text{MSE} = the\\ Mean\\ Square\\ Error\\] with (denominator) degrees of freedom \\(df_{2} = n - p - 1\\) \\[{\\text{where\\ n} = the\\ number\\ of\\ observations }{p = the\\ number\\ of\\ independent\\ variables}\\] Deciding whether or not to Reject \\(\\mathbf{H}_{\\mathbf{0}}\\): ANOVAs are always one-tailed, upper tail tests. To calculate the p-value by Excel function, you would use =F.DIST.RT(\\(F_{\\text{test}},\\ numerator\\ df_{1},\\ denominator\\ df_{2})\\) Always a One-Tailed, Upper Tail Test: p-value approach: The upper-tailed \\(p\\text{-}\\text{value}\\) is the upper tail probability of \\(F_{\\text{test}}.\\) If the \\(p\\text{-}value \\leq \\ \\alpha,\\) then reject \\(H_{0}\\) and accept \\(H_{A}.\\) If the \\(p\\text{-}value &gt; \\ \\alpha,\\) then do not reject \\(H_{0}\\text{.\\ }H_{A}\\)is unsupported. NOTE: this \\(\\mathbf{ p}\\text{-}\\mathbf{\\text{value}}\\) is reported by Excel in the Regression Output as Significance F in the ANOVA table!! Critical Value Approach: If \\(F _{\\text{test}} \\geq F_{\\alpha}\\), then reject \\(H_{0}\\)and accept \\(H_{A}\\). If \\(F_{\\text{test}} &lt; F_{\\alpha}\\), then do not reject \\(H_{0}\\text{.\\ }H_{A}\\)is unsupported. NOTES: \\(F_{\\text{test}}\\ \\)is the Test Statistic \\(F_{\\alpha}\\) is an upper tail Critical Value The numerator degrees of freedom are \\(df = p\\), and the denominator degrees of freedom are \\(df = n - p - 1\\) Interpreting the test: When you: The Interpretation is: Reject \\(\\mathbf{H}_{\\mathbf{0}}\\) At the \\(\\alpha\\) significance level, we can conclude that overall regression model is statistically significant. A significant relationship is present between \\(y\\) and \\(x_{1},x_ {2},\\ldots,\\ x_{p},\\ \\)considered together. Our regression model will do a better job predicting \\(\\text{y\\ }\\)than using the mean to predict \\(\\text{y.}\\) Do not reject \\(\\mathbf{H}_{\\mathbf{0}}\\) At the \\(\\alpha\\) significance level, we cannot conclude that the overall regression model is statistically significant. "],
["how-to-handle-insignificant-xs.html", "How to handle insignificant x’s", " How to handle insignificant x’s In multiple regression, the population slopes are estimated using all of the x’s at once. The procedure takes into account each x’s relationship with y, while simultaneously accounting for each x’s relationship to all the other x’s. Here’s what I mean, conceptually speaking: The upshot of all of this is that a couple of our interpretations have to change to acknowledge the fact that the other x’s are being accounted for. Other than that, things stay pretty much the same. Example: Showtime Theaters An analyst at Showtime Theaters has been asked to analyze the relationship between weekly on-line advertising ($1000s), weekly television advertising ($1000s), and weekly gross revenue ($1000s) What is the Dependent Variable? (in other words, what is being explained?) What are the Independent Variables? (in other words, what variables might explain changes in the dependent variable? Note: all of the variables are expressed in thousands of dollars. Regression calculates the estimated regression equation in the same units as the variables. This means that you have to consider the units when interpreting the slopes, when plugging x values into the ERE to make predictions, and when interpreting predicted values. The equation will expect x values in the same units as the data, and will produce y values in the same units as the data. 15 weeks are randomly selected. Weekly on-line advertising ($1000s), weekly television advertising ($1000s), and weekly gross revenue ($1000s) are measured. This analysis will use an \\(\\mathbf{\\alpha = 0.01}\\) significance level Here is the dataset: Week \\(i\\) Weekly Revenue \\[(y_{i})\\] TV Advertising \\[(x_{1_{i}})\\] On-line Advertising \\[(x_{2_{i}})\\] 1 97.92 5 1.5 2 95.94 4 1.5 3 93.8 2.5 2.5 4 95.31 3 3.3 5 94.62 3.5 2 6 94.63 2.5 4.2 7 94.64 3.2 2 8 93.47 2.2 4 9 97.73 4.6 3 10 96.3 4.25 2.6 11 95.6 3.9 1.7 12 94.46 3.3 1.8 13 93.01 2.7 2.25 14 93.07 2.75 1.2 15 95.25 3.4 3.1 Now we need to scatterplot y versus each x to look for non-linear patterns. \\[CHART\\] \\[CHART\\] SUMMARY OUTPUT Regression Statistics Multiple R 0.9745 R Square 0.9497 Adjusted R Square 0.9413 NOTE: all numbers have been rounded to Standard Error 0.3618 four decimal places. Observations 15 ANOVA df SS MS F Significance F Regression 2 29.6320 14.8160 113.1856 1.63E-08 Residual 12 1.5704 0.1309 Total 14 31.2024 Coefficients Standard Error t Stat P-value Lower 99% Upper 99% Intercept 87.0304 0.6208 140.1907 1.16E-20 85.1346 88.9263 TV advertising 1.9365 0.1292 14.9884 3.92E-09 1.5420 2.3311 On-line advertising 0.5980 0.1163 5.1419 0.000244 0.2426 0.9534 ————————- —————- —————— —————————————- ———– —————— ————- What is the Regression Model we are estimating here? What is the estimated regression equation (ERE)? Do the hypothesis test to determine whether there is a statistically significant relationship between TV Advertising and Revenue. Do the hypothesis test to determine whether there is a statistically significant relationship between On-line Advertising and Revenue. Do the hypothesis test to determine whether the overall regression model is statistically significant (See Ch 15: Handout #2) Now that we have confirmed this regression reflects real relationships in the population, we can go ahead and start using the results. Remember what the ERE was? \\[\\widehat{y} = 87.0304 + 1.9365x_{1} + 0.5980x_{2}\\] \\[NOTE:\\ \\widehat{y} = predicted\\ Gross\\ Weekly\\ Revenue\\ \\left( in\\ \\$ 1000s \\right)\\] \\[x_{1} = TV\\ Advertising\\ \\left( in\\ \\$ 1000s \\right)\\] \\[x_{2} = On\\text{-}\\text{line~advertising~}\\left( in\\ \\$ 1000s \\right)\\] What is the predicted Gross Weekly Revenue if $3750 is spent on TV Advertising and $2300 is spent on On-line Advertising? Note: the problem gives the spending in $s, but the Estimated Regression Equation is calculated using data in $1000s. So you must convert the $s to $1000s: \\(x_{1} = \\frac{\\$ 3750}{\\$ 1000} = 3.75\\ and\\ x_{2} = \\frac{\\$ 2300}{\\$ 1000} = 2.3\\) plug the x-values into the ERE: \\(\\widehat{y} = 87.0304 + 1.9365\\left( 3.75 \\right) + 0.5980\\left( 2.3 \\right) = 95.67\\) Interpretation: For all weeks in which TV Advertising is $3750 and On-line Advertising is $2300, Gross Weekly Revenue is predicted to be $95,670 on average. What is the interpretation of \\(\\mathbf{b}_{\\mathbf{1}}\\mathbf{,}\\) the slope coefficient on TV advertising? NOTE: for all interpretations, we should convert the $1000s to $s. If the units were already in $s, we would leave them alone, so here we multiply 1.9365 x $1000 = $1936.50. Interpretation: For every $1000 increase in TV Advertising, Gross Weekly Revenue is expected to increase by $1936.50 on average, holding On-line Advertising constant. What is the interpretation of \\(\\mathbf{b}_{\\mathbf{2}}\\mathbf{,}\\) the slope coefficient on On-line Advertising? (multiply 0.5980 x $1000 to convert to dollars) Interpretation: For every $1000 increase in On-line Advertising, Gross Weekly Revenue is expected to increase by $598 on average, holding TV Advertising constant. Uh-oh Interpret the Coefficient of Determination, \\(\\mathbf{R}^{\\mathbf{2}}\\) First, multiply \\(R^{2} = 0\\ .9497\\ \\times 100 = 94.97\\%\\) Interpretation: 94.97% of the variability in Gross Weekly Revenue is explained by TV Advertising and On-line Advertising Interpret the Standard Error of the Estimate, \\(\\mathbf{s}\\) \\(s = 0.3618\\) Again, it helps communicate this interpretation if you convert \\(s\\), which is in $1000s, to $s: \\(0.3618 \\times \\$ 1000 = \\$ 361.80\\) Interpretation: If we used the Estimated Regression Equation to predict Gross Weekly Revenue, our average error would be $361.80. Not bad! There ends our discussion of the Showtime Theaters regression. Moving on to other important issues… What about insignificant Independent Variables? So far, we have only dealt with models where all the IVs are significant. This is NOT always the case. If you do not reject the null hypothesis in a t-test for a slope coefficient, then you CANNOT conclude that the population slope on that x is different from zero. Therefore, you CANNOT conclude that the IV for that slope is related to the DV This may be a substantively interesting result. Was it an IV that you were certain would help explain the DV? You may need to investigate why it does not in this case. However, if you are using the model for prediction, then you should drop the insignificant IV from the analysis and re-estimate the regression model including only the significant IVs. Let’s look at an example: Cruise Ship Ratings Condé Nast Traveler magazine conducts an annual Reader’s Choice Survey in which they ask readers to rate small cruise ships on several criteria: Itineraries/Schedule, Shore Excursions, and Food/Dining. The readers also give an overall quality rating to each ship. The variable values represent the percentage of readers who rated the cruise ship excellent or very good on each criterion. Research question: which (or what combination) of the three criteria explain the overall rating? What is the DV? What are the IVs? Use an α = 0.05 significance level Here is the dataset: ShipID Ship Overall Score Itin eraries/ Schedule Shore Ex cursions Foo d/Dining 1 Seabourn Odyssey 94.4 94.6 90.9 97.8 2 Seabourn Pride 93 96.7 84.2 96.7 3 National Ge ographic Endeavor 92.9 100 100 88.5 4 Seabourn Sojourn 91.3 88.6 94.8 97.1 5 Paul Gauguin 90.5 95.1 87.9 91.2 6 Seabourn Legend 90.3 92.5 82.1 98.8 7 Seabourn Spirit 90.2 96 86.3 92 8 Silver Explorer 89.9 92.6 92.6 88.9 9 Silver Spirit 89.4 94.7 85.9 90.8 10 Seven Seas N avigator 89.2 90.6 83.3 90.5 11 Silver W hisperer 89.2 90.9 82 88.6 12 National Ge ographic Explorer 89.1 93.1 93.1 89.7 13 Silver Cloud 88.7 92.6 78.3 91.3 14 C elebrity X pedition 87.2 93.1 91.7 73.6 15 Silver Shadow 87.2 91 75 89.7 16 Silver Wind 86.6 94.4 78.1 91.6 17 SeaDream II 86.2 95.5 77.4 90.9 18 Wind Star 86.1 94.9 76.5 91.5 19 Wind Surf 86.1 92.1 72.3 89.3 20 Wind Spirit 85.2 93.5 77.4 91.9 \\[CHART\\] \\[CHART\\] \\[CHART\\] Here is the regression output: SUMMARY OUTPUT Regression Statistics Multiple R 0.865922 R Square 0.749821 Adjusted R Square 0.702912 Standard Error 1.387747 Observations 20 ANOVA df SS MS F Significance F Regression 3 92.35202 30.78401 15.9847 4.52E-05 Residual 16 30.81348 1.925843 Total 19 123.1655 Coefficients Standard Error t Stat P-value Lower 95% Upper 95% Intercept 35.61838 13.23083 2.692075 0.01603 7.570276 63.66648 Itineraries/Schedule 0.110454 0.129662 0.851859 0.406863 -0.16442 0.385325 Shore Excursions 0.244537 0.043357 5.64005 3.69E-05 0.152624 0.336451 Food/Dining 0.247357 0.062117 3.982137 0.001072 0.115675 0.379038 Is this model statistically significant overall? (remember, we are using α = 0.05) The F test statistic has p-value = 0.0000452 0.0000452 ≤ 0.05, so we would reject \\(H_{0}\\) in the F test. YES, the overall model is statistically significant. Going from the bottom of the Coefficients Table up, is each IV statistically significant? Food/Dining: p-value = 0.001072 0.001072 ≤ 0.05, so we would reject \\(H_{0}\\) in the t-test for \\(\\beta_{3}\\). YES, Food/Dining has a statistically significant relationship with Overall Score. Shore Excursions: p-value = 0.0000369 0.0000369 ≤ 0.05, so we would reject \\(H_{0}\\) in the t-test for \\(\\beta_{2}\\). YES, Shore Excursions has a statistically significant relationship with Overall Score. What about Itineraries/Schedules? p-value = 0.406863 Uh-oh! 0.406863 &gt; 0.05, so we would not reject \\(H_{0}\\) in the t-test for \\(\\beta_{1}\\). NO, Itineraries/Schedule does NOT have statistically significant relationship with Overall Score. We assume this model is being used for prediction purposes, so we should drop the Itineraries/Schedule variable by going into Excel and re-running the regression with only Food/Dining and Shore Excursions included in the x variable range. Here are the results of that: SUMMARY OUTPUT ————————- —————- —————— ———- ———– —————— ————- Regression Statistics Multiple R 0.859345 R Square 0.738474 Adjusted R Square 0.707706 Standard Error 1.376504 Observations 20 ANOVA df SS MS F Significance F Regression 2 90.95451 45.47725 24.00154 1.12E-05 Residual 17 32.21099 1.894764 Total 19 123.1655 Coefficients Standard Error t Stat P-value Lower 95% Upper 95% Intercept 45.17796 6.951848 6.498698 5.46E-06 30.51084 59.84508 Shore Excursions 0.252892 0.041891 6.036882 1.33E-05 0.16451 0.341275 Food/Dining 0.248189 0.061606 4.028667 0.000871 0.118212 0.378166 Now all the variables are statistically significant, and see how the slopes changed? Even though Itineraries/Schedule was not statistically significant, it was still messing up the estimates for the other slopes. "],
["model-comparisons.html", "Model Comparisons", " Model Comparisons Frequently when using regression, we end up with more than one model for a particular dependent variable. We need some criteria to choose between models in that situation. \\(\\text{Adjusted~R}^{2}\\ \\)for model comparisons: When comparing two models that explain the same DV, it is tempting to choose the one with the higher Coefficient of Determination (\\(R^{2}\\)), but this would NOT be the right thing to do. \\(R^{2}\\) increases if you add additional IVs, even if the new variables do not add any substantive explanatory power to the model. \\(\\mathbf{\\text{Adjusted~}}\\mathbf{R}^{\\mathbf{2}}\\), \\(\\mathbf{R}_{\\mathbf{A}}^{\\mathbf{2}}\\), is modified to account for the number of IVs in the model. It only increases if the added variables improve the model more than would be expected to happen by chance. Therefore, \\(\\text{Adjusted~}R^{2}\\) is a useful statistic to compare two models. The one with the HIGHER \\(\\mathbf{\\text{Adjusted~}}\\mathbf{R}^{\\mathbf{2}}\\) is BETTER. (NOTE: \\(\\text{Adjusted~}R^{2}\\) is NOT interpretable like \\(R^{2}\\) is.) \\(\\text{Adjusted~}R^{2} = \\ R_{A}^{2} = 1 - \\left\\lbrack \\left( 1 - R^{2} \\right)\\left( \\frac{n\\ - 1}{n\\ - p\\ - 1} \\right) \\right\\rbrack\\) \\[where\\ n = \\#\\ of\\ observations\\ and\\ p = \\#\\ of\\ IVs\\] Standard Error of the Estimate, \\(s\\), for model comparisons: Reminder: the Standard Error of the Estimate, \\(s\\), is the average error we would make when using the estimated regression equation to predict the DV. Two models with the same DV measured in the same units can be compared using s. The one with the LOWER s gives more accurate predictions, and therefore is the BETTER model. Illustration of this: Here is a summary table of some statistics from the two cruise ship regressions (one with all three IVs, one with only the two significant IVs): Model 1: Three IVs Model 2: Two IVs Coefficient of Determination \\({(R}^{2})\\) 0.7498 0.7385 Adjusted \\(R^{2}\\ (R_{A}^{2})\\) 0.7029 0.7077 Standard Error of the Estimate (s) 1.3877 1.3765 \\(\\text{Adjusted~R}^{2}\\) is useful for model comparisons. It is not affected by additional IVs like \\(R^{2}\\) is. HIGHER values of \\(\\mathbf{\\text{Adjusted~}}\\mathbf{R}^{\\mathbf{2}}\\) are BETTER, so Model 2 is better than Model 1 The Standard Error of the Estimate (s) gives the average error made when using the regression equation to predict the DV. The lower the s, the more accurate the predictions are. When comparing two models, LOWER values of the Standard Error of the Estimate are BETTER. So, Model 2 is better than Model 1 "]
]
