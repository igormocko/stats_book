[Notation & Definitions]{.underline}

The **standard deviation** and the **variance** are measures of
variability. They measure how much the values of a variable in a given
sample or population vary above and below the mean. A population or
sample with values that are closer to the mean would have a lower
variance and a lower standard deviation than a population or sample with
values more spread out around the mean.

+----------------+----------------+----------------+----------------+
| Population     | Sample         |                |                |
| Parameters     | Statistics     |                |                |
+================+================+================+================+
| $$\ma          | The standard   | $$\mathbf{s}$$ | The standard   |
| thbf{\sigma}$$ | deviation of a |                | deviation of a |
|                | population     |                | sample         |
|                |                |                |                |
|                | The lowercase  |                |                |
|                | Greek letter   |                |                |
|                | "sigma"        |                |                |
+----------------+----------------+----------------+----------------+
| $$\m           | The variance   | $$\mathbf{s}^  | The variance   |
| athbf{\sigma}^ | of a           | {\mathbf{2}}$$ | of a sample    |
| {\mathbf{2}}$$ | population     |                |                |
|                |                |                |                |
|                | "Sigma         |                |                |
|                | squared"       |                |                |
+----------------+----------------+----------------+----------------+

The **standard deviation** is the **square root** of the **variance**:
$\sigma = \sqrt{\sigma^{2}}\ and\ s = \sqrt{s^{2}}.$

Likewise, the **variance** is the **standard deviation squared**,
exactly how it appears in the notation.

+----------------------------------------------------------------------+
| *Example.* If the standard deviation of a sample is 12, what is the  |
| variance of the sample?                                              |
|                                                                      |
| $$\ \ \ \ We\ are\ given\ that\ s = 12.$$                            |
|                                                                      |
| $$\text{\ \ \ \ The\ sample\ variance\ is\ }s^                       |
| {2},\ so\ we\ can\ calculate\ the\ variance:\ s^{2} = 12^{2} = 144$$ |
|                                                                      |
| Thus, the variance of the sample is 144 and is denoted with $s^{2}.$ |
+----------------------------------------------------------------------+

NOTE: problem descriptions could refer to either a variance OR a
standard deviation. You must treat them accordingly in the equations for
this chapter. If you notate the values properly, then you will know when
a value needs to be squared and when it does not.

Each of these letters can be subscripted to refer to specific
populations and samples, like this:

  Population Parameters with subscripts           Sample Statistics with subscripts                                                   
  ----------------------------------------------- ---------------------------------------- ------------------------------------------ ------------------------------------------------------------------------
  $$\mathbf{\sigma}_{\mathbf{1}}$$                The standard deviation of population 1   $$\mathbf{s}_{\mathbf{1}}$$                The standard deviation of sample 1 (i.e. the sample from population 1)
  $$\mathbf{\sigma}_{\mathbf{2}}$$                The standard deviation of population 2   $$\mathbf{s}_{\mathbf{2}}$$                The standard deviation of sample 2 (i.e. the sample from population 2)
  $$\mathbf{\sigma}_{\mathbf{1}}^{\mathbf{2}}$$   The variance of population 1             $$\mathbf{s}_{\mathbf{1}}^{\mathbf{2}}$$   The variance of sample 1 (i.e. the sample from population 1)
  $$\mathbf{\sigma}_{\mathbf{2}}^{\mathbf{2}}$$   The variance of population 2             $$\mathbf{s}_{\mathbf{2}}^{\mathbf{2}}$$   The variance of sample 2 (i.e. the sample from population 2)

This chapter will introduce two new test statistics, with corresponding
sampling distributions and a host of critical values. The notation we
will use is:

+----------------------------------+----------------------------------+
| Chi-Square                       |                                  |
| $\mathbf{(}\math                 |                                  |
| bf{\chi}^{\mathbf{2}}\mathbf{)}$ |                                  |
+==================================+==================================+
| $$\mathbf{\chi}^{\mathbf{2}}$$   | This is the Greek letter "chi"   |
|                                  | squared.                         |
|                                  |                                  |
|                                  | Pronounced "kai square\"         |
+----------------------------------+----------------------------------+
| $$\mathbf{\chi}_{\mat            | A chi-square test statistic      |
| hbf{\text{test}}}^{\mathbf{2}}$$ |                                  |
+----------------------------------+----------------------------------+
| $$\mathbf{\chi}_{\m              | A critical value of chi-square   |
| athbf{\alpha,LT}}^{\mathbf{2}}$$ | at $\alpha$ in the lower tail.   |
|                                  |                                  |
|                                  | The critical value in a          |
|                                  | **lower-tail** test with a       |
|                                  | $\chi^{2}$ **test statistic.**   |
+----------------------------------+----------------------------------+
| $$\mathbf{\chi}_{\m              | A critical value of chi-square   |
| athbf{\alpha,UT}}^{\mathbf{2}}$$ | at $\alpha$ in the upper tail.   |
|                                  |                                  |
|                                  | The critical value in an         |
|                                  | **upper-tail** test with a       |
|                                  | $\chi^{2}$ **test statistic.**   |
+----------------------------------+----------------------------------+
| $$\mathbf{\chi}_{\mat            | A critical value of chi-square   |
| hbf{\alpha/2,LT}}^{\mathbf{2}}$$ | at $\alpha/2$ in the lower tail. |
|                                  |                                  |
|                                  | The **lower tail critical        |
|                                  | value** in a **two-tailed** test |
|                                  | with a $\chi^{2}$ **test         |
|                                  | statistic.**                     |
+----------------------------------+----------------------------------+
| $$\mathbf{\chi}_{\mat            | A critical value of chi-square   |
| hbf{\alpha/2,UT}}^{\mathbf{2}}$$ | at $\alpha/2$ in the upper tail. |
|                                  |                                  |
|                                  | The **upper tail critical        |
|                                  | value** in a **two-tailed** test |
|                                  | with a $\chi^{2}$ **test         |
|                                  | statistic.**                     |
+----------------------------------+----------------------------------+

+----------------------------------+----------------------------------+
| $$\mathbf{F}$$                   |                                  |
+==================================+==================================+
| $$\ma                            | An $F$ test statistic            |
| thbf{F}_{\mathbf{\text{test}}}$$ |                                  |
+----------------------------------+----------------------------------+
| $$\mathbf{F}_{\mathbf{\alpha}}$$ | An upper-tail critical value of  |
|                                  | $F$ at $\text{Î±.}$               |
|                                  |                                  |
|                                  | The critical value in an         |
|                                  | **upper-tail** test with an      |
|                                  | $\text{F\ }$**test statistic.**  |
+----------------------------------+----------------------------------+
| $$                               | A two-tailed critical value of   |
| \mathbf{F}_{\mathbf{\alpha/2}}$$ | $F$ at $\alpha/2.$               |
|                                  |                                  |
|                                  | The critical value in a          |
|                                  | **two-tailed** test with an $F$  |
|                                  | **test statistic.**              |
+----------------------------------+----------------------------------+

**[Part One: Hypothesis Tests about a Single Population
Variance,]{.underline}** $\mathbf{\sigma}^{\mathbf{2}}$

1.  **[Formulating the Hypotheses]{.underline}**: There are three
    possible forms of hypotheses. They each correspond to a different
    question you might want to ask about the true value of the
    population variance, $\sigma^{2}.$
    $Note:\ in\ each\ of\ the\ hypotheses,\ \sigma_{0}^{2}\text{\ is\ a\ number.\ \ It\ is\ a\ value\ that\ }\sigma^{2}\text{\ hypothesized\ to\ be.}$

+----------------------+----------------------+----------------------+
| Hypotheses for       |                      |                      |
| Hypothesis Tests     |                      |                      |
| about a Single       |                      |                      |
| Population Variance, |                      |                      |
| $\sigma^{2}$         |                      |                      |
+======================+======================+======================+
| **Lower Tail Test**  | **Upper Tail Test**  | **Two-Tailed Test**  |
+----------------------+----------------------+----------------------+
| $$                   | $$                   | $$H_{0}:\ \sigma^{2  |
| H_{0}:\ \sigma^{2} \ | H_{0}:\ \sigma^{2} \ | } = \sigma_{0}^{2}$$ |
| geq \sigma_{0}^{2}$$ | leq \sigma_{0}^{2}$$ |                      |
|                      |                      | $$                   |
| $$H_{A}:\ \sigma^{2  | $$H_{A}:\ \sigma^{2  | H_{A}:\ \sigma^{2} \ |
| } < \sigma_{0}^{2}$$ | } > \sigma_{0}^{2}$$ | neq \sigma_{0}^{2}$$ |
+----------------------+----------------------+----------------------+
| Answers Questions    |                      |                      |
| About:               |                      |                      |
+----------------------+----------------------+----------------------+
| If the true          | If the true          | If the true          |
| population variance, | population variance, | population variance, |
| $\sigma^{2},$ is     | $\sigma^{2},$ is     | $\sigma^{2},$ is     |
| less than a given    | greater than a given | different from a     |
|                      | number,              | given number,        |
| number,              | $\sigma_{0}^{2}$     | $\sigma_{0}^{2}$     |
| $\sigma_{0}^{2}$     |                      |                      |
+----------------------+----------------------+----------------------+

> Upper Tail and Lower Tail Tests are called *one-tailed* or
> *directional* tests. Two-tailed tests are called *non-directional*
> tests.

2.  **[The Test Statistic]{.underline}:**

> Under the assumption that $H_{0}$ is true as an equality, the sampling
> distribution of the sample variance, $s^{2},\ $follows the
> ${\text{chi}\text{-}square\ (\chi}^{2})\ distribution$ distribution.
> Therefore, we standardize our sample against the $\chi^{2}$
> distribution by calculating the following $\chi_{\text{test}}^{2}$
> test statistic:

$$\chi_{\text{test}}^{2} = \frac{\left( n - 1 \right)s^{2}}{\sigma_{0}^{2}}$$

> where

$${s^{2} = the\ sample\ variance\ (or\ the\ sample\ standard\ deviation\ squared)
}{\sigma_{0}^{2} = the\ hypothesized\ population\ variance\ \left( \text{from\ the\ hypotheses} \right)
}{n = the\ sample\ size}$$

> and the degrees of freedom are $df = n - 1$
>
> NOTE: some problems will give you the variances
> ($s^{2}\text{\ and\ }\sigma_{0}^{2})$, in which case you simply plug
> the values into the equation as is. However, some problems will give
> you standard deviations ($\text{s\ and\ }\sigma_{0}),$ in which case
> you would need to square those values in this equation.

3.  **[Deciding whether or not to Reject]{.underline}**
    $\mathbf{H}_{\mathbf{0}}$**:**

> There are two approaches to deciding whether or not to reject the
> Null:

-   In the ***p-value approach***, we **compare** the **p-value** of our
    test statistic to the $\mathbf{\alpha}$ **significance level** and
    reject $H_{0}$ if the p-value is less than or equal to the $\alpha$
    significance level.

-   In the ***critical value approach***, we **compare** the **test
    statistic** to **a critical value** -- this can be done with a
    diagram in which we use the critical value(s) to construct a
    rejection region or regions; if the test statistic is in a rejection
    region, we reject $H_{0}$ and accept $H_{A}.$ Or, this step can be
    accomplished by following the mathematical rules given in the tables
    below.

> REMINDER: you can never *accept* the null hypothesis. Remember the
> swans.

+----------------+----------------+----------------+----------------+
| **When to      |                |                |                |
| Reject**       |                |                |                |
| $\mathbf{H}    |                |                |                |
| _{\mathbf{0}}$ |                |                |                |
| **for**        |                |                |                |
| $\mathbf{\tex  |                |                |                |
| t{Chi}}\text{- |                |                |                |
| }\mathbf{Squar |                |                |                |
| e\ (}\mathbf{\ |                |                |                |
| chi}^{\mathbf{ |                |                |                |
| 2}}\mathbf{)}$ |                |                |                |
| **Test         |                |                |                |
| Statistics:**  |                |                |                |
+================+================+================+================+
|                | **For a Lower  | **For an Upper | **For a        |
|                | Tail Test:**   | Tail Test:**   | Two-Tailed     |
|                |                |                | Test:**        |
+----------------+----------------+----------------+----------------+
| **p-value      | Calculate the  | Calculate the  | The two-tailed |
| approach:**    | lower tail     | upper tail     | $p\text{-      |
|                | $p\text{-      | $p\text{-      | }\text{value}$ |
|                | }\text{value}$ | }\text{value}$ | is two times   |
|                | of             | of             | the one-tailed |
|                | $\chi_{\te     | $\chi_{\te     | p-value of     |
|                | xt{test}}^{2}$ | xt{test}}^{2}$ | $\chi_{\tex    |
|                |                |                | t{test}}^{2}.$ |
|                | If the         | If the         |                |
|                | $\text{LT\ p}\ | $\text{UT\ p}\ | If the         |
|                | text{-}value \ | text{-}value \ | $2T\ p\        |
|                | leq \ \alpha,$ | leq \ \alpha,$ | text{-}value \ |
|                | then reject    | then reject    | leq \ \alpha,$ |
|                | $H_{0}$ and    | $H_{0}$ and    | then reject    |
|                | accept         | accept         | $H_{0}$ and    |
|                | $H_{A}.$       | $H_{A}.$       | accept         |
|                |                |                | $H_{A}.$       |
|                | If the         | If the         |                |
|                | $\text{LT\     | $\text{UT\     | If the         |
|                | p}\text{-}valu | p}\text{-}valu | $2T\           |
|                | e > \ \alpha,$ | e > \ \alpha,$ |  p\text{-}valu |
|                | then do not    | then do not    | e > \ \alpha,$ |
|                | reject         | reject         | then do not    |
|                | $H_{0}$.       | $H_{0}$.       | reject         |
|                | $H_{A}$ is     | $H_{A}$ is     | $H_{0}$.       |
|                | unsupported.   | unsupported.   | $H_{A}$ is     |
|                |                |                | unsupported.   |
+----------------+----------------+----------------+----------------+
| **Critical     | If             | If             | If             |
| Value:         | $              | $              | $\chi_{        |
| Approach**     | \chi_{\text{te | \chi_{\text{te | \text{test}}^{ |
|                | st}}^{2} \leq  | st}}^{2} \geq  | 2} \leq \chi_{ |
|                | \chi_{\alpha,L | \chi_{\alpha,U | \alpha/2,LT}^{ |
|                | T}^{2},\ $then | T}^{2},\ $then | 2}\text{\ OR}$ |
|                | reject         | reject $H_{0}$ |                |
|                | $H_{0}\ $and   | and accept     | $\chi_{\tex    |
|                | accept $H_{A}$ | $H_{A}$        | t{test}}^{2} \ |
|                |                |                | geq \chi_{\alp |
|                | If             | If             | ha/2,UT}^{2}$, |
|                | $\chi_         | $\chi_         | then reject    |
|                | {\text{test}}^ | {\text{test}}^ | $H_{0}$ and    |
|                | {2} > \chi_{\a | {2} < \chi_{\a | accept         |
|                | lpha,LT}^{2}$, | lpha,UT}^{2}$, | $H_{A}$.       |
|                | then do not    | then do not    |                |
|                | reject         | reject         | If             |
|                | $H_{0}\t       | $H_{0}\t       | $\chi_         |
|                | ext{.\ }H_{A}$ | ext{.\ }H_{A}$ | {\alpha/2,LT}^ |
|                | is             | is             | {2} < \ \chi_{ |
|                | unsupported.   | unsupported.   | \text{test}}^{ |
|                |                |                | 2} < \chi_{\al |
|                |                |                | pha/2,UT}^{2}$ |
|                |                |                | then do not    |
|                |                |                | reject         |
|                |                |                | $H_{0}\t       |
|                |                |                | ext{.\ }H_{A}$ |
|                |                |                | is             |
|                |                |                | unsupported.   |
+----------------+----------------+----------------+----------------+
| NOTES:         |                |                |                |
|                |                |                |                |
| 1)             |                |                |                |
| $\chi_{\text{t |                |                |                |
| est}}^{2}\ $is |                |                |                |
|     a Test     |                |                |                |
|     Statistic  |                |                |                |
|                |                |                |                |
| 2)  $\chi_{    |                |                |                |
| \alpha,LT}^{2} |                |                |                |
| ,\ \chi_{\alph |                |                |                |
| a,UT}^{2},\ \c |                |                |                |
| hi_{\alpha/2,L |                |                |                |
| T}^{2},\ and\  |                |                |                |
| \chi_{\alpha/2 |                |                |                |
| ,UT}^{2}\ $are |                |                |                |
|     Critical   |                |                |                |
|     Values     |                |                |                |
|                |                |                |                |
| 3)  $\chi^{2}$ |                |                |                |
|     is based   |                |                |                |
|     on degrees |                |                |                |
|     of         |                |                |                |
|     freedom.   |                |                |                |
|     Be sure to |                |                |                |
|     calculate  |                |                |                |
|     the        |                |                |                |
|                |                |                |                |
|    appropriate |                |                |                |
|     degrees of |                |                |                |
|     freedom    |                |                |                |
|     for the    |                |                |                |
|     test you   |                |                |                |
|     are        |                |                |                |
|                |                |                |                |
|    performing. |                |                |                |
+----------------+----------------+----------------+----------------+

4.  **[Interpreting the test]{.underline}:**

> (Note: This explanation of interpretation holds for ALL hypothesis
> tests.) We start every hypothesis test with a question about the
> parameter of interest, so we must end every hypothesis test with the
> answer to that question. In other words, we must *interpret* the
> conclusion of our test in terms of the original question.
>
> Remember: in hypothesis testing you can never prove the null
> hypothesis. You can only prove the alternative hypothesis: when you
> reject the null and accept the alternative, then at your given
> $\alpha$ level of significance you may conclude that $H_{A}$ is true.
> If you do not reject $H_{0},\ $then you must conclude that $H_{A}$ is
> unsupported by the evidence. This gives us a clear guideline for how
> to *interpret* hypothesis tests: *always look at the alternative
> hypothesis!*
>
> In all that follows, you would substitute the actual words and numbers
> from your hypothesis test for the symbols. Notice that each
> interpretation simply states the alternative hypothesis in words, and
> says either that it is true or that we cannot conclude that it is
> true.

+----------------+----------------+----------------+----------------+
| **How to       |                |                |                |
| Interpret a    |                |                |                |
| Hypothesis     |                |                |                |
| Test about a   |                |                |                |
| Single         |                |                |                |
| Population     |                |                |                |
| Variance,**    |                |                |                |
| $\mathb        |                |                |                |
| f{\sigma}^{\ma |                |                |                |
| thbf{2}}$**:** |                |                |                |
+================+================+================+================+
| **When you:**  | **For a Lower  | **For an Upper | **For a        |
|                | Tail Test:**   | Tail Test:**   | Two-Tailed     |
|                |                |                | Test:**        |
+----------------+----------------+----------------+----------------+
| **Reject**     | At the         | At the         | At the         |
| $\mathbf{H}    | $\text{Î±\      | $\text{Î±\      | $\text{Î±\      |
| _{\mathbf{0}}$ | }$significance | }$significance | }$significance |
|                | level, we can  | level, we can  | level, we can  |
|                | conclude that  | conclude that  | conclude that  |
|                | $\             | $\             | $\             |
|                | sigma^{2}\ $is | sigma^{2}\ $is | sigma^{2}\ $is |
|                | less than      | greater than   | different than |
|                | $\s            | $\s            | $\s            |
|                | igma_{0}^{2}$. | igma_{0}^{2}$. | igma_{0}^{2}$. |
+----------------+----------------+----------------+----------------+
| **Do not       | At the         | At the         | At the         |
| reject**       | $\text{Î±\      | $\text{Î±\      | $\text{Î±\      |
| $\mathbf{H}    | }$significance | }$significance | }$significance |
| _{\mathbf{0}}$ | level, we      | level, we      | level, we      |
|                | cannot         | cannot         | cannot         |
|                | conclude that  | conclude that  | conclude that  |
|                | $\             | $\             | $\             |
|                | sigma^{2}\ $is | sigma^{2}\ $is | sigma^{2}\ $is |
|                | less than      | greater than   | different than |
|                | $\s            | $\s            | $\s            |
|                | igma_{0}^{2}$. | igma_{0}^{2}$. | igma_{0}^{2}$. |
+----------------+----------------+----------------+----------------+
| NOTES:         |                |                |                |
|                |                |                |                |
| 1)  $\         |                |                |                |
| sigma^{2}\ $is |                |                |                |
|     the true   |                |                |                |
|     value of   |                |                |                |
|     the        |                |                |                |
|     population |                |                |                |
|     variance   |                |                |                |
|                |                |                |                |
| 2)  $\         |                |                |                |
| sigma_{0}^{2}$ |                |                |                |
|     is a       |                |                |                |
|     number     |                |                |                |
|     that is    |                |                |                |
|     the        |                |                |                |
|                |                |                |                |
|   hypothesized |                |                |                |
|     value of   |                |                |                |
|                |                |                |                |
|   $\sigma^{2}$ |                |                |                |
+----------------+----------------+----------------+----------------+

**[Assumptions Underlying Hypothesis Tests about a Single Population
Variance]{.underline}**

All hypothesis tests use sampling distributions to determine the
probability of sample statistics. In order for us to be confident that
our choice of sampling distribution for any given test really is the way
the sample statistic is distributed, certain assumptions must be met. If
the assumptions are not met -- that is, if any given assumption is not
true -- then we cannot rely on the results of the hypothesis tests. They
may mislead us, give us the wrong answers, and cause us to draw the
wrong conclusions.

When making inferences about a single population variance using
$\chi^{2}$:

a)  The underlying population must be normally distributed. If it is not
    normal, then the sampling distribution is unknown, so we cannot use
    $\chi^{2}$as the sampling distribution.

b)  The sample must be random. If it is not random, then the sampling
    distribution is unknown, so we cannot use $\chi^{2}$as the sampling
    distribution.

**[Part Two: Hypothesis Tests about Two Population
Variances,]{.underline}**
$\mathbf{\sigma}_{\mathbf{1}}^{\mathbf{2}}\mathbf{\text{\ and\ }}\mathbf{\sigma}_{\mathbf{2}}^{\mathbf{2}}$

1.  **[Formulating the Hypotheses]{.underline}**: There are two possible
    forms of hypotheses: upper tail and two-tailed. Whenever a
    directional question is asked (that is, a less-than or greater-than
    question), we will formulate the hypothesis test as an upper tail
    test. The two forms correspond to different questions you might want
    to ask about the true values of two population variances,
    $\sigma_{1}^{2}\text{\ and\ }\sigma_{2}^{2}.$

$${Note:\ in\ each\ hypothesis,\ \sigma_{1}^{2} = \ the\ variance\ of\ population\ 1,and
}{\text{\ \ }\sigma_{2}^{2} = the\ variance\ of\ population\ 2}$$

+----------------------------------+----------------------------------+
| Hypotheses for Hypothesis Tests  |                                  |
| about Two Population Variances,  |                                  |
| $\sigma_{1}^                     |                                  |
| {2}\text{\ and\ }\sigma_{2}^{2}$ |                                  |
+==================================+==================================+
| **Upper Tail Test**              | **Two-Tailed Test**              |
+----------------------------------+----------------------------------+
| $$H_{0}:\ \sig                   | $$H_{0}:\ \                      |
| ma_{1}^{2} \leq \sigma_{2}^{2}$$ | sigma_{1}^{2} = \sigma_{2}^{2}$$ |
|                                  |                                  |
| $$H_{A}:\ \                      | $$H_{A}:\ \sig                   |
| sigma_{1}^{2} > \sigma_{2}^{2}$$ | ma_{1}^{2} \neq \sigma_{2}^{2}$$ |
+----------------------------------+----------------------------------+
| Answers questions about:         |                                  |
+----------------------------------+----------------------------------+
| If the true population variance  | If the true population variance  |
| of population 1,                 | of population 1,                 |
| $\mathbf{\si                     | $\mathbf{\si                     |
| gma}_{\mathbf{1}}^{\mathbf{2}},$ | gma}_{\mathbf{1}}^{\mathbf{2}},$ |
| is **greater than** the true     | is **different from** the true   |
| population variance of           | population variance of           |
| population 2,                    | population 2,                    |
| $\mathbf{\si                     | $\mathbf{\si                     |
| gma}_{\mathbf{2}}^{\mathbf{2}}.$ | gma}_{\mathbf{2}}^{\mathbf{2}}.$ |
|                                  |                                  |
| NOTE: This test also tells you   |                                  |
| if the variance of population 2, |                                  |
| $\mathbf{\si                     |                                  |
| gma}_{\mathbf{2}}^{\mathbf{2}}$, |                                  |
| is **less than** the variance of |                                  |
| population 1,                    |                                  |
| $\mathbf{\si                     |                                  |
| gma}_{\mathbf{1}}^{\mathbf{2}}$. |                                  |
+----------------------------------+----------------------------------+

> The Upper Tail Test is a *one-tailed* or *directional* test.
> Two-tailed tests are called *non-directional* tests.

**[\
]{.underline}**

2.  **[The Test Statistic]{.underline}:**

> **To properly formulate the test statistic, you MUST use the larger
> sample variance as** $\mathbf{s}_{\mathbf{1}}^{\mathbf{2}}$ **and the
> smaller sample variance as** $\mathbf{s}_{\mathbf{2}}^{\mathbf{2}}$**.
> You can use this information to label the populations: the sample with
> the larger sample variance is sample 1 from Population 1, and the
> sample with the smaller variance is sample 2 from Population 2. By
> doing this, you will ensure that the test statistic is in the upper
> tail of the F distribution, and the critical values on the F table
> will work properly.**
>
> For hypothesis testing about two population variances, the sample
> statistics of interest are the sample variances,
> $s_{1}^{2}\text{\ and\ }s_{2}^{2}.$ Under the assumption that $H_{0}$
> is true as an equality, the sampling distribution of the ratio of the
> sample variances follows the F distribution. Therefore, we standardize
> our samples against the F distribution by calculating the following F
> test statistic:

$$F_{\text{test}} = \ \frac{s_{1}^{2}}{s_{2}^{2}}\ $$

$$with\ Numerator\ degrees\ of\ freedom = df_{1} = \ n_{1} - 1$$

$$\ and\ Denominator\ degrees\ of\ freedom = df_{2} = \ n_{2} - 1$$

> where

$${s_{1}^{2} = the\ sample\ variance\ of\ sample\ 1
}{s_{2}^{2} = the\ sample\ variance\ of\ sample\ 2
}{n_{1} = the\ sample\ size\ of\ sample\ 1
}{n_{2} = the\ sample\ size\ of\ sample\ 2}$$

> NOTE: some problems will give you the variances
> ($s_{1}^{2}\text{\ and\ }s_{2}^{2})$, in which case you simply plug
> the values into the equation as is. However, some problems will give
> you standard deviations ($s_{1}\text{\ and\ }s_{2}),$ in which case
> you would need to square those values in this equation.

3.  **[Deciding whether or not to Reject]{.underline}**
    $\mathbf{H}_{\mathbf{0}}$**:**

> There are two approaches to deciding whether or not to reject the
> Null:

-   In the ***p-value approach***, we **compare** the **p-value** of our
    test statistic to the $\mathbf{\alpha}$ **significance level** and
    reject $H_{0}$ if the p-value is less than or equal to the $\alpha$
    significance level.

-   In the ***critical value approach***, we **compare** the **test
    statistic** to **a critical value** -- this can be done with a
    diagram in which we use the critical value(s) to construct a
    rejection region or regions; if the test statistic is in a rejection
    region, we reject $H_{0}$ and accept $H_{A}.$ Or, this step can be
    accomplished by following the mathematical rules given in the tables
    below.

> REMINDER: you can never *accept* the null hypothesis. Remember the
> swans.

The Hypothesis Testing rejection rules for $F$ test statistics are:

+----------------------+----------------------+----------------------+
| **When to Reject**   |                      |                      |
| $\mat                |                      |                      |
| hbf{H}_{\mathbf{0}}$ |                      |                      |
| **if the Test        |                      |                      |
| Statistic is**       |                      |                      |
| $\mathbf{F}$         |                      |                      |
+======================+======================+======================+
|                      | **For an Upper Tail  | **For a Two-Tailed   |
|                      | Test:**              | Test:**              |
+----------------------+----------------------+----------------------+
| **p-value            | Calculate the upper  | The two-tailed       |
| approach:**          | tail                 | $p\t                 |
|                      | $p\                  | ext{-}\text{value\ o |
|                      | text{-}\text{value}$ | f\ }F_{\text{test}}$ |
|                      | of $F_{\text{test}}$ | is                   |
|                      |                      |                      |
|                      | If the               | 2 x upper tail       |
|                      | $\                   | p-value of           |
|                      | text{UT\ p}\text{-}v | $F_{\text{test}}$    |
|                      | alue \leq \ \alpha,$ |                      |
|                      | then reject $H_{0}$  | If the               |
|                      | and accept $H_{A}.$  | $2T\ p\text{-}v      |
|                      |                      | alue \leq \ \alpha,$ |
|                      | If the               | then reject $H_{0}$  |
|                      | $\text{UT\ }p\tex    | and accept $H_{A}.$  |
|                      | t{-}value > \alpha,$ |                      |
|                      | then do not reject   | If the               |
|                      | $H_{0}               | $2T\ p\tex           |
|                      | \text{.\ }H_{A}\ $is | t{-}value > \alpha,$ |
|                      | unsupported.         | then do not reject   |
|                      |                      | $H_{0}               |
|                      |                      | \text{.\ }H_{A}\ $is |
|                      |                      | unsupported.         |
+----------------------+----------------------+----------------------+
| **Critical Value     | If                   | If                   |
| Approach:**          | $F_{\text{test}} \ge | $F                   |
|                      | q F_{\alpha},\ $then | _{\text{test}} \geq  |
|                      | reject $H_{0}$ and   | F_{\alpha/2},\ $then |
|                      | accept $H_{A}.$      | reject $H_{0}$ and   |
|                      |                      | accept $H_{A}.$      |
|                      | If                   |                      |
|                      | $F_{\text{test}}     | If                   |
|                      | < F_{\alpha},\ $then | $F_{\text{test}} <   |
|                      | do not reject        | F_{\alpha/2},\ $then |
|                      | $H_{0}\text{.\ }$    | do not reject        |
|                      | $H_{A}\ $is          | $H_{0}\text{.\ }$    |
|                      | unsupported.         | $H_{A}\ $is          |
|                      |                      | unsupported.         |
+----------------------+----------------------+----------------------+
| NOTES:               |                      |                      |
|                      |                      |                      |
| 1)  $                |                      |                      |
| F_{\text{test}}\ $is |                      |                      |
|     a Test Statistic |                      |                      |
|                      |                      |                      |
| 2)  $F_{\alpha}$ and |                      |                      |
|     $F_{\alpha/2}$   |                      |                      |
|     are Critical     |                      |                      |
|     Values.          |                      |                      |
|                      |                      |                      |
| 3)  The degrees of   |                      |                      |
|     freedom are      |                      |                      |
|     $numera          |                      |                      |
| tor\ df = n_{1} - 1$ |                      |                      |
|     and              |                      |                      |
|     $denomina        |                      |                      |
| tor\ df = n_{2} - 1$ |                      |                      |
+----------------------+----------------------+----------------------+

4.  **[Interpreting the test]{.underline}:**

> (Note: This explanation of interpretation holds for ALL hypothesis
> tests.) We start every hypothesis test with a question about the
> parameter of interest, so we must end every hypothesis test with the
> answer to that question. In other words, we must *interpret* the
> conclusion of our test in terms of the original question.
>
> Remember: in hypothesis testing you can never prove the null
> hypothesis. You can only prove the alternative hypothesis: when you
> reject the null and accept the alternative, then at your given
> $\alpha$ level of significance you may conclude that $H_{A}$ is true.
> If you do not reject $H_{0},\ $then you must conclude that $H_{A}$ is
> unsupported by the evidence. This gives us a clear guideline for how
> to *interpret* hypothesis tests: *always look at the alternative
> hypothesis!*
>
> In all that follows, you would substitute the actual words and numbers
> from your hypothesis test for the symbols. Notice that each
> interpretation simply states the alternative hypothesis in words, and
> says either that it is true or that we cannot conclude that it is
> true.

+----------------------+----------------------+----------------------+
| **How to Interpret a |                      |                      |
| Hypothesis Test      |                      |                      |
| about Two Population |                      |                      |
| Variances,**         |                      |                      |
| $\m                  |                      |                      |
| athbf{\sigma}_{\math |                      |                      |
| bf{1}}^{\mathbf{2}}\ |                      |                      |
| mathbf{\text{\ and\  |                      |                      |
| }}\mathbf{\sigma}_{\ |                      |                      |
| mathbf{2}}^{\mathbf{ |                      |                      |
| 2}}\mathbf{\ }$**:** |                      |                      |
+======================+======================+======================+
| **When you:**        | **For an Upper Tail  | **For a Two-Tailed   |
|                      | Test:**              | Test:**              |
+----------------------+----------------------+----------------------+
| **Reject**           | This result can be   | At the               |
| $\mat                | interpreted two      | $\te                 |
| hbf{H}_{\mathbf{0}}$ | ways. Choose the     | xt{Î±\ }$significance |
|                      | interpretation that  | level, we can        |
|                      | is appropriate in    | conclude that        |
|                      | the context of the   | $\sigma_{1}^{2}\ $is |
|                      | problem:             | different than       |
|                      |                      | $\sigma_{2}^{2}$.    |
|                      | 1)  At the           |                      |
|                      |     $\te             |                      |
|                      | xt{Î±\ }$significance |                      |
|                      |     level, we can    |                      |
|                      |     conclude that    |                      |
|                      |     $\mathb          |                      |
|                      | f{\sigma}_{\mathbf{1 |                      |
|                      | }}^{\mathbf{2}}\ $is |                      |
|                      |     **greater** than |                      |
|                      |     $\ma             |                      |
|                      | thbf{\sigma}_{\mathb |                      |
|                      | f{2}}^{\mathbf{2}}$. |                      |
|                      |                      |                      |
|                      | 2)  At the           |                      |
|                      |     $\te             |                      |
|                      | xt{Î±\ }$significance |                      |
|                      |     level, we can    |                      |
|                      |     conclude that    |                      |
|                      |     $\mathb          |                      |
|                      | f{\sigma}_{\mathbf{2 |                      |
|                      | }}^{\mathbf{2}}\ $is |                      |
|                      |     **less** than    |                      |
|                      |     $\ma             |                      |
|                      | thbf{\sigma}_{\mathb |                      |
|                      | f{1}}^{\mathbf{2}}$. |                      |
+----------------------+----------------------+----------------------+
| **Do not reject**    | This result can be   | At the               |
| $\mat                | interpreted two      | $\te                 |
| hbf{H}_{\mathbf{0}}$ | ways. Choose the     | xt{Î±\ }$significance |
|                      | interpretation that  | level, we cannot     |
|                      | is appropriate in    | conclude that        |
|                      | the context of the   | $\sigma_{1}^{2}\ $is |
|                      | problem:             | different than       |
|                      |                      | $\sigma_{2}^{2}$.    |
|                      | 1)  At the           |                      |
|                      |     $\te             |                      |
|                      | xt{Î±\ }$significance |                      |
|                      |     level, we cannot |                      |
|                      |     conclude that    |                      |
|                      |     $\mathb          |                      |
|                      | f{\sigma}_{\mathbf{1 |                      |
|                      | }}^{\mathbf{2}}\ $is |                      |
|                      |     **greater** than |                      |
|                      |     $\ma             |                      |
|                      | thbf{\sigma}_{\mathb |                      |
|                      | f{2}}^{\mathbf{2}}$. |                      |
|                      |                      |                      |
|                      | 2)  At the           |                      |
|                      |     $\te             |                      |
|                      | xt{Î±\ }$significance |                      |
|                      |     level, we cannot |                      |
|                      |     conclude that    |                      |
|                      |     $\mathb          |                      |
|                      | f{\sigma}_{\mathbf{2 |                      |
|                      | }}^{\mathbf{2}}\ $is |                      |
|                      |     **less** than    |                      |
|                      |     $\ma             |                      |
|                      | thbf{\sigma}_{\mathb |                      |
|                      | f{1}}^{\mathbf{2}}$. |                      |
+----------------------+----------------------+----------------------+
| NOTES:               |                      |                      |
|                      |                      |                      |
| 1)                   |                      |                      |
| $\sigma_{1}^{2}\ $is |                      |                      |
|     the true         |                      |                      |
|     variance of      |                      |                      |
|     population 1;    |                      |                      |
|                      |                      |                      |
| $\sigma_{2}^{2}\ $is |                      |                      |
|     the true         |                      |                      |
|     variance of      |                      |                      |
|     population 2     |                      |                      |
+----------------------+----------------------+----------------------+

**[Assumptions Underlying Hypothesis Tests about Two Population
Variances]{.underline}**

All hypothesis tests use sampling distributions to determine the
probability of sample statistics. In order for us to be confident that
our choice of sampling distribution for any given test really is the way
the sample statistic is distributed, certain assumptions must be met. If
the assumptions are not met -- that is, if any given assumption is not
true -- then we cannot rely on the results of the hypothesis tests. They
may mislead us, give us the wrong answers, and cause us to draw the
wrong conclusions.

When making inferences about two population variances using $F$:

a)  The underlying populations must both be normally distributed. If
    they are not normal, then the sampling distribution is unknown, so
    you cannot use F as the sampling distribution.

b)  Both samples must be random, and they must be independent of one
    another. If they are not random and independent, then the sampling
    distribution is unknown, so you cannot use F as the sampling
    distribution.

**[The Chi-Square (]{.underline}**$\mathbf{\chi}^{\mathbf{2}}\mathbf{)}$
**[Distribution]{.underline}**

The $\chi^{2}$ test statistics and $\chi^{2}$ sampling distributions are
used for inferences about a single population variance (Ch 11), goodness
of fit tests (Ch 12), and tests of independence (Ch 12).

The **Chi-Square distribution**:

-   is a probability distribution, so the area under the curve is 1

-   has degrees of freedom (df). The calculation for degrees of freedom
    will depend on the type of hypothesis test or confidence interval.

-   is ALWAYS positive and is NOT symmetrical

    -   Implication: Critical Values in the upper tail **and** lower
        tail will be **two different positive numbers**

-   The mean of a chi-square distribution is equal to its degrees of
    freedom. How is this relevant?

    -   If
        $\mathbf{\chi}_{\mathbf{\text{test}}}^{\mathbf{2}}\mathbf{> df}$,
        then the test statistic is in the **upper tail**

    -   If
        $\mathbf{\chi}_{\mathbf{\text{test}}}^{\mathbf{2}}\mathbf{< df}$,
        then the test statistic is in the **lower tail**

The Chi-Square table contains Critical Values. Use the column headings
marked "Area in Lower Tail" for LT critical values, and "Area in Upper
Tail" for UT critical values. Excel is used to calculate p-values for
Chi-Square test statistics.

The Hypothesis Testing rejection rules for $\chi^{2}$ test statistics
are listed in Ch 11: Handout \#2, Part 1.

**[Practice using Chi-Square: Exercises]{.underline}**

***Exercise 1*:**

a\) If df = 20 and $\alpha = 0.10$, what is the critical value of
Chi-square for an upper tail test?

b\) Suppose the test-statistic in this upper tail test is
$\chi_{\text{test}}^{2} = 31.4$. Use the critical value approach to
decide whether or not to reject the null hypothesis.

c\) Suppose the test-statistic in this upper tail test is
$\chi_{\text{test}}^{2} = 31.4$. Use the p-value approach to decide
whether or not to reject the null hypothesis.

***Exercise 2:***

a\) If df = 45 and $\alpha = 0.05$, what is the critical value of
Chi-Square for a lower tail test?

b\) Suppose that the test-statistic in this lower tail test is
$\chi_{\text{test}}^{2} = 46.1$. Use the critical value approach to
decide whether or not to reject the null hypothesis.

c\) Suppose that the test-statistic in this lower tail test is
$\chi_{\text{test}}^{2} = 46.1$. Use the p-value approach to decide
whether or not to reject the null hypothesis.

***Exercise 3:***

a\) If df = 16 and $\alpha = 0.05$, what are the critical values of
Chi-square for a two tailed test?

b\) Suppose that the test-statistic in this two-tailed test is
$\chi_{\text{test}}^{2} = 27.88.$ Use the critical value approach to
decide whether or not to reject the null hypothesis.

c\) Suppose that the test-statistic in this two-tailed test is
$\chi_{\text{test}}^{2} = 27.88.$ Use the p-value approach to decide
whether or not to reject the null hypothesis.

**[Hypothesis Tests about a Single Population Variance,]{.underline}**
$\mathbf{\sigma}^{\mathbf{2}}$**[: Exercises]{.underline}**

*Example 1:* A cranberry juice manufacturing company has designed its
bottle filling process with a variance of 0.0225 ounces^2^. It conducts
regular quality control tests to see whether the process is running
according to this design specification. To that end, a quality control
analyst takes a random sample of 25 bottles of cranberry juice and finds
this sample has a variance of 0.04 ounces^2^. The population of bottle
contents is assumed to be normal.

Conduct and interpret a hypothesis test at the $\alpha = 0.05$
significance level to determine whether the population variance of the
contents of the bottles is different from 0.0225. Is there evidence that
the process is out of adjustment?

*\
*

*Example 2:* Kombucha is a sparkling, fermented tea which naturally
contains some alcohol. In order to prevent kombucha from being regulated
as an alcoholic beverage, makers must keep their tea from exceeding the
legal limit for alcohol percentage. In order to stay legal, the
manufacturer needs the population variance of their kombucha to be less
than 0.00985. The population is normally distributed. The manufacturer
takes a random sample of 66 bottles of kombucha and calculates a sample
variance of 0.00624.

Conduct and interpret a hypothesis test to determine whether the
population variance of alcohol percentage in the kombucha is less than
0.00985 at an $\alpha = 0.01$ significance level. Is the kombucha brewer
staying within the legal limit?

**[The F distribution]{.underline}**

The F test statistic and F sampling distribution are used for inferences
about two population variances (Ch 11), analysis of variance tests (Ch
13), and regression analysis (Ch 14-15).

The $F$ distribution:

-   A probability distribution, so the area under the curve is 1

-   Has TWO separate degrees of freedom

    -   The *numerator degrees of freedom* $(df_{1})$*,* which is based
        on the numerator in the test statistic.

    -   The *denominator degrees of freedom* $(df_{2})$, which is based
        on the denominator of the test statistic.

-   is ALWAYS positive and is NOT symmetrical

    -   In hypothesis tests about two population variances, we will
        construct our test statistic to be in the upper tail, so there
        will be one critical value ${(F}_{\alpha})$ for the directional
        test (upper tail) and one critical value $(F_{\alpha/2})$ for
        the two-tailed test.

The F table contains a selection of Critical Values. Excel is used to
calculate p-values for F test statistics.

The Hypothesis Testing rejection rules for $F$ test statistics are
listed in Ch 11: Handout \#2, Part 2.

**[Practice using F: Exercises]{.underline}**

*Exercise 1:* Consider an upper tail test at an $\alpha = .05$
significance level, with
$numerator\ degrees\ of\ freedom\ df = 25\ and\ denominator\ degrees\ of\ freedom\ df = 29.$

a)  What is the critical value?

b)  Suppose the test statistic is $F_{\text{test}} = 3.1$. By the
    > Critical Value approach, decide whether or not to reject the null
    > hypothesis.

c)  Suppose the test statistic is $F_{\text{test}} = 3.1$. By the
    > p-value approach, decide whether or not to reject the null
    > hypothesis.

*Exercise 2:* Consider a two-tailed test at an $\alpha = .05$
significance level, with
$numerator\ degrees\ of\ freedom\ df = 6\ and\ denominator\ degrees\ of\ freedom\ df = 7.$

a)  What is the critical value?

b)  Suppose that the test statistic is $F_{\text{test}} = 4.82$. By the
    Critical Value approach, decide whether or not to reject the null
    hypothesis.

c)  Suppose that the test statistic is $F_{\text{test}} = 4.82.$ By the
    p-value approach, decide whether or not to reject the null
    hypothesis.

**[Hypothesis Tests about Two Population Variances,]{.underline}**
$\mathbf{\sigma}_{\mathbf{1}}^{\mathbf{2}}\mathbf{\text{\ and\ }}\mathbf{\sigma}_{\mathbf{2}}^{\mathbf{2}}$**[:
Exercises]{.underline}**

*Example 1:* Our transport company would like to prove that we have
lower variability in delivery times than our competitor. An analyst at
our company obtains two random, independent samples. A random sample of
41 of our deliveries had a variance in delivery time of 2.56. A random
sample of 61 of our competitor's deliveries had a variance in delivery
time of 4.15. Delivery times for both companies are normally
distributed.

Conduct and interpret a hypothesis test to determine whether we have a
lower population variance than our competitor at an $\alpha = 0.01$
significance level.

*Example 2:* A manufacturer of bathroom scales makes two models, Model A
and Model B. Reliability, which is the extent to which a measurement is
consistent (i.e. the same) when repeated under similar conditions, is an
important characteristic of a bathroom scale. A scale that is more
reliable can command a higher price. (HINT: Consider what reliability
means in terms of the variance)

The manufacturer conducted a series of tests in which it weighed a
standard 150lb weight on the two scales. It then sampled the weights
displayed on the scales. These were independent, random samples, and the
populations are assumed to be normally distributed. The sample size for
Model A was 21 weigh-ins and the sample variance was 0.16. The sample
size for Model B was 26 weigh-ins and the sample variance was 0.0625.

Conduct and interpret a hypothesis test to determine whether the
population variances for these two scales differ at the $\alpha = .05$
significance level. Based on your analysis, which scale should have a
higher price?
