**[Simple Linear Regression: What it all means]{.underline}**

Simple linear regression is a statistical modeling procedure that models
the relationship between one Dependent Variable (DV), $y,$ and one
Independent Variable (IV), $\text{x.}$ A model is a simplified
representation that captures important characteristics but leaves out
many details. The model of the relationship between $x$ and
$\text{y\ }$in linear regression is a **straight line**.

The linear relationship between $x$ and $y$ in the whole **population of
interest** is represented in the **Regression Model**:

$$y = \ \beta_{0} + \beta_{1}x + \ \epsilon$$

> where

$${y = the\ dependent\ variable
}{x = the\ independent\ variable
}{\beta_{0} = the\ population\ intercept,\ called\ beta\ nought\ or\ beta\ zero
}{\beta_{1} = the\ slope\ on\ x,the\ coefficient\ on\ x,\ called\ beta\ one
}{\epsilon = random\ error
}$$

The betas are population parameters, and as such, we cannot observe them
directly. Instead, we take a random sample from the population and use
the data from the sample to **estimate** the betas. That is what the
regression procedure does: it analyzes all the data in the sample and
uses it to calculate the **Estimated Regression Equation**:

$$\widehat{y} = b_{0} + b_{1}x$$

$${\text{where\ }\widehat{y} = the\ expected\ or\ predicted\ value\ of\ y\ (the\ mean\ of\ y)\ 
}{b_{0} = the\ sample\ intercept;the\ estimate\ of\ \beta_{0}
}{b_{1} = the\ sample\ slope;the\ estimate\ of\ \beta_{1}}$$

Regression output contains many different quantities, all of which are
calculated from the data: that is, from the individual values of $x$ and
$y$ in the observations in the data set.

The data set will look like this, although the columns do not have to be
in this order:

+----------------------+----------------------+----------------------+
| **Observation**      | **Dependent**        | **Independent**      |
|                      |                      |                      |
| **                   | **Variable**         | **Variable**         |
| (**$\mathbf{i}$**)** |                      |                      |
|                      | **(**$\mathbf{y      | **(**$\mathbf{x      |
|                      | }_{\mathbf{i}}$**)** | }_{\mathbf{i}}$**)** |
+======================+======================+======================+
| 1                    | $$y_{1}$$            | $$x_{1}$$            |
+----------------------+----------------------+----------------------+
| 2                    | $$y_{2}$$            | $$x_{2}$$            |
+----------------------+----------------------+----------------------+
| ...                  | ...                  | ...                  |
+----------------------+----------------------+----------------------+
| $$n$$                | $$y_{n}$$            | $$x_{n}$$            |
+----------------------+----------------------+----------------------+

The Observation column -- which may be labeled by Case or Subject, or
whatever is appropriate for the study -- is a unique identifier for each
observation in the dataset. Every observation in the dataset consists of
measurements for each variable. For example, if this dataset was for a
regression investigating income and years of education, each observation
would be an individual person ($i$), and for each person, income
($y_{i}$) and years of education ($x_{i}$) would be measured and
recorded. From the values of $y_{i}$ and $x_{i}$ for each observation
$i$, we can calculate all of the values in the regression output.

Linear regression calculates the estimated regression equation by
employing something called the **Least Squares Criterion**, which
[minimizes]{.underline} the squared vertical distance between each
observed value of $y$ in the sample (each $y_{i}$) and the predicted $y$
from the estimated regression equation (each ${\widehat{y}}_{i}$) at
every value of $x_{i}$. This vertical distance between observed and
predicted ($y_{i} - {\widehat{y}}_{i}$) is a very important quantity in
regression, called the Residual. Mathematically, the Least Squares
Criterion is stated:

$$Least\ Squares\ Criterion = min\sum_{}^{}\left( y_{i} - {\widehat{y}}_{i} \right)^{2}$$

So, the Least Squares Criterion minimizes the sum of the squared
residuals. **Conceptually, the estimated regression equation that linear
regression calculates is the line that is as close as possible to all
the points in the data set at once: that is what it means to satisfy the
Least Squares Criterion.**

For linear regression with more than one independent variable, matrix
algebra or calculus is required to calculate the estimated regression
equation. The equations simplify in the case of Simple Linear
Regression, and so we could easily calculate the equation, and in fact
all the regression output, by hand -- if we only had the time.

For now, let's concentrate on learning what the numbers in the
regression output mean.

**[Interpreting Linear Regression Output]{.underline}**

NOTE: in many of the interpretations, I refer to the DV as $y$ and the
IVs as $x,\ x_{1},\ x_{2}\ $etc. When actually interpreting a
regression, these variables have meaning and the meaning should be
substituted in for these placeholder variables.

**[In the *Regression Statistics* table (in order of
importance/informativeness)]{.underline}:**

1)  *The **Coefficient of Determination,
    ***$\mathbf{R}^{\mathbf{2}}\mathbf{,\ }$*(called **R Square** in
    Excel) gives the proportion of the variability in the dependent
    variable that is explained by the independent variable or
    variables.* $R^{2}$ *varies from 0 to 1, and when interpreted, it is
    converted to a percentage.*

    a.  *In Simple Linear Regression:* $R^{2}$ *measures the proportion
        of the variability in* $\text{y\ }$*that is explained by* $x$*.*

> *[Example]{.underline}: If* $R^{2} = 0.7231,$ *then the
> **interpretation** is: 72.31% of the variability in* $y$ *is explained
> by* $\text{x.}$

b.  In Multiple Regression: $R^{2}$ *measures the proportion of the
    variability in* $\text{y\ }$*that is explained by all of the
    independent variables,* $x_{1},\ x_{2},\ \ldots,\ x_{p}$*.*

> *[Example]{.underline}: If* $R^{2} = 0.4979,$ *then the
> **interpretation** is: 49.79% of the variability in* $y$ *is explained
> by* $x_{1},\ x_{2},\ldots,\ x_{p}.$

2)  The **Standard Error of the Estimate, s,** often called the **Root
    Mean Square Error (RMSE)**, measures the accuracy of the predictions
    made by the Estimated Regression Equation. It is the average
    distance that the observed $y$ values in the sample fall from the
    regression line.

    a.  **Interpretation:** the average error we would make using the
        estimated regression equation to predict $y$. In other words: if
        we used the estimated regression equation to predict $y$, we
        would be off by **s** on average.

> [Example]{.underline}: If $s = 4\ units,$ then the **interpretation**
> is: $4\ units$ *is the average error we would make if we used the
> estimated regression equation to predict* $\text{y.}$
>
> Another way to state the same thing is: *If we used the estimated
> regression equation to predict y, we would be off by 4 units on
> average.*

b.  The Standard Error of the Estimate is in the [same units
    as]{.underline} $y$[,]{.underline} which makes it very easy to
    understand and interpret.

c.  The lower the Standard Error of the Estimate, the more accurate the
    predictions, so the lower the better!

d.  Given two regression models that predict the same DV measured in the
    same units, **the Standard Error of the Estimate can be used to
    choose between models**: **whichever one has the lower Standard
    Error of the Estimate is the better model**, because the predictions
    made with it will be more accurate.

```{=html}
<!-- -->
```
3)  **Adjusted**
    $\mathbf{R}^{\mathbf{2}}\mathbf{\ (notated\ }\mathbf{R}_{\mathbf{A}}^{\mathbf{2}}\mathbf{)}$
    is used as a model comparison statistic in [multiple
    regression]{.underline}. When comparing two models that predict the
    same DV in the same units, the one with the higher **Adjusted**
    $\mathbf{R}^{\mathbf{2}}$ is the better model.

    a.  NOTE: it is **not** appropriate to use regular old $R^{2}$ to
        compare two models, because adding another IV to a regression
        model will increase $R^{2}$, whether that IV explains anything
        or not. **Adjusted** $\mathbf{R}^{\mathbf{2}}$**,** on the other
        hand, will only increase if the additional IV adds explanatory
        power, and will actually decrease if it does not.

4)  The **Correlation Coefficient** (called **Multiple R** in Excel) is
    denoted $R_{\text{xy}}$ in simple linear regression and $R_{Å·y}$ *in
    multiple regression. It gives a descriptive measure of the strength
    of the linear association between* $y$ *and* $x$ *(in simple linear
    regression) and between* $y$ *and all the* $x\text{'}s$ *in multiple
    regression.*

    a.  The closer to 1, the stronger the association. Values close to 0
        indicate that x and y are not linearly related.

    b.  No rules are set in stone about what constitutes a "strong" or
        "weak" relationship. A common rule of thumb: \< 0.25: weak
        linear association; between 0.25 and 0.75: moderate linear
        association; \> 0.75: strong linear association

**[The *ANOVA* table:]{.underline}**

-   recall: the sample data is our best representation of the underlying
    population, so if our model is good at predicting the sample values,
    we can infer it will also be good at predicting population values

-   The ANOVA table in the regression output compares two different
    methods of predicting the sample data in an effort to help us decide
    if our regression model is any good.

    -   Premise: there are two alternative models you could use to
        predict $y$. One model includes information about $x$ and one
        does not:

        -   First, you could use the Estimated Regression Equation to
            predict $y$ (in other words, predict ${\widehat{y}}_{i}$
            $\ $at each $x_{i}$)

        -   Second, you could use the next best option, which is to just
            use $\overline{y}$ to predict $y$ (in other words, predict
            $\overline{y}$ (the mean of y) at each $x_{i}$)

            -   This is also called the 'Intercept only model' because
                the equation of $\text{y\ }$does not include any $x$
                variables, thus being only an intercept, and
                representing a horizontal line at $y = \ \overline{y}$.

    -   In the ANOVA table, you calculate the total errors you would
        make if you predicted the actual observed data using each of
        these models, square those errors, and sum them. In this way, we
        can compare the two possible models and decide whether our
        regression model is worth using.

1)  The **Sum of Squares due to Error,**
    $\mathbf{\text{SSE}},\ $sometimes called the **Residual Sum of
    Squares,** is the total amount of prediction error we would make
    using the estimated regression equation to predict the observed
    sample data. It is the sum of the squared **residuals.**

    a.  A **residual** is the **difference between the
        [observed]{.underline} and [predicted]{.underline}**
        $\mathbf{y}_{\mathbf{i}}$ **at a given**
        $\mathbf{x}_{\mathbf{i}}\text{.\ }$In other words:

$$\mathbf{\text{At\ any\ given\ }}\mathbf{x}_{\mathbf{i}}\mathbf{,\ the\ residual =}\mathbf{y}_{\mathbf{i}}\mathbf{-}{\widehat{\mathbf{y}}}_{\mathbf{i}}$$

b.  The $SSE = \sum_{}^{}\left( y_{i} - {\widehat{y}}_{i} \right)^{2}$,
    and has degrees of freedom $df_{2} = n - p - 1$

c.  Note: the sum of the squared residuals is the quantity that is
    minimized by the Least Squares Criterion!

```{=html}
<!-- -->
```
2)  The **Total Sum of Squares,** $\mathbf{SST,}\ $is the total amount
    of prediction error we would make if we used $\overline{y}$ (the
    mean of $y_{i}$) to predict the observed sample data.

    a.  The $SST = \sum_{}^{}\left( y_{i} - \overline{y} \right)^{2}$,
        and has degrees of freedom $df = n - 1$

3)  The **Sum of Squares due to Regression,** $\mathbf{SSR,}$ is how
    much we will reduce prediction error by using the Estimated
    Regression Equation instead of the mean to predict the observed
    sample data.

    a.  The
        $SSR = \sum_{}^{}\left( {\widehat{y}}_{i} - \overline{y} \right)^{2}$,
        and has degrees of freedom $df_{1} = p$

4)  The **Mean Square Error,** $\mathbf{MSE,}$ is the estimate of the
    variance of $\epsilon$. Remember $\epsilon?$ It is the random error
    term in the Regression Model:
    $y = \beta_{0} + \beta_{1}x + \epsilon$

    a.  The $MSE = \frac{\text{SSE}}{n - p - 1}$

**[\
]{.underline}**

**[The *Coefficients* table:]{.underline}**

1)  The $\mathbf{\text{sample\ slope\ coefficients}}$**,**
    $\mathbf{b}_{\mathbf{1}}\mathbf{,\ }\mathbf{b}_{\mathbf{2}}\mathbf{,\ldots,\ }\mathbf{b}_{\mathbf{p}}$
    **,** quantify the magnitude and direction of the relationship
    between each IV and the DV. They are point estimates, respectively,
    of $\beta_{1},\ \beta_{2},\ldots,\ \beta_{p}.$

    a.  In Simple Linear Regression, the **interpretation** **of**
        $\mathbf{b}_{\mathbf{1}}$ is:

        i.  **If** $\mathbf{b}_{\mathbf{1}}\mathbf{> 0}:$

> *For every one unit increase in* $x$*,* $\text{y\ }$*is predicted to
> increase by* $b_{1}$ *on average.*

ii. **If** $\mathbf{b}_{\mathbf{1}}\mathbf{< 0}:$

> *For every one unit increase in* $x$*,* $\text{y\ }$*is predicted to
> decrease by* $b_{1}$ *on average.*

b.  In Multiple Regression, each slope is interpreted. The
    interpretation for $b_{1}$ is given here, and the interpretation of
    the other slopes follows the same pattern. The **interpretation**
    **of** $\mathbf{b}_{\mathbf{1}}$ is:

    i.  **If** $\mathbf{b}_{\mathbf{1}}\mathbf{> 0}:$

> *For every one unit increase in* $x$*,* $\text{y\ }$*is predicted to
> increase by* $b_{1}$ *on average, holding all other independent
> variables constant.*

ii. **If** $\mathbf{b}_{\mathbf{1}}\mathbf{< 0}:$

> *For every one unit increase in* $x$*,* $\text{y\ }$*is predicted to
> decrease by* $b_{1}$ *on average, holding all other independent
> variables constant.*

2)  The $\mathbf{sample\ intercept,\ }\mathbf{b}_{\mathbf{0}},$ is the
    value of $\text{y\ }$when $x = 0.$ This is not always interpretable
    in the context of a given regression. Interpretability depends on
    whether the value $x = 0\ $has substantive meaning.

3)  The
    $\mathbf{\text{confidence\ intervals\ for\ the\ slope\ coefficients}}$**,**
    $\mathbf{\beta}_{\mathbf{1}}\mathbf{,\ }\mathbf{\beta}_{\mathbf{2}}\mathbf{,\ \ldots,\ }\mathbf{\beta}_{\mathbf{p}}$**,**
    and the $\mathbf{intercept,\ }\mathbf{\beta}_{\mathbf{0}}\mathbf{,}$
    are based on the $\alpha$ significance level and are interpreted
    similarly to the other confidence intervals we have encountered:

    a.  For a given **slope coefficient, say**
        $\mathbf{\beta}_{\mathbf{1}}\mathbf{,}$ the **interpretation**
        is:

> *We can be* $\_\_\_\%\ $*confident that the true value of the
> population slope coefficient for* $x_{1}$ *is between \[lower bound\]
> and \[upper bound\].*

b.  For the **intercept,** $\mathbf{\beta}_{\mathbf{0}}$**,** the
    **interpretation** is:

> *We can be* $\_\_\_\%\ $*confident that the true value of the
> population intercept is between \[lower bound\] and \[upper bound\].*
